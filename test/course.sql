-- MySQL dump 10.13  Distrib 5.5.54, for debian-linux-gnu (x86_64)
--
-- Host: localhost    Database: course
-- ------------------------------------------------------
-- Server version	5.5.54-0ubuntu0.14.04.1

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `auth_group`
--

DROP TABLE IF EXISTS `auth_group`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_group` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(80) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `name` (`name`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_group`
--

LOCK TABLES `auth_group` WRITE;
/*!40000 ALTER TABLE `auth_group` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_group` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_group_permissions`
--

DROP TABLE IF EXISTS `auth_group_permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_group_permissions` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `group_id` int(11) NOT NULL,
  `permission_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_group_permissions_group_id_0cd325b0_uniq` (`group_id`,`permission_id`),
  KEY `auth_group_permissi_permission_id_84c5c92e_fk_auth_permission_id` (`permission_id`),
  CONSTRAINT `auth_group_permissi_permission_id_84c5c92e_fk_auth_permission_id` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
  CONSTRAINT `auth_group_permissions_group_id_b120cbf9_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_group_permissions`
--

LOCK TABLES `auth_group_permissions` WRITE;
/*!40000 ALTER TABLE `auth_group_permissions` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_group_permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_permission`
--

DROP TABLE IF EXISTS `auth_permission`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_permission` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `name` varchar(255) NOT NULL,
  `content_type_id` int(11) NOT NULL,
  `codename` varchar(100) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_permission_content_type_id_01ab375a_uniq` (`content_type_id`,`codename`),
  CONSTRAINT `auth_permissi_content_type_id_2f476e4b_fk_django_content_type_id` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=40 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_permission`
--

LOCK TABLES `auth_permission` WRITE;
/*!40000 ALTER TABLE `auth_permission` DISABLE KEYS */;
INSERT INTO `auth_permission` VALUES (1,'Can add neuron',1,'add_neuron'),(2,'Can change neuron',1,'change_neuron'),(3,'Can delete neuron',1,'delete_neuron'),(4,'Can add question',2,'add_question'),(5,'Can change question',2,'change_question'),(6,'Can delete question',2,'delete_question'),(7,'Can add log entry',3,'add_logentry'),(8,'Can change log entry',3,'change_logentry'),(9,'Can delete log entry',3,'delete_logentry'),(10,'Can add permission',4,'add_permission'),(11,'Can change permission',4,'change_permission'),(12,'Can delete permission',4,'delete_permission'),(13,'Can add user',5,'add_user'),(14,'Can change user',5,'change_user'),(15,'Can delete user',5,'delete_user'),(16,'Can add group',6,'add_group'),(17,'Can change group',6,'change_group'),(18,'Can delete group',6,'delete_group'),(19,'Can add content type',7,'add_contenttype'),(20,'Can change content type',7,'change_contenttype'),(21,'Can delete content type',7,'delete_contenttype'),(22,'Can add session',8,'add_session'),(23,'Can change session',8,'change_session'),(24,'Can delete session',8,'delete_session'),(25,'Can add user neuron',12,'add_userneuron'),(26,'Can change user neuron',12,'change_userneuron'),(27,'Can delete user neuron',12,'delete_userneuron'),(28,'Can add user question',13,'add_userquestion'),(29,'Can change user question',13,'change_userquestion'),(30,'Can delete user question',13,'delete_userquestion'),(31,'Can add chapter',9,'add_chapter'),(32,'Can change chapter',9,'change_chapter'),(33,'Can delete chapter',9,'delete_chapter'),(34,'Can add users',11,'add_users'),(35,'Can change users',11,'change_users'),(36,'Can delete users',11,'delete_users'),(37,'Can add connect',10,'add_connect'),(38,'Can change connect',10,'change_connect'),(39,'Can delete connect',10,'delete_connect');
/*!40000 ALTER TABLE `auth_permission` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_user`
--

DROP TABLE IF EXISTS `auth_user`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `password` varchar(128) NOT NULL,
  `last_login` datetime DEFAULT NULL,
  `is_superuser` tinyint(1) NOT NULL,
  `username` varchar(150) NOT NULL,
  `first_name` varchar(30) NOT NULL,
  `last_name` varchar(30) NOT NULL,
  `email` varchar(254) NOT NULL,
  `is_staff` tinyint(1) NOT NULL,
  `is_active` tinyint(1) NOT NULL,
  `date_joined` datetime NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `username` (`username`)
) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_user`
--

LOCK TABLES `auth_user` WRITE;
/*!40000 ALTER TABLE `auth_user` DISABLE KEYS */;
INSERT INTO `auth_user` VALUES (1,'pbkdf2_sha256$30000$GnYzQLEAymdP$QFA57N3Zbg5L2D41vAJnhg92fGwBigugkcL6UqeivtY=','2017-04-04 07:29:25',1,'cobbb11','','','cobbb11@163.com',1,1,'2017-03-15 15:30:44'),(2,'pbkdf2_sha256$30000$aaptTM6FsgwK$R0saxg06dsd7bZg7Gyh2ByGgqfxvBvSwP5p/XBHIh1c=','2017-04-04 06:13:02',1,'qweasd','','','',1,1,'2017-03-15 15:31:01');
/*!40000 ALTER TABLE `auth_user` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_user_groups`
--

DROP TABLE IF EXISTS `auth_user_groups`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_user_groups` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) NOT NULL,
  `group_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_user_groups_user_id_94350c0c_uniq` (`user_id`,`group_id`),
  KEY `auth_user_groups_group_id_97559544_fk_auth_group_id` (`group_id`),
  CONSTRAINT `auth_user_groups_group_id_97559544_fk_auth_group_id` FOREIGN KEY (`group_id`) REFERENCES `auth_group` (`id`),
  CONSTRAINT `auth_user_groups_user_id_6a12ed8b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_user_groups`
--

LOCK TABLES `auth_user_groups` WRITE;
/*!40000 ALTER TABLE `auth_user_groups` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_user_groups` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `auth_user_user_permissions`
--

DROP TABLE IF EXISTS `auth_user_user_permissions`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `auth_user_user_permissions` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `user_id` int(11) NOT NULL,
  `permission_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `auth_user_user_permissions_user_id_14a6b632_uniq` (`user_id`,`permission_id`),
  KEY `auth_user_user_perm_permission_id_1fbb5f2c_fk_auth_permission_id` (`permission_id`),
  CONSTRAINT `auth_user_user_perm_permission_id_1fbb5f2c_fk_auth_permission_id` FOREIGN KEY (`permission_id`) REFERENCES `auth_permission` (`id`),
  CONSTRAINT `auth_user_user_permissions_user_id_a95ead1b_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `auth_user_user_permissions`
--

LOCK TABLES `auth_user_user_permissions` WRITE;
/*!40000 ALTER TABLE `auth_user_user_permissions` DISABLE KEYS */;
/*!40000 ALTER TABLE `auth_user_user_permissions` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_admin_log`
--

DROP TABLE IF EXISTS `django_admin_log`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_admin_log` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `action_time` datetime NOT NULL,
  `object_id` longtext,
  `object_repr` varchar(200) NOT NULL,
  `action_flag` smallint(5) unsigned NOT NULL,
  `change_message` longtext NOT NULL,
  `content_type_id` int(11) DEFAULT NULL,
  `user_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `django_admin__content_type_id_c4bce8eb_fk_django_content_type_id` (`content_type_id`),
  KEY `django_admin_log_user_id_c564eba6_fk_auth_user_id` (`user_id`),
  CONSTRAINT `django_admin_log_user_id_c564eba6_fk_auth_user_id` FOREIGN KEY (`user_id`) REFERENCES `auth_user` (`id`),
  CONSTRAINT `django_admin__content_type_id_c4bce8eb_fk_django_content_type_id` FOREIGN KEY (`content_type_id`) REFERENCES `django_content_type` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=722 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_admin_log`
--

LOCK TABLES `django_admin_log` WRITE;
/*!40000 ALTER TABLE `django_admin_log` DISABLE KEYS */;
INSERT INTO `django_admin_log` VALUES (1,'2017-03-15 15:32:47','10','ExplProb3.4.5',3,'',2,1),(2,'2017-03-15 15:33:30','1','z',1,'[{\"added\": {}}]',1,1),(3,'2017-03-15 15:33:33','3','Prob3.1.3',2,'[]',2,1),(4,'2017-03-15 15:37:53','9','Expl3.4.5',2,'[]',2,1),(5,'2017-03-15 15:38:02','9','Expl3.4.5',2,'[]',2,1),(6,'2017-03-15 15:42:40','2','Discrete MC',1,'[{\"added\": {}}]',1,1),(7,'2017-03-15 15:43:24','3','Markov chain',1,'[{\"added\": {}}]',1,1),(8,'2017-03-15 15:43:39','2','Discrete MC',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,1),(9,'2017-03-15 15:43:43','3','Markov chain',2,'[]',1,1),(10,'2017-03-15 15:44:24','4','Markov process',1,'[{\"added\": {}}]',1,1),(11,'2017-03-15 15:44:49','5','n-step transition',1,'[{\"added\": {}}]',1,1),(12,'2017-03-15 15:45:21','6','One-step transition P',1,'[{\"added\": {}}]',1,1),(13,'2017-03-15 15:45:56','7','Conditional Independence',1,'[{\"added\": {}}]',1,1),(14,'2017-03-15 15:46:01','1','z',3,'',1,1),(15,'2017-03-15 16:06:43','8','2',1,'[{\"added\": {}}]',1,1),(16,'2017-03-15 16:07:01','8','2',3,'',1,1),(17,'2017-03-15 16:09:52','9','Expl3.4.5',2,'[{\"changed\": {\"fields\": [\"difficulty\"]}}]',2,1),(18,'2017-03-15 16:11:47','1','Expl1.1.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,1),(19,'2017-03-15 16:11:54','9','Expl3.4.5',2,'[]',2,1),(20,'2017-03-15 16:12:04','9','Expl3.4.5',2,'[]',2,1),(21,'2017-03-16 14:31:51','11','DIY3.1.5',1,'[{\"added\": {}}]',2,2),(22,'2017-03-16 14:34:59','11','DIY3.1.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(23,'2017-03-16 14:41:59','12','DIY3.1.6',1,'[{\"added\": {}}]',2,2),(24,'2017-03-16 14:45:02','11','DIY3.1.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(25,'2017-03-16 14:48:47','13','DIY3.1.7',1,'[{\"added\": {}}]',2,2),(26,'2017-03-16 14:49:44','13','DIY3.1.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(27,'2017-03-16 14:51:00','12','DIY3.1.6',2,'[]',2,2),(28,'2017-03-16 14:56:31','14','DIY3.1.8',1,'[{\"added\": {}}]',2,2),(29,'2017-03-16 14:58:34','14','DIY3.1.8',2,'[]',2,2),(30,'2017-03-16 14:58:53','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(31,'2017-03-16 14:59:01','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(32,'2017-03-16 14:59:45','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(33,'2017-03-16 14:59:52','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(34,'2017-03-16 15:00:49','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(35,'2017-03-16 15:00:57','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(36,'2017-03-16 15:02:37','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(37,'2017-03-16 15:02:44','14','DIY3.1.8',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(38,'2017-03-16 15:03:41','12','DIY3.1.6',2,'[]',2,2),(39,'2017-03-16 15:06:35','15','DIY3.2.1',1,'[{\"added\": {}}]',2,2),(40,'2017-03-16 15:08:31','15','DIY3.2.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(41,'2017-03-16 15:08:53','15','DIY3.2.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(42,'2017-03-16 15:13:04','9','First step analysis',1,'[{\"added\": {}}]',1,2),(43,'2017-03-16 15:14:28','16','DIY3.4.6',1,'[{\"added\": {}}]',2,2),(44,'2017-03-16 15:17:09','17','DIY3.4.7',1,'[{\"added\": {}}]',2,2),(45,'2017-03-16 15:20:00','18','DIY3.4.8',1,'[{\"added\": {}}]',2,2),(46,'2017-03-16 15:22:27','19','DIY3.4.9',1,'[{\"added\": {}}]',2,2),(47,'2017-03-16 15:23:03','19','DIY3.4.9',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(48,'2017-03-16 15:26:17','20','DIY3.4.10',1,'[{\"added\": {}}]',2,2),(49,'2017-03-16 15:30:58','20','DIY3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(50,'2017-03-16 15:32:17','16','DIY3.4.6',2,'[]',2,2),(51,'2017-03-16 15:32:40','17','DIY3.4.7',2,'[]',2,2),(52,'2017-03-16 15:33:03','18','DIY3.4.8',2,'[]',2,2),(53,'2017-03-16 15:33:18','19','DIY3.4.9',2,'[]',2,2),(54,'2017-03-16 15:34:53','17','DIY3.4.7',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(55,'2017-03-16 15:35:33','17','DIY3.4.7',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(56,'2017-03-17 12:07:14','21','Qui0.1',1,'[{\"added\": {}}]',2,2),(57,'2017-03-17 12:08:49','21','Qui0.1',2,'[]',2,2),(58,'2017-03-17 12:12:13','22','Qui0.2',1,'[{\"added\": {}}]',2,2),(59,'2017-03-17 12:12:38','22','Qui0.2',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(60,'2017-03-17 12:15:37','23','Qui0.3',1,'[{\"added\": {}}]',2,2),(61,'2017-03-17 12:20:50','24','Qui0.4',1,'[{\"added\": {}}]',2,2),(62,'2017-03-17 12:22:59','25','Qui0.5',1,'[{\"added\": {}}]',2,2),(63,'2017-03-17 12:26:29','26','Qui0.6',1,'[{\"added\": {}}]',2,2),(64,'2017-03-17 12:29:25','27','Qui0.7',1,'[{\"added\": {}}]',2,2),(65,'2017-03-17 12:32:10','28','Qui0.8',1,'[{\"added\": {}}]',2,2),(66,'2017-03-17 12:34:59','29','Qui0.9',1,'[{\"added\": {}}]',2,2),(67,'2017-03-17 12:35:30','29','Qui0.9',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(68,'2017-03-17 12:39:05','30','Qui0.10',1,'[{\"added\": {}}]',2,2),(69,'2017-03-17 13:37:59','31','ExplExpl  3.1.1',1,'[{\"added\": {}}]',2,2),(70,'2017-03-17 13:38:20','31','Expl3.1.1',2,'[{\"changed\": {\"fields\": [\"code\"]}}]',2,2),(71,'2017-03-17 13:47:19','32','Expl3.1.2',1,'[{\"added\": {}}]',2,2),(72,'2017-03-17 13:50:36','33','Expl3.1.3',1,'[{\"added\": {}}]',2,2),(73,'2017-03-17 13:54:00','34','Expl3.1.4',1,'[{\"added\": {}}]',2,2),(74,'2017-03-17 13:57:37','35','Expl3.1.5',1,'[{\"added\": {}}]',2,2),(75,'2017-03-17 14:01:02','36','Expl3.1.6',1,'[{\"added\": {}}]',2,2),(76,'2017-03-17 14:01:37','35','Expl3.1.5',2,'[]',2,2),(77,'2017-03-17 14:02:12','35','Expl3.1.5',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(78,'2017-03-18 09:09:05','2','Discrete MC',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(79,'2017-03-18 09:09:47','2','Discrete MC',2,'[]',1,2),(80,'2017-03-18 09:10:54','3','Markov chain',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(81,'2017-03-18 09:12:22','4','Markov process',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(82,'2017-03-18 09:13:47','5','n-step transition',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(83,'2017-03-18 09:15:18','6','One-step transition P',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(84,'2017-03-18 09:15:51','7','Conditional Independence',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(85,'2017-03-18 09:16:03','7','Conditional Independence',2,'[]',1,2),(86,'2017-03-18 09:19:16','6','One-step transition P',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(87,'2017-03-18 09:21:16','5','n-step transition',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(88,'2017-03-18 09:22:12','3','Markov chain',2,'[]',1,2),(89,'2017-03-18 09:22:37','3','Markov chain',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(90,'2017-03-18 09:26:04','10','$\\bf{P}^{(n)} = \\bf{P}^n$',1,'[{\"added\": {}}]',1,2),(91,'2017-03-18 09:26:27','10','{P}^{(n)} = {P}^n',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(92,'2017-03-18 09:26:58','10','{P}^{(n)} = {P}^n',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(93,'2017-03-18 09:28:31','11','First step analysis',1,'[{\"added\": {}}]',1,2),(94,'2017-03-18 09:30:21','11','First step analysis',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(95,'2017-03-18 09:32:13','12','Calculate the probability of an event',1,'[{\"added\": {}}]',1,2),(96,'2017-03-18 09:34:32','13','Calculate Mean Numbers of Steps to a State',1,'[{\"added\": {}}]',1,2),(97,'2017-03-18 09:35:53','14','Calculate Mean number of visits to a state',1,'[{\"added\": {}}]',1,2),(98,'2017-03-18 09:37:16','13','Calculate Mean Steps to a State',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(99,'2017-03-18 09:37:25','14','Calculate Mean visits to a state',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(100,'2017-03-18 09:39:03','14','Calculate Mean visits to a state',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(101,'2017-03-18 09:39:28','14','Branching Process',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(102,'2017-03-18 09:40:08','14','Branching Process',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(103,'2017-03-18 09:41:23','15','Mean number of visits to a state',1,'[{\"added\": {}}]',1,2),(104,'2017-03-18 09:42:55','16','Mean,Varance in Branching Process',1,'[{\"added\": {}}]',1,2),(105,'2017-03-18 09:43:25','16','Mean,Varance in Branching Process',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(106,'2017-03-18 09:44:40','17','xi^{(n)} iid',1,'[{\"added\": {}}]',1,2),(107,'2017-03-18 09:46:03','18','$g_N[\\phi(x)]$',1,'[{\"added\": {}}]',1,2),(108,'2017-03-18 09:46:31','17','$\\xi^{(n)} iid$',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(109,'2017-03-18 09:48:25','19','$\\phi_{\\xi}(s)$',1,'[{\"added\": {}}]',1,2),(110,'2017-03-18 09:50:38','20','Probability Genarating Function',1,'[{\"added\": {}}]',1,2),(111,'2017-03-18 09:51:14','37','Exer3.1.1',1,'[{\"added\": {}}]',2,2),(112,'2017-03-18 09:51:57','21','Extinction Probability',1,'[{\"added\": {}}]',1,2),(113,'2017-03-18 09:53:13','21','Extinction Probability',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(114,'2017-03-18 09:53:27','21','Extinction Probability',2,'[]',1,2),(115,'2017-03-18 09:53:29','38','Exer3.1.2',1,'[{\"added\": {}}]',2,2),(116,'2017-03-18 09:54:03','22','Eventual Extinction',1,'[{\"added\": {}}]',1,2),(117,'2017-03-18 09:54:30','22','Eventual Extinction',2,'[]',1,2),(118,'2017-03-18 09:55:13','39','Exer3.1.3',1,'[{\"added\": {}}]',2,2),(119,'2017-03-18 09:55:17','10','$P^n$',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(120,'2017-03-18 09:56:36','9','First step analysis',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(121,'2017-03-18 09:57:43','9','First step analysis',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(122,'2017-03-18 09:57:53','40','Exer3.1.4',1,'[{\"added\": {}}]',2,2),(123,'2017-03-18 09:59:12','11','First step analysis',3,'',1,2),(124,'2017-03-18 09:59:38','15','Calculate Mean number of visits to a state',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(125,'2017-03-18 09:59:42','41','Exer3.1.5',1,'[{\"added\": {}}]',2,2),(126,'2017-03-18 09:59:48','22','Eventual Extinction',2,'[]',1,2),(127,'2017-03-18 10:02:03','42','Exer3.2.1',1,'[{\"added\": {}}]',2,2),(128,'2017-03-18 10:04:51','43','Exer3.2.2',1,'[{\"added\": {}}]',2,2),(129,'2017-03-18 10:06:59','44','Exer3.2.3',1,'[{\"added\": {}}]',2,2),(130,'2017-03-18 10:08:58','45','Exer3.2.4',1,'[{\"added\": {}}]',2,2),(131,'2017-03-18 10:10:38','46','Exer3.2.5',1,'[{\"added\": {}}]',2,2),(132,'2017-03-18 10:13:51','47','Exer3.2.6',1,'[{\"added\": {}}]',2,2),(133,'2017-03-18 11:29:59','48','DIY3.1.2',1,'[{\"added\": {}}]',2,2),(134,'2017-03-18 11:33:40','48','DIY3.1.2',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(135,'2017-03-18 11:34:39','48','DIY3.1.2',2,'[]',2,2),(136,'2017-03-18 11:37:14','17','DIY3.4.7',2,'[]',2,2),(137,'2017-03-18 11:56:13','49','DIY3.1.4',1,'[{\"added\": {}}]',2,2),(138,'2017-03-18 11:57:51','12','DIY3.1.6',2,'[]',2,2),(139,'2017-03-18 11:58:10','47','Exer3.2.6',2,'[]',2,2),(140,'2017-03-18 11:58:49','22','Qui0.2',2,'[]',2,2),(141,'2017-03-18 11:59:25','49','DIY3.1.4',2,'[]',2,2),(142,'2017-03-18 11:59:39','48','DIY3.1.2',2,'[]',2,2),(143,'2017-03-18 11:59:53','49','DIY3.1.4',2,'[{\"changed\": {\"fields\": [\"sensitivity\"]}}]',2,2),(144,'2017-03-18 12:24:12','50','DIY3.4.2',1,'[{\"added\": {}}]',2,2),(145,'2017-03-18 12:26:12','50','DIY3.4.2',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(146,'2017-03-18 14:39:36','51','DIY3.4.4',1,'[{\"added\": {}}]',2,2),(147,'2017-03-18 14:41:01','51','DIY3.4.4',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(148,'2017-03-18 14:51:48','52','DIY3.9.1',1,'[{\"added\": {}}]',2,2),(149,'2017-03-18 14:59:50','53','DIY3.9.3',1,'[{\"added\": {}}]',2,2),(150,'2017-03-18 15:00:44','53','DIY3.9.3',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(151,'2017-03-18 15:01:10','52','DIY3.9.1',2,'[{\"changed\": {\"fields\": [\"difficulty\"]}}]',2,2),(152,'2017-03-19 00:38:35','2','DIY3.1.1',2,'[{\"changed\": {\"fields\": [\"linkability1\", \"linkability2\", \"linkability3\", \"linkability4\", \"linkability5\", \"linkability6\", \"gussingparameter\"]}}]',2,2),(153,'2017-03-19 00:39:52','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"category\"]}}]',2,2),(154,'2017-03-19 00:40:47','2','DIY3.1.1',2,'[]',2,2),(155,'2017-03-19 00:44:18','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(156,'2017-03-19 00:44:56','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(157,'2017-03-19 00:45:32','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(158,'2017-03-19 00:46:20','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(159,'2017-03-19 00:46:51','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(160,'2017-03-19 00:47:36','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(161,'2017-03-19 00:48:13','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(162,'2017-03-19 00:48:36','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(163,'2017-03-19 00:49:04','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(164,'2017-03-19 00:53:25','3','DIY3.1.3',2,'[{\"changed\": {\"fields\": [\"linkability1\", \"linkability2\", \"linkability3\", \"linkability4\", \"linkability5\", \"linkability6\", \"gussingparameter\"]}}]',2,2),(165,'2017-03-19 00:53:43','3','DIY3.1.3',2,'[]',2,2),(166,'2017-03-19 00:57:10','7','DIY3.4.1',2,'[{\"changed\": {\"fields\": [\"category\"]}}]',2,2),(167,'2017-03-19 01:05:22','7','DIY3.4.1',2,'[{\"changed\": {\"fields\": [\"linkability1\", \"linkability2\", \"linkability3\", \"linkability4\", \"linkability5\", \"linkability6\"]}}]',2,2),(168,'2017-03-19 01:06:36','7','DIY3.4.1',2,'[]',2,2),(169,'2017-03-19 01:07:41','7','DIY3.4.1',2,'[]',2,2),(170,'2017-03-19 01:08:13','7','DIY3.4.1',2,'[]',2,2),(171,'2017-03-19 01:09:22','8','DIY3.4.3',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(172,'2017-03-19 01:13:06','8','DIY3.4.3',2,'[]',2,2),(173,'2017-03-19 01:16:07','8','DIY3.4.3',2,'[{\"changed\": {\"fields\": [\"linkability1\", \"linkability2\", \"linkability3\", \"linkability4\", \"linkability5\", \"linkability6\"]}}]',2,2),(174,'2017-03-19 01:17:37','8','DIY3.4.3',2,'[{\"changed\": {\"fields\": [\"difficulty\"]}}]',2,2),(175,'2017-03-19 01:18:33','9','DIY3.4.5',2,'[{\"changed\": {\"fields\": [\"category\"]}}]',2,2),(176,'2017-03-19 01:19:34','9','Expl3.4.5',2,'[{\"changed\": {\"fields\": [\"category\"]}}]',2,2),(177,'2017-03-19 01:38:42','54','DIY3.4.5',1,'[{\"added\": {}}]',2,2),(178,'2017-03-19 01:40:35','54','DIY3.4.5',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\", \"alternativesolutions\"]}}]',2,2),(179,'2017-03-19 01:40:56','54','DIY3.4.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(180,'2017-03-19 01:42:14','54','DIY3.4.5',2,'[{\"changed\": {\"fields\": [\"alternativesolutions\"]}}]',2,2),(181,'2017-03-19 01:42:38','54','DIY3.4.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(182,'2017-03-19 01:42:59','54','DIY3.4.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(183,'2017-03-19 01:45:22','54','DIY3.4.5',2,'[{\"changed\": {\"fields\": [\"messagefailure\", \"messagesuccess\"]}}]',2,2),(184,'2017-03-19 01:46:17','54','DIY3.4.5',2,'[]',2,2),(185,'2017-03-19 01:58:42','55','DIY3.9.2',1,'[{\"added\": {}}]',2,2),(186,'2017-03-19 01:59:24','55','DIY3.9.2',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(187,'2017-03-19 04:23:23','23','Events and Probability',1,'[{\"added\": {}}]',1,2),(188,'2017-03-19 04:28:43','24','Random variable',1,'[{\"added\": {}}]',1,2),(189,'2017-03-19 04:31:44','25','Conditional probability',1,'[{\"added\": {}}]',1,2),(190,'2017-03-19 04:33:46','25','Conditional probability',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(191,'2017-03-19 04:34:09','23','Events and Probability',2,'[]',1,2),(192,'2017-03-19 06:32:54','26','Slice Universe',1,'[{\"added\": {}}]',1,2),(193,'2017-03-19 06:37:52','27','Bayes\' Theorem',1,'[{\"added\": {}}]',1,2),(194,'2017-03-19 06:47:47','28','Independence of events',1,'[{\"added\": {}}]',1,2),(195,'2017-03-19 06:48:54','28','Independence of events',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(196,'2017-03-19 06:49:09','28','Independence of events',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(197,'2017-03-19 06:50:26','28','Independence of events',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(198,'2017-03-19 06:50:41','28','Independence of events',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(199,'2017-03-19 07:08:10','29','Central limit theory',1,'[{\"added\": {}}]',1,2),(200,'2017-03-19 07:10:12','29','Central Limit Theory',2,'[{\"changed\": {\"fields\": [\"title\", \"detail\"]}}]',1,2),(201,'2017-03-19 07:11:33','4','Markov Process',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(202,'2017-03-19 07:11:41','3','Markov Chain',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(203,'2017-03-19 07:11:51','5','N-step Transition',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(204,'2017-03-19 07:11:59','6','One-step Transition P',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(205,'2017-03-19 07:12:10','9','First Step Analysis',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(206,'2017-03-19 07:12:24','12','Calculate the Probability of an Event',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(207,'2017-03-19 07:12:42','15','Calculate Mean Number of Visits to a State',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(208,'2017-03-19 07:12:52','24','Random Variable',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(209,'2017-03-19 07:12:59','28','Independence of Events',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(210,'2017-03-19 07:17:49','30','Joint distribution',1,'[{\"added\": {}}]',1,2),(211,'2017-03-19 07:35:54','30','Joint distribution',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(212,'2017-03-19 07:36:39','30','Joint distribution',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(213,'2017-03-19 07:37:24','30','Joint Distribution',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(214,'2017-03-19 07:37:34','25','Conditional Probability',2,'[{\"changed\": {\"fields\": [\"title\", \"detail\"]}}]',1,2),(215,'2017-03-19 07:41:42','30','Joint Distribution',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(216,'2017-03-19 07:45:37','31','Mean and Variance',1,'[{\"added\": {}}]',1,2),(217,'2017-03-19 07:48:59','32','Independence of random variables',1,'[{\"added\": {}}]',1,2),(218,'2017-03-19 07:53:49','31','Mean and Variance',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(219,'2017-03-19 08:02:29','33','Correlate',1,'[{\"added\": {}}]',1,2),(220,'2017-03-19 08:03:01','33','Correlate',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(221,'2017-03-19 08:03:13','33','Correlate',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(222,'2017-03-19 08:04:05','28','Independence of Events',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(223,'2017-03-19 08:04:52','21','Qui0.1',2,'[]',2,2),(224,'2017-03-19 08:05:19','22','Qui0.2',2,'[]',2,2),(225,'2017-03-19 08:05:42','23','Qui0.3',2,'[]',2,2),(226,'2017-03-19 08:06:00','24','Qui0.4',2,'[]',2,2),(227,'2017-03-19 08:06:34','25','Qui0.5',2,'[]',2,2),(228,'2017-03-19 08:06:57','26','Qui0.6',2,'[]',2,2),(229,'2017-03-19 08:07:12','27','Qui0.7',2,'[]',2,2),(230,'2017-03-19 08:07:53','29','Qui0.9',2,'[]',2,2),(231,'2017-03-19 08:08:03','30','Qui0.10',2,'[]',2,2),(232,'2017-03-19 08:10:16','56','ProbProb.3.1.1',1,'[{\"added\": {}}]',2,2),(233,'2017-03-19 08:10:32','56','Prob3.1.1',2,'[{\"changed\": {\"fields\": [\"code\"]}}]',2,2),(234,'2017-03-19 08:13:15','34','Distribution and Density',1,'[{\"added\": {}}]',1,2),(235,'2017-03-19 08:13:57','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(236,'2017-03-19 08:14:18','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(237,'2017-03-19 08:14:20','9','Prob3.4.5',2,'[{\"changed\": {\"fields\": [\"category\", \"solutions\", \"difficulty\"]}}]',2,2),(238,'2017-03-19 08:15:15','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(239,'2017-03-19 08:16:20','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(240,'2017-03-19 08:16:56','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(241,'2017-03-19 08:19:12','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(242,'2017-03-19 08:19:35','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(243,'2017-03-19 08:20:34','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(244,'2017-03-19 08:21:19','56','Prob3.1.1',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\", \"choicesd\", \"gussingparameter\"]}}]',2,2),(245,'2017-03-19 08:21:26','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(246,'2017-03-19 08:21:55','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(247,'2017-03-19 08:22:08','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(248,'2017-03-19 08:22:23','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(249,'2017-03-19 08:22:54','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(250,'2017-03-19 08:40:28','57','Prob3.1.4',1,'[{\"added\": {}}]',2,2),(251,'2017-03-19 10:49:02','58','Prob3.1.2',1,'[{\"added\": {}}]',2,2),(252,'2017-03-19 10:52:06','58','Prob3.1.2',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"choicese\"]}}]',2,2),(253,'2017-03-19 10:53:23','58','Prob3.1.2',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"choicese\"]}}]',2,2),(254,'2017-03-19 12:12:48','4','Expl3.2.8',2,'[{\"changed\": {\"fields\": [\"problem\", \"problempicture1\"]}}]',2,2),(255,'2017-03-19 12:13:38','4','Expl3.2.8',2,'[]',2,2),(256,'2017-03-19 12:15:09','6','Expl3.3.12',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(257,'2017-03-19 12:17:50','59','Expl3.3.11',1,'[{\"added\": {}}]',2,2),(258,'2017-03-19 12:19:17','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(259,'2017-03-19 12:19:43','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(260,'2017-03-19 12:20:18','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(261,'2017-03-19 12:21:32','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(262,'2017-03-19 12:22:59','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(263,'2017-03-19 12:23:20','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(264,'2017-03-19 12:23:45','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(265,'2017-03-19 12:25:45','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(266,'2017-03-19 12:26:28','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(267,'2017-03-19 12:27:44','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(268,'2017-03-19 12:30:07','60','Expl3.2.7',1,'[{\"added\": {}}]',2,2),(269,'2017-03-19 12:33:25','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(270,'2017-03-19 12:34:00','59','Expl3.3.11',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(271,'2017-03-19 12:34:27','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(272,'2017-03-19 12:34:45','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(273,'2017-03-19 12:38:58','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(274,'2017-03-19 12:40:14','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(275,'2017-03-19 12:41:12','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(276,'2017-03-19 12:43:20','61','Prob3.2.3',1,'[{\"added\": {}}]',2,2),(277,'2017-03-19 12:47:19','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(278,'2017-03-19 12:48:17','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(279,'2017-03-19 12:49:09','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(280,'2017-03-19 12:50:04','60','Expl3.2.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(281,'2017-03-19 13:09:29','62','Prob3.3.1',1,'[{\"added\": {}}]',2,2),(282,'2017-03-19 13:58:08','63','Prob3.2.1',1,'[{\"added\": {}}]',2,2),(283,'2017-03-19 14:01:54','64','Prob3.3.7',1,'[{\"added\": {}}]',2,2),(284,'2017-03-19 14:18:23','63','Prob3.2.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(285,'2017-03-19 14:18:42','65','Prob3.3.10.a',1,'[{\"added\": {}}]',2,2),(286,'2017-03-19 14:19:22','63','Prob3.2.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(287,'2017-03-19 14:21:09','63','Prob3.2.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(288,'2017-03-19 14:22:22','63','Prob3.2.1',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\"]}}]',2,2),(289,'2017-03-19 14:22:53','63','Prob3.2.1',2,'[]',2,2),(290,'2017-03-19 14:28:17','65','Prob3.3.10.a',3,'',2,2),(291,'2017-03-19 14:30:58','66','Prob3.2.4',1,'[{\"added\": {}}]',2,2),(292,'2017-03-19 14:31:39','66','Prob3.2.4',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(293,'2017-03-19 14:32:13','66','Prob3.2.4',2,'[]',2,2),(294,'2017-03-19 14:33:33','67','Prob3.3.10',1,'[{\"added\": {}}]',2,2),(295,'2017-03-19 14:36:34','68','Prob3.3.2',1,'[{\"added\": {}}]',2,2),(296,'2017-03-19 14:52:13','68','Prob3.3.2',2,'[]',2,2),(297,'2017-03-19 14:55:07','69','Prob3.3.5',1,'[{\"added\": {}}]',2,2),(298,'2017-03-19 14:56:05','69','Prob3.3.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(299,'2017-03-19 14:56:32','69','Prob3.3.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(300,'2017-03-19 14:57:09','69','Prob3.3.5',2,'[]',2,2),(301,'2017-03-19 15:06:55','70','Prob3.3.8',1,'[{\"added\": {}}]',2,2),(302,'2017-03-19 15:07:37','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(303,'2017-03-19 15:07:57','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(304,'2017-03-19 15:08:18','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(305,'2017-03-19 15:09:04','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(306,'2017-03-19 15:09:30','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(307,'2017-03-19 15:09:59','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(308,'2017-03-19 15:10:35','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(309,'2017-03-19 15:11:13','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"choicese\", \"solutions\"]}}]',2,2),(310,'2017-03-19 15:11:39','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(311,'2017-03-19 15:12:00','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(312,'2017-03-19 15:12:29','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(313,'2017-03-19 15:12:45','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(314,'2017-03-19 15:13:12','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(315,'2017-03-19 15:13:48','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(316,'2017-03-19 15:14:16','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(317,'2017-03-19 15:14:50','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(318,'2017-03-19 15:15:09','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(319,'2017-03-19 15:15:43','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(320,'2017-03-19 15:16:14','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(321,'2017-03-19 15:16:34','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(322,'2017-03-19 15:16:57','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(323,'2017-03-19 15:17:23','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(324,'2017-03-19 15:18:24','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(325,'2017-03-19 15:19:01','71','Prob3.4.6',1,'[{\"added\": {}}]',2,2),(326,'2017-03-19 15:19:08','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(327,'2017-03-19 15:19:54','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(328,'2017-03-19 15:20:26','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(329,'2017-03-19 15:21:26','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(330,'2017-03-19 15:24:46','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(331,'2017-03-19 15:25:17','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(332,'2017-03-19 15:26:31','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(333,'2017-03-19 15:28:37','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(334,'2017-03-19 15:29:11','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(335,'2017-03-19 15:33:58','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(336,'2017-03-19 15:34:27','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(337,'2017-03-19 15:36:24','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(338,'2017-03-19 15:37:06','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(339,'2017-03-19 15:37:48','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(340,'2017-03-19 15:38:09','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(341,'2017-03-19 15:38:31','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(342,'2017-03-19 15:39:10','70','Prob3.3.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(343,'2017-03-19 15:39:39','70','Prob3.3.8',2,'[]',2,2),(344,'2017-03-19 15:47:40','72','Prob3.4.9',1,'[{\"added\": {}}]',2,2),(345,'2017-03-19 16:04:01','73','Prob3.4.12',1,'[{\"added\": {}}]',2,2),(346,'2017-03-19 16:04:18','73','Prob3.4.12',2,'[]',2,2),(347,'2017-03-19 16:51:02','74','Prob3.4.15',1,'[{\"added\": {}}]',2,2),(348,'2017-03-19 17:33:50','75','Prob3.4.1',1,'[{\"added\": {}}]',2,2),(349,'2017-03-19 17:35:56','75','Prob3.4.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(350,'2017-03-19 17:36:42','75','Prob3.4.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(351,'2017-03-19 17:37:06','75','Prob3.4.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(352,'2017-03-19 17:37:37','75','Prob3.4.1',2,'[]',2,2),(353,'2017-03-19 17:42:41','76','Prob3.4.4',1,'[{\"added\": {}}]',2,2),(354,'2017-03-19 17:43:22','76','Prob3.4.4',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(355,'2017-03-19 17:44:08','76','Prob3.4.4',2,'[]',2,2),(356,'2017-03-19 17:45:48','77','Prob3.4.7',1,'[{\"added\": {}}]',2,2),(357,'2017-03-19 17:46:22','77','Prob3.4.7',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(358,'2017-03-19 17:49:50','77','Prob3.4.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(359,'2017-03-19 17:50:30','77','Prob3.4.7',2,'[]',2,2),(360,'2017-03-19 17:56:09','78','Prob3.4.16',1,'[{\"added\": {}}]',2,2),(361,'2017-03-19 17:56:55','78','Prob3.4.16',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(362,'2017-03-19 17:57:12','78','Prob3.4.16',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(363,'2017-03-19 17:57:25','78','Prob3.4.16',2,'[]',2,2),(364,'2017-03-19 18:00:55','79','Prob3.8.3',1,'[{\"added\": {}}]',2,2),(365,'2017-03-19 18:01:34','79','Prob3.8.3',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(366,'2017-03-19 18:02:02','79','Prob3.8.3',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"choicese\"]}}]',2,2),(367,'2017-03-19 18:03:33','79','Prob3.8.3',2,'[]',2,2),(368,'2017-03-19 18:05:43','80','Prob3.9.2',1,'[{\"added\": {}}]',2,2),(369,'2017-03-19 18:07:42','80','Prob3.9.2',2,'[]',2,2),(370,'2017-03-19 18:10:54','81','Prob3.9.5',1,'[{\"added\": {}}]',2,2),(371,'2017-03-19 18:11:58','82','Prob3.8.2',1,'[{\"added\": {}}]',2,2),(372,'2017-03-19 18:17:20','81','Prob3.9.5',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(373,'2017-03-19 18:18:12','81','Prob3.9.5',2,'[]',2,2),(374,'2017-03-19 18:20:26','83','Prob3.9.8',1,'[{\"added\": {}}]',2,2),(375,'2017-03-19 18:21:28','83','Prob3.9.8',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(376,'2017-03-19 18:22:12','83','Prob3.9.8',2,'[]',2,2),(377,'2017-03-19 18:22:23','83','Prob3.9.8',2,'[]',2,2),(378,'2017-03-19 18:24:35','63','Prob3.2.1',2,'[]',2,2),(379,'2017-03-19 18:25:28','63','Prob3.2.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(380,'2017-03-19 18:26:04','63','Prob3.2.1',2,'[]',2,2),(381,'2017-03-19 18:27:12','66','Prob3.2.4',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(382,'2017-03-19 18:29:09','66','Prob3.2.4',2,'[]',2,2),(383,'2017-03-19 18:29:46','84','Prob3.9.1',1,'[{\"added\": {}}]',2,2),(384,'2017-03-19 18:35:27','68','Prob3.3.2',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(385,'2017-03-19 18:36:01','68','Prob3.3.2',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(386,'2017-03-19 18:36:29','68','Prob3.3.2',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(387,'2017-03-19 18:38:18','68','Prob3.3.2',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(388,'2017-03-19 18:38:43','68','Prob3.3.2',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(389,'2017-03-19 18:39:40','68','Prob3.3.2',2,'[]',2,2),(390,'2017-03-19 18:49:41','69','Prob3.3.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(391,'2017-03-19 18:50:17','69','Prob3.3.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(392,'2017-03-19 18:51:21','75','Prob3.4.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(393,'2017-03-19 18:53:11','84','Prob3.9.1',2,'[{\"changed\": {\"fields\": [\"problem\", \"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"choicese\", \"solutions\", \"difficulty\"]}}]',2,2),(394,'2017-03-20 03:29:06','85','Prob3.9.7',1,'[{\"added\": {}}]',2,2),(395,'2017-03-20 03:30:26','85','Prob3.9.7',2,'[]',2,2),(396,'2017-03-21 13:30:21','86','Prob3.9.10.a',1,'[{\"added\": {}}]',2,2),(397,'2017-03-21 13:31:39','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"choicesd\"]}}]',2,2),(398,'2017-03-21 13:51:23','56','Prob3.1.1',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\"]}}]',2,2),(399,'2017-03-21 13:53:29','57','Prob3.1.4',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\"]}}]',2,2),(400,'2017-03-21 13:54:24','61','Prob3.2.3',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(401,'2017-03-21 13:55:25','62','Prob3.3.1',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"solutions\"]}}]',2,2),(402,'2017-03-21 13:56:49','64','Prob3.3.7',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"solutions\"]}}]',2,2),(403,'2017-03-21 13:57:53','73','Prob3.4.12',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(404,'2017-03-21 14:01:40','84','Prob3.9.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(405,'2017-03-21 20:15:44','87','Prob3.4.10',1,'[{\"added\": {}}]',2,2),(406,'2017-03-21 20:16:43','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(407,'2017-03-21 20:17:11','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(408,'2017-03-21 20:17:47','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(409,'2017-03-21 20:18:13','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(410,'2017-03-21 20:18:45','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(411,'2017-03-21 20:19:23','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(412,'2017-03-21 20:19:51','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(413,'2017-03-21 20:20:18','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(414,'2017-03-21 20:20:40','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(415,'2017-03-21 20:21:00','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(416,'2017-03-21 20:21:31','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(417,'2017-03-21 20:21:57','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(418,'2017-03-21 20:22:27','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(419,'2017-03-21 20:22:58','87','Prob3.4.10',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(420,'2017-03-21 20:23:43','87','Prob3.4.10',2,'[]',2,2),(421,'2017-03-21 20:26:01','88','Prob3.4.19',1,'[{\"added\": {}}]',2,2),(422,'2017-03-21 20:27:07','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(423,'2017-03-21 20:28:20','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(424,'2017-03-21 20:28:43','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(425,'2017-03-21 20:29:02','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(426,'2017-03-21 20:29:18','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(427,'2017-03-21 20:29:33','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(428,'2017-03-21 20:29:53','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(429,'2017-03-21 20:30:10','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(430,'2017-03-21 20:30:37','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(431,'2017-03-21 20:31:33','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(432,'2017-03-21 20:31:50','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(433,'2017-03-21 20:32:05','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(434,'2017-03-21 20:32:20','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(435,'2017-03-21 20:32:38','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(436,'2017-03-21 20:32:50','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(437,'2017-03-21 20:33:52','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutionspicture1\", \"solutions\"]}}]',2,2),(438,'2017-03-21 20:36:56','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(439,'2017-03-21 20:37:35','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(440,'2017-03-21 20:39:01','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(441,'2017-03-21 20:46:09','88','Prob3.4.19',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(442,'2017-03-21 20:49:09','89','Prob3.4.13',1,'[{\"added\": {}}]',2,2),(443,'2017-03-21 20:51:22','89','Prob3.4.13',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(444,'2017-03-21 20:51:58','89','Prob3.4.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(445,'2017-03-21 20:52:20','89','Prob3.4.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(446,'2017-03-21 20:52:48','89','Prob3.4.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(447,'2017-03-21 20:53:10','89','Prob3.4.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(448,'2017-03-21 20:53:40','89','Prob3.4.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(449,'2017-03-21 20:54:13','89','Prob3.4.13',2,'[]',2,2),(450,'2017-03-22 02:50:02','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(451,'2017-03-22 02:55:12','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(452,'2017-03-22 02:55:53','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(453,'2017-03-22 02:56:04','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"difficulty\"]}}]',2,2),(454,'2017-03-22 02:57:01','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(455,'2017-03-22 02:59:05','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(456,'2017-03-22 03:01:06','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(457,'2017-03-22 03:02:51','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(458,'2017-03-22 03:05:59','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(459,'2017-03-22 03:06:25','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(460,'2017-03-22 03:07:37','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(461,'2017-03-22 03:08:29','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\", \"choicesd\", \"choicese\", \"choicesf\"]}}]',2,2),(462,'2017-03-22 06:28:13','90','Exer3.3.1',1,'[{\"added\": {}}]',2,2),(463,'2017-03-22 06:28:56','90','Exer3.3.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(464,'2017-03-22 06:29:32','90','Exer3.3.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(465,'2017-03-22 06:32:06','90','Exer3.3.1',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\", \"solutions\", \"messagefailure\", \"messagesuccess\", \"gussingparameter\"]}}]',2,2),(466,'2017-03-22 06:44:44','91','Exer3.3.2',1,'[{\"added\": {}}]',2,2),(467,'2017-03-22 06:47:16','91','Exer3.3.2',2,'[{\"changed\": {\"fields\": [\"choicesb\"]}}]',2,2),(468,'2017-03-22 06:48:21','91','Exer3.3.2',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\"]}}]',2,2),(469,'2017-03-22 06:48:53','91','Exer3.3.2',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(470,'2017-03-22 06:49:51','91','Exer3.3.2',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(471,'2017-03-22 06:50:57','91','Exer3.3.2',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\", \"gussingparameter\"]}}]',2,2),(472,'2017-03-22 06:51:27','91','Exer3.3.2',2,'[{\"changed\": {\"fields\": [\"messagefailure\", \"messagesuccess\"]}}]',2,2),(473,'2017-03-22 07:14:33','92','Exer3.3.3',1,'[{\"added\": {}}]',2,2),(474,'2017-03-22 07:29:20','93','Exer3.3.4',1,'[{\"added\": {}}]',2,2),(475,'2017-03-22 07:30:17','93','Exer3.3.4',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(476,'2017-03-22 07:30:38','93','Exer3.3.4',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(477,'2017-03-22 07:32:33','93','Exer3.3.4',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\", \"solutions\"]}}]',2,2),(478,'2017-03-22 07:33:25','93','Exer3.3.4',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\"]}}]',2,2),(479,'2017-03-22 07:35:11','93','Exer3.3.4',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(480,'2017-03-22 07:35:43','90','Exer3.3.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(481,'2017-03-22 07:47:43','94','Exer3.3.5',1,'[{\"added\": {}}]',2,2),(482,'2017-03-22 07:48:14','94','Exer3.3.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(483,'2017-03-22 07:48:40','94','Exer3.3.5',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(484,'2017-03-22 08:11:14','18','$\\phi_{X_n}(s)$',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(485,'2017-03-22 10:12:32','86','Prob3.9.10.a',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(486,'2017-03-23 02:07:20','95','Exer3.4.1',1,'[{\"added\": {}}]',2,2),(487,'2017-03-23 02:25:35','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(488,'2017-03-23 02:26:23','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(489,'2017-03-23 02:28:10','95','Exer3.4.1',2,'[]',2,2),(490,'2017-03-23 02:28:36','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(491,'2017-03-23 02:29:13','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(492,'2017-03-23 02:29:42','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(493,'2017-03-23 02:30:09','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(494,'2017-03-23 02:30:44','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(495,'2017-03-23 02:31:07','95','Exer3.4.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(496,'2017-03-23 03:28:32','96','Exer3.4.2',1,'[{\"added\": {}}]',2,2),(497,'2017-03-23 03:29:11','96','Exer3.4.2',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(498,'2017-03-23 03:37:05','96','Exer3.4.2',2,'[{\"changed\": {\"fields\": [\"messagefailure\", \"messagesuccess\", \"gussingparameter\"]}}]',2,2),(499,'2017-03-23 04:01:37','97','Exer3.4.3',1,'[{\"added\": {}}]',2,2),(500,'2017-03-23 04:02:43','97','Exer3.4.3',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"solutions\"]}}]',2,2),(501,'2017-03-23 04:04:37','97','Exer3.4.3',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(502,'2017-03-23 04:05:21','97','Exer3.4.3',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"solutions\"]}}]',2,2),(503,'2017-03-23 04:05:40','97','Exer3.4.3',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(504,'2017-03-23 04:06:14','97','Exer3.4.3',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\", \"choicesd\"]}}]',2,2),(505,'2017-03-23 04:06:38','97','Exer3.4.3',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(506,'2017-03-23 06:02:01','98','Exer3.4.4',1,'[{\"added\": {}}]',2,2),(507,'2017-03-23 06:02:56','97','Exer3.4.3',2,'[]',2,2),(508,'2017-03-23 06:03:40','98','Exer3.4.4',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(509,'2017-03-23 06:25:52','98','Exer3.4.4',2,'[]',2,2),(510,'2017-03-23 06:48:15','99','Exer3.4.5',1,'[{\"added\": {}}]',2,2),(511,'2017-03-23 06:54:46','100','Exer3.4.6',1,'[{\"added\": {}}]',2,2),(512,'2017-03-23 06:55:36','100','Exer3.4.6',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\"]}}]',2,2),(513,'2017-03-23 06:56:17','100','Exer3.4.6',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\"]}}]',2,2),(514,'2017-03-23 08:40:02','100','Exer3.4.6',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(515,'2017-03-23 08:40:35','100','Exer3.4.6',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesc\"]}}]',2,2),(516,'2017-03-23 08:40:52','100','Exer3.4.6',2,'[{\"changed\": {\"fields\": [\"difficulty\"]}}]',2,2),(517,'2017-03-23 13:45:23','99','Exer3.4.5',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\"]}}]',2,2),(518,'2017-03-24 12:55:55','1','1',1,'[{\"added\": {}}]',9,1),(519,'2017-03-24 12:55:58','2','2',1,'[{\"added\": {}}]',9,1),(520,'2017-03-24 12:56:00','3','3',1,'[{\"added\": {}}]',9,1),(521,'2017-03-24 12:56:02','4','4',1,'[{\"added\": {}}]',9,1),(522,'2017-03-24 12:56:03','5','5',1,'[{\"added\": {}}]',9,1),(523,'2017-03-24 12:56:05','6','6',1,'[{\"added\": {}}]',9,1),(524,'2017-03-24 12:56:50','34','Distribution and Density',2,'[]',1,1),(525,'2017-03-24 12:57:07','1','Connect object',1,'[{\"added\": {}}]',10,1),(526,'2017-03-24 12:57:15','2','Connect object',1,'[{\"added\": {}}]',10,1),(527,'2017-03-25 09:49:35','1','Users object',1,'[{\"added\": {}}]',11,1),(528,'2017-03-25 10:00:21','1','Users object',3,'',11,1),(529,'2017-03-25 10:03:11','2','Users object',1,'[{\"added\": {}}]',11,1),(530,'2017-03-25 11:51:46','3','Users object',1,'[{\"added\": {}}]',11,1),(531,'2017-03-25 11:52:08','4','Users object',1,'[{\"added\": {}}]',11,1),(532,'2017-03-25 11:52:41','5','Users object',1,'[{\"added\": {}}]',11,1),(533,'2017-03-25 12:01:37','6','Users object',1,'[{\"added\": {}}]',11,1),(534,'2017-03-25 12:05:07','7','Users object',1,'[{\"added\": {}}]',11,1),(535,'2017-03-25 12:05:09','7','Users object',2,'[]',11,1),(536,'2017-03-25 12:06:34','8','Users object',1,'[{\"added\": {}}]',11,2),(537,'2017-03-25 12:12:44','8','Users object',2,'[{\"changed\": {\"fields\": [\"token\"]}}]',11,1),(538,'2017-03-25 12:12:49','8','Users object',2,'[{\"changed\": {\"fields\": [\"login\"]}}]',11,1),(539,'2017-03-25 12:39:04','9','Users object',1,'[{\"added\": {}}]',11,1),(540,'2017-03-29 10:36:24','10','Users object',1,'[{\"added\": {}}]',11,2),(541,'2017-03-31 03:43:59','11','Users object',1,'[{\"added\": {}}]',11,2),(542,'2017-03-31 03:45:03','11','Users object',2,'[{\"changed\": {\"fields\": [\"login\"]}}]',11,2),(543,'2017-03-31 07:01:33','35','Regular M.C',1,'[{\"added\": {}}]',1,2),(544,'2017-03-31 07:03:20','35','Regular M.C',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(545,'2017-03-31 07:03:46','35','Regular M.C',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(546,'2017-03-31 07:35:27','36','A sufficient condition',1,'[{\"added\": {}}]',1,2),(547,'2017-03-31 07:35:43','36','A sufficient condition for regular MC',2,'[{\"changed\": {\"fields\": [\"title\"]}}]',1,2),(548,'2017-03-31 07:36:32','36','A sufficient condition for regular MC',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(549,'2017-03-31 07:42:16','35','Regular M.C',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(550,'2017-03-31 08:13:04','37','Long run behavior',1,'[{\"added\": {}}]',1,2),(551,'2017-03-31 08:26:44','37','Long run behavior',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(552,'2017-03-31 08:27:10','37','Long run behavior',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(553,'2017-03-31 08:36:37','39','$\\pi = \\pi P$',1,'[{\"added\": {}}]',1,2),(554,'2017-03-31 09:15:48','40','Space average and time average',1,'[{\"added\": {}}]',1,2),(555,'2017-03-31 09:16:23','40','Space average and time average',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',1,2),(556,'2017-04-01 04:19:20','101','Prob4.1.1',1,'[{\"added\": {}}]',2,2),(557,'2017-04-01 05:41:34','101','Prob4.1.1',2,'[]',2,2),(558,'2017-04-01 05:45:25','102','Prob4.1.4',1,'[{\"added\": {}}]',2,2),(559,'2017-04-01 05:46:38','102','Prob4.1.4',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(560,'2017-04-01 05:47:13','102','Prob4.1.4',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(561,'2017-04-01 05:47:29','102','Prob4.1.4',2,'[]',2,2),(562,'2017-04-01 05:48:28','101','Prob4.1.1',2,'[]',2,2),(563,'2017-04-01 05:48:38','102','Prob4.1.4',2,'[]',2,2),(564,'2017-04-01 06:08:08','103','Prob4.1.7',1,'[{\"added\": {}}]',2,2),(565,'2017-04-01 06:08:19','102','Prob4.1.4',2,'[{\"changed\": {\"fields\": [\"difficulty\"]}}]',2,2),(566,'2017-04-01 06:08:54','101','Prob4.1.1',2,'[]',2,2),(567,'2017-04-01 06:10:52','103','Prob4.1.7',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(568,'2017-04-01 07:02:28','104','Prob4.1.10',1,'[{\"added\": {}}]',2,2),(569,'2017-04-01 07:09:04','104','Prob4.1.10',2,'[]',2,2),(570,'2017-04-01 07:51:50','105','Prob4.1.13',1,'[{\"added\": {}}]',2,2),(571,'2017-04-01 07:52:38','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(572,'2017-04-01 07:53:00','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(573,'2017-04-01 07:56:15','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(574,'2017-04-01 07:57:48','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(575,'2017-04-01 07:58:00','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(576,'2017-04-01 07:59:06','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(577,'2017-04-01 07:59:45','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(578,'2017-04-01 08:00:12','105','Prob4.1.13',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(579,'2017-04-01 08:04:45','102','Prob4.1.4',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"choicese\", \"choicesf\"]}}]',2,2),(580,'2017-04-01 08:06:17','102','Prob4.1.4',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(581,'2017-04-01 08:13:32','102','Prob4.1.4',2,'[{\"changed\": {\"fields\": [\"alternativesolutions\"]}}]',2,2),(582,'2017-04-01 08:14:16','102','Prob4.1.4',2,'[{\"changed\": {\"fields\": [\"alternativesolutions\"]}}]',2,2),(583,'2017-04-01 09:32:38','106','Prob4.2.3',1,'[{\"added\": {}}]',2,2),(584,'2017-04-01 09:33:30','106','Prob4.2.3',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(585,'2017-04-01 10:03:49','107','Prob4.2.6',1,'[{\"added\": {}}]',2,2),(586,'2017-04-01 10:04:35','107','Prob4.2.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(587,'2017-04-01 10:05:34','107','Prob4.2.6',2,'[{\"changed\": {\"fields\": [\"choicesa\"]}}]',2,2),(588,'2017-04-01 12:59:04','108','Expl4.1.5',1,'[{\"added\": {}}]',2,2),(589,'2017-04-01 13:00:32','109','Expl4.1.6',1,'[{\"added\": {}}]',2,2),(590,'2017-04-01 13:01:59','110','Expl4.1.7',1,'[{\"added\": {}}]',2,2),(591,'2017-04-01 13:04:00','111','Expl4.1.8',1,'[{\"added\": {}}]',2,2),(592,'2017-04-01 13:06:01','112','Exer4.2.2',1,'[{\"added\": {}}]',2,2),(593,'2017-04-01 13:12:58','113','Exer4.2.3',1,'[{\"added\": {}}]',2,2),(594,'2017-04-01 13:14:24','114','Exer4.2.4',1,'[{\"added\": {}}]',2,2),(595,'2017-04-01 13:16:00','115','Exer4.2.5',1,'[{\"added\": {}}]',2,2),(596,'2017-04-01 13:17:44','116','Exer4.2.6',1,'[{\"added\": {}}]',2,2),(597,'2017-04-01 13:23:00','117','Exer4.2.7',1,'[{\"added\": {}}]',2,2),(598,'2017-04-01 13:24:32','118','Exer4.2.8',1,'[{\"added\": {}}]',2,2),(599,'2017-04-01 13:26:22','119','Exer4.3.1',1,'[{\"added\": {}}]',2,2),(600,'2017-04-01 13:27:56','120','Exer4.3.2',1,'[{\"added\": {}}]',2,2),(601,'2017-04-01 13:29:34','121','Exer4.3.3',1,'[{\"added\": {}}]',2,2),(602,'2017-04-01 13:31:24','122','Exer4.3.4',1,'[{\"added\": {}}]',2,2),(603,'2017-04-01 13:31:59','108','Expl4.1.5',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(604,'2017-04-01 13:32:19','109','Expl4.1.6',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(605,'2017-04-01 13:32:44','110','Expl4.1.7',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(606,'2017-04-01 13:34:40','112','Exer4.2.2',2,'[{\"changed\": {\"fields\": [\"problempicture1\", \"problempicture2\", \"problempicture3\", \"problempicture4\"]}}]',2,2),(607,'2017-04-01 13:35:19','113','Exer4.2.3',2,'[{\"changed\": {\"fields\": [\"problempicture1\", \"problempicture2\", \"problempicture3\", \"problempicture4\"]}}]',2,2),(608,'2017-04-01 13:36:05','114','Exer4.2.4',2,'[{\"changed\": {\"fields\": [\"problempicture1\", \"problempicture2\", \"problempicture3\", \"problempicture4\"]}}]',2,2),(609,'2017-04-01 13:36:34','117','Exer4.2.7',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(610,'2017-04-01 13:37:17','119','Exer4.3.1',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(611,'2017-04-01 13:37:49','120','Exer4.3.2',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(612,'2017-04-01 13:38:06','121','Exer4.3.3',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(613,'2017-04-01 13:38:21','122','Exer4.3.4',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(614,'2017-04-02 02:22:52','123','DIY4.1.1',1,'[{\"added\": {}}]',2,2),(615,'2017-04-02 02:25:47','123','DIY4.1.1',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(616,'2017-04-02 02:44:43','124','DIY4.2.2',1,'[{\"added\": {}}]',2,2),(617,'2017-04-02 07:26:11','125','DIY4.2.4',1,'[{\"added\": {}}]',2,2),(618,'2017-04-02 07:44:01','126','DIY4.3.2',1,'[{\"added\": {}}]',2,2),(619,'2017-04-02 07:44:57','126','DIY4.3.2',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(620,'2017-04-02 08:42:08','127','DIY3.1.9',1,'[{\"added\": {}}]',2,2),(621,'2017-04-02 09:16:16','128','DIY3.4.12',1,'[{\"added\": {}}]',2,2),(622,'2017-04-02 12:02:01','129','DIY3.4.14',1,'[{\"added\": {}}]',2,2),(623,'2017-04-02 12:15:42','129','DIY3.4.14',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(624,'2017-04-02 12:33:49','130','DIY3.4.16',1,'[{\"added\": {}}]',2,2),(625,'2017-04-02 13:08:08','131','DIY3.9.4',1,'[{\"added\": {}}]',2,2),(626,'2017-04-02 13:09:19','131','DIY3.9.4',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(627,'2017-04-02 13:10:15','131','DIY3.9.4',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(628,'2017-04-02 13:36:37','132','DIY3.9.6',1,'[{\"added\": {}}]',2,2),(629,'2017-04-02 13:39:55','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(630,'2017-04-02 13:40:38','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(631,'2017-04-02 13:41:29','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(632,'2017-04-02 13:41:58','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(633,'2017-04-02 13:43:26','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(634,'2017-04-02 13:44:02','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(635,'2017-04-02 13:44:27','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(636,'2017-04-02 13:45:39','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(637,'2017-04-02 13:46:20','132','DIY3.9.6',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(638,'2017-04-03 07:38:22','133','Prob4.3.1',1,'[{\"added\": {}}]',2,2),(639,'2017-04-03 07:40:10','133','Prob4.3.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(640,'2017-04-03 07:42:07','133','Prob4.3.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(641,'2017-04-03 07:42:56','133','Prob4.3.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(642,'2017-04-03 07:44:08','133','Prob4.3.1',2,'[{\"changed\": {\"fields\": [\"problem\", \"solutions\"]}}]',2,2),(643,'2017-04-03 07:44:30','133','Prob4.3.1',2,'[{\"changed\": {\"fields\": [\"problem\"]}}]',2,2),(644,'2017-04-03 07:46:39','133','Prob4.3.1',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(645,'2017-04-03 07:50:51','9','Prob3.4.5',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"choicese\"]}}]',2,2),(646,'2017-04-03 07:51:53','71','Prob3.4.6',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\"]}}]',2,2),(647,'2017-04-03 07:54:24','84','Prob3.9.1',2,'[{\"changed\": {\"fields\": [\"choicesb\", \"choicesd\"]}}]',2,2),(648,'2017-04-03 08:54:35','41','With states Rewards',1,'[{\"added\": {}}]',1,2),(649,'2017-04-03 08:55:29','42','Coupling',1,'[{\"added\": {}}]',1,2),(650,'2017-04-03 08:56:11','43','Include History',1,'[{\"added\": {}}]',1,2),(651,'2017-04-03 08:59:42','134','DIY4.1.2',1,'[{\"added\": {}}]',2,2),(652,'2017-04-03 09:00:11','134','DIY4.1.2',2,'[{\"changed\": {\"fields\": [\"problempicture1\"]}}]',2,2),(653,'2017-04-03 09:19:45','135','DIY4.1.4',1,'[{\"added\": {}}]',2,2),(654,'2017-04-03 09:48:16','134','DIY4.1.2',2,'[{\"changed\": {\"fields\": [\"choicesa\", \"choicesb\", \"choicesc\", \"choicesd\", \"answer\", \"solutions\"]}}]',2,2),(655,'2017-04-03 09:49:37','135','DIY4.1.4',2,'[{\"changed\": {\"fields\": [\"solutions\"]}}]',2,2),(656,'2017-04-03 10:12:08','136','DIY4.1.6',1,'[{\"added\": {}}]',2,2),(657,'2017-04-03 11:38:07','137','DIY4.2.6',1,'[{\"added\": {}}]',2,2),(658,'2017-04-03 12:05:01','138','DIY4.3.4',1,'[{\"added\": {}}]',2,2),(659,'2017-04-04 09:05:39','3','Connect object',1,'[{\"added\": {}}]',10,1),(660,'2017-04-04 09:19:24','1&2','1&2',1,'[{\"added\": {}}]',9,1),(661,'2017-04-04 09:20:45','23','Events and Probability',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(662,'2017-04-04 09:21:34','26','Slice Universe',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(663,'2017-04-04 09:21:55','25','Conditional Probability',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(664,'2017-04-04 09:22:06','24','Random Variable',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(665,'2017-04-04 09:22:16','29','Central Limit Theory',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(666,'2017-04-04 09:22:28','32','Independence of random variables',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(667,'2017-04-04 09:23:13','28','Independence of Events',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(668,'2017-04-04 09:24:15','7','Conditional Independence',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(669,'2017-04-04 09:24:54','44','Conditional Independence',1,'[{\"added\": {}}]',1,2),(670,'2017-04-04 09:25:32','27','Bayes\' Rule',2,'[{\"changed\": {\"fields\": [\"title\", \"chapter\"]}}]',1,2),(671,'2017-04-04 09:25:57','9','First Step Analysis',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(672,'2017-04-04 09:26:10','33','Correlation',2,'[{\"changed\": {\"fields\": [\"title\", \"chapter\"]}}]',1,2),(673,'2017-04-04 09:26:21','30','Joint Distribution',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(674,'2017-04-04 09:26:31','31','Mean and Variance',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(675,'2017-04-04 09:26:51','34','Distribution and Density',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(676,'2017-04-04 09:28:02','2','Discrete MC',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(677,'2017-04-04 09:28:10','3','Markov Chain',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(678,'2017-04-04 09:28:21','4','Markov Process',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(679,'2017-04-04 09:28:47','5','N-step Transition',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(680,'2017-04-04 09:32:54','45','First step analysis',1,'[{\"added\": {}}]',1,2),(681,'2017-04-04 09:33:06','10','$P^n$',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(682,'2017-04-04 09:33:38','12','Calculate the Probability of an Event',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(683,'2017-04-04 09:33:50','13','Calculate Mean Steps to a State',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(684,'2017-04-04 09:34:08','14','Branching Process',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(685,'2017-04-04 09:34:22','17','$\\xi^{(n)} iid$',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(686,'2017-04-04 09:34:34','18','$\\phi_{X_n}(s)$',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(687,'2017-04-04 09:34:42','19','$\\phi_{\\xi}(s)$',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(688,'2017-04-04 09:34:51','20','Probability Genarating Function',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(689,'2017-04-04 09:34:59','21','Extinction Probability',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(690,'2017-04-04 09:35:08','22','Eventual Extinction',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(691,'2017-04-04 09:35:38','16','Mean,Varance in Branching Process',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(692,'2017-04-04 09:45:26','37','Long run behavior-4',2,'[{\"changed\": {\"fields\": [\"names\"]}}]',1,2),(693,'2017-04-04 09:48:13','46','Application-4',1,'[{\"added\": {}}]',1,2),(694,'2017-04-04 09:48:36','37','Long run behavior-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"bolder\"]}}]',1,2),(695,'2017-04-04 09:49:22','39','$\\pi = \\pi P$-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(696,'2017-04-04 09:50:02','40','Space average and time average-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(697,'2017-04-04 09:52:56','47','$\\pi_k=\\frac{1}{E(T|X_0=k)}$-4',1,'[{\"added\": {}}]',1,2),(698,'2017-04-04 09:56:34','35','Regular M.C-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(699,'2017-04-04 09:57:11','36','A sufficient condition for regular MC-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(700,'2017-04-04 09:57:35','46','Application-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(701,'2017-04-04 09:58:10','2','Markov Chain-3-First Step Analysis-1&2',3,'',10,1),(702,'2017-04-04 09:58:28','41','With states Rewards-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(703,'2017-04-04 09:59:14','42','Coupling-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(704,'2017-04-04 09:59:34','4','Long run behavior-4-$\\pi = \\pi P$-4',1,'[{\"added\": {}}]',10,1),(705,'2017-04-04 09:59:53','43','Include History-4',2,'[{\"changed\": {\"fields\": [\"x\", \"y\", \"names\"]}}]',1,2),(706,'2017-04-04 10:00:05','5','Regular M.C-4-Long run behavior-4',1,'[{\"added\": {}}]',10,1),(707,'2017-04-04 10:00:26','6','A sufficient condition for regular MC-4-Regular M.C-4',1,'[{\"added\": {}}]',10,1),(708,'2017-04-04 10:01:24','7','Application-4-With states Rewards-4',1,'[{\"added\": {}}]',10,1),(709,'2017-04-04 10:01:35','7','Application-4-With states Rewards-4',3,'',10,1),(710,'2017-04-04 10:01:56','8','Space average and time average-4-$\\pi_k=\\frac{1}{E(T|X_0=k)}$-4',1,'[{\"added\": {}}]',10,2),(711,'2017-04-04 10:02:16','9','Application-4-Coupling-4',1,'[{\"added\": {}}]',10,1),(712,'2017-04-04 10:02:27','10','Long run behavior-4-Application-4',1,'[{\"added\": {}}]',10,2),(713,'2017-04-04 10:02:37','11','Application-4-Include History-4',1,'[{\"added\": {}}]',10,1),(714,'2017-04-04 10:02:51','12','Application-4-With states Rewards-4',1,'[{\"added\": {}}]',10,2),(715,'2017-04-04 10:03:12','4','Long run behavior-4-$\\pi = \\pi P$-4',2,'[{\"changed\": {\"fields\": [\"detail\"]}}]',10,2),(716,'2017-04-04 10:03:20','3','Long run behavior-4-Space average and time average-4',2,'[]',10,2),(717,'2017-04-04 10:03:42','15','Calculate Mean Number of Visits to a State-1&2',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(718,'2017-04-04 10:03:49','6','One-step Transition P-1&2',2,'[{\"changed\": {\"fields\": [\"chapter\"]}}]',1,2),(719,'2017-04-04 10:04:41','2','2',3,'',9,1),(720,'2017-04-04 10:04:45','1','1',3,'',9,1),(721,'2017-04-04 10:06:33','48','Application-3',1,'[{\"added\": {}}]',1,2);
/*!40000 ALTER TABLE `django_admin_log` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_content_type`
--

DROP TABLE IF EXISTS `django_content_type`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_content_type` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `app_label` varchar(100) NOT NULL,
  `model` varchar(100) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `django_content_type_app_label_76bd3d3b_uniq` (`app_label`,`model`)
) ENGINE=InnoDB AUTO_INCREMENT=14 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_content_type`
--

LOCK TABLES `django_content_type` WRITE;
/*!40000 ALTER TABLE `django_content_type` DISABLE KEYS */;
INSERT INTO `django_content_type` VALUES (3,'admin','logentry'),(6,'auth','group'),(4,'auth','permission'),(5,'auth','user'),(7,'contenttypes','contenttype'),(9,'mathematics','chapter'),(10,'mathematics','connect'),(1,'mathematics','neuron'),(2,'mathematics','question'),(12,'mathematics','userneuron'),(13,'mathematics','userquestion'),(11,'mathematics','users'),(8,'sessions','session');
/*!40000 ALTER TABLE `django_content_type` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_migrations`
--

DROP TABLE IF EXISTS `django_migrations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_migrations` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `app` varchar(255) NOT NULL,
  `name` varchar(255) NOT NULL,
  `applied` datetime NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=25 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_migrations`
--

LOCK TABLES `django_migrations` WRITE;
/*!40000 ALTER TABLE `django_migrations` DISABLE KEYS */;
INSERT INTO `django_migrations` VALUES (1,'contenttypes','0001_initial','2017-03-15 15:24:13'),(2,'auth','0001_initial','2017-03-15 15:24:14'),(3,'admin','0001_initial','2017-03-15 15:24:14'),(4,'admin','0002_logentry_remove_auto_add','2017-03-15 15:24:14'),(5,'contenttypes','0002_remove_content_type_name','2017-03-15 15:24:14'),(6,'auth','0002_alter_permission_name_max_length','2017-03-15 15:24:14'),(7,'auth','0003_alter_user_email_max_length','2017-03-15 15:24:14'),(8,'auth','0004_alter_user_username_opts','2017-03-15 15:24:14'),(9,'auth','0005_alter_user_last_login_null','2017-03-15 15:24:14'),(10,'auth','0006_require_contenttypes_0002','2017-03-15 15:24:14'),(11,'auth','0007_alter_validators_add_error_messages','2017-03-15 15:24:14'),(12,'auth','0008_alter_user_username_max_length','2017-03-15 15:24:14'),(13,'sessions','0001_initial','2017-03-15 15:24:14'),(14,'mathematics','0001_initial','2017-03-15 15:24:26'),(15,'mathematics','0002_auto_20170315_1527','2017-03-15 15:27:32'),(16,'mathematics','0003_auto_20170315_1537','2017-03-15 15:37:20'),(17,'mathematics','0004_auto_20170317_0924','2017-03-17 09:24:06'),(18,'mathematics','0005_auto_20170317_0926','2017-03-17 09:26:11'),(19,'mathematics','0006_auto_20170317_0928','2017-03-17 09:28:59'),(20,'mathematics','0007_auto_20170324_1238','2017-03-24 12:58:51'),(21,'mathematics','0008_auto_20170324_1318','2017-03-24 13:32:33'),(22,'mathematics','0009_auto_20170325_1211','2017-03-25 12:11:26'),(23,'mathematics','0010_auto_20170404_0937','2017-04-04 09:37:29'),(24,'mathematics','0011_auto_20170404_0945','2017-04-04 09:45:03');
/*!40000 ALTER TABLE `django_migrations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `django_session`
--

DROP TABLE IF EXISTS `django_session`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `django_session` (
  `session_key` varchar(40) NOT NULL,
  `session_data` longtext NOT NULL,
  `expire_date` datetime NOT NULL,
  PRIMARY KEY (`session_key`),
  KEY `django_session_de54fa62` (`expire_date`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `django_session`
--

LOCK TABLES `django_session` WRITE;
/*!40000 ALTER TABLE `django_session` DISABLE KEYS */;
INSERT INTO `django_session` VALUES ('10cwd3j9ipgy35ow19xsyizpvbqaqlal','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-12 04:53:18'),('1xkcmj4456fcxvq1t6y7t6c8vxesmxgc','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-03-30 08:17:07'),('33flt2g51i0pl1wq928wmwt1jft2qfrt','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-12 10:26:29'),('3dame42kbfut1y67v5d2w6vbo5ljvx0i','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-12 10:14:33'),('3t9acmv53h9128ghuxp0381bnm5nieji','ZGViYjY2ZWNhN2E1Mzk0NTdjNmQ0NTdmNmRhMzJiZmI0NDk2ZWE2ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjQ0MzE0YTJhNjE2NjEzNzM3NjE3ZGJjYzA5OTcwNTk3N2U5Y2JlOWMiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-04-18 07:29:25'),('4w1y2u6u2gdcjvtv1sutkrfgbxoo26a3','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-03-30 08:14:56'),('51h9jxcfdbdrc2hgfrjadns1i7n62mzl','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-03 03:06:56'),('5541g8znjma8p4w4zp7qyyiy8ok4x5s6','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-03-30 02:45:41'),('5mzq955jtphatuyr0r4ofzlfp48jtlpk','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-12 10:26:30'),('5z2akvnxi8tpevhvqm6ydabc5gizi54l','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-01 09:10:27'),('7b4vjivi6kotj96waimbhacptjjch0c6','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-03 05:20:56'),('87u45py8hies3nd1jm8lbdaj1lnc7gb9','ZGViYjY2ZWNhN2E1Mzk0NTdjNmQ0NTdmNmRhMzJiZmI0NDk2ZWE2ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjQ0MzE0YTJhNjE2NjEzNzM3NjE3ZGJjYzA5OTcwNTk3N2U5Y2JlOWMiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-03-29 15:31:27'),('abihaygs4z5ktott21y0nkhv39hc6tje','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-12 10:34:55'),('f44wfttnkfyo6t3l8nybsde5kez8swn5','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-01 08:54:09'),('faqx53pyz2474vxby31jra8w1yrxphcc','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-17 08:49:48'),('fl948vo4nvtj226vhciyo57hr0zw76hv','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-02 03:31:01'),('fpju0op8d7xlab9d1ogtsd5hdw0elu7w','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-03-31 11:54:51'),('gvp8b5dyh2unflnhv0icw30l93a424if','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-05 02:54:21'),('nmsbhgded1m6ivn322llreovr24vkasv','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-06 04:52:18'),('oghuiila9zmv58ioo16admwyinhrvzzd','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-13 06:11:46'),('qbcnfcfl99yv0ctqoz0pe4rglflhuhtx','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-04 20:13:45'),('slv45qr8kwfkf9m1wiyum60fsbjijj6d','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-02 10:39:38'),('tp7sjnqldz43cc2n8qj60oep2y2mb875','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-03-30 08:17:11'),('u851sgxmhchvqaap2ucl0naxniau7aba','ZGViYjY2ZWNhN2E1Mzk0NTdjNmQ0NTdmNmRhMzJiZmI0NDk2ZWE2ZTp7Il9hdXRoX3VzZXJfaGFzaCI6IjQ0MzE0YTJhNjE2NjEzNzM3NjE3ZGJjYzA5OTcwNTk3N2U5Y2JlOWMiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIxIn0=','2017-04-08 11:45:51'),('uurkxdtlggbezdqw3eg2qtueuorfp6lt','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-03-31 06:33:56'),('x2ggra3y085ilofo40uhaykzwqqw5q85','NTQ2Y2Y0MmM4NTFjODBhOWYwMzA2YmQ1ZjcyMTI4OWFmZmFhOGE3NDp7Il9hdXRoX3VzZXJfaGFzaCI6IjUyMjMzOTMyMjNmMWM1ZjU3Mzk0ZDI2YzgzMGJiZTI4ZWUxY2NiNzkiLCJfYXV0aF91c2VyX2JhY2tlbmQiOiJkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZCIsIl9hdXRoX3VzZXJfaWQiOiIyIn0=','2017-04-18 06:13:02');
/*!40000 ALTER TABLE `django_session` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_chapter`
--

DROP TABLE IF EXISTS `mathematics_chapter`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_chapter` (
  `id` varchar(100) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_chapter`
--

LOCK TABLES `mathematics_chapter` WRITE;
/*!40000 ALTER TABLE `mathematics_chapter` DISABLE KEYS */;
INSERT INTO `mathematics_chapter` VALUES ('1&2'),('3'),('4'),('5'),('6');
/*!40000 ALTER TABLE `mathematics_chapter` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_connect`
--

DROP TABLE IF EXISTS `mathematics_connect`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_connect` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `detail` varchar(200) DEFAULT NULL,
  `begin_id` int(11) NOT NULL,
  `ending_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  KEY `mathematics_connect_begin_id_d1578d92_fk_mathematics_neuron_id` (`begin_id`),
  KEY `mathematics_connect_ending_id_a3fd9223_fk_mathematics_neuron_id` (`ending_id`),
  CONSTRAINT `mathematics_connect_begin_id_d1578d92_fk_mathematics_neuron_id` FOREIGN KEY (`begin_id`) REFERENCES `mathematics_neuron` (`id`),
  CONSTRAINT `mathematics_connect_ending_id_a3fd9223_fk_mathematics_neuron_id` FOREIGN KEY (`ending_id`) REFERENCES `mathematics_neuron` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_connect`
--

LOCK TABLES `mathematics_connect` WRITE;
/*!40000 ALTER TABLE `mathematics_connect` DISABLE KEYS */;
INSERT INTO `mathematics_connect` VALUES (1,'1',2,3),(3,'interpretataion',37,40),(4,'calculation',37,39),(5,'',35,37),(6,'',36,35),(8,'',40,47),(9,'',46,42),(10,'',37,46),(11,'',46,43),(12,'',46,41);
/*!40000 ALTER TABLE `mathematics_connect` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_neuron`
--

DROP TABLE IF EXISTS `mathematics_neuron`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_neuron` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(200) NOT NULL,
  `detail` longtext NOT NULL,
  `a` double NOT NULL,
  `b` double NOT NULL,
  `x` double NOT NULL,
  `y` double NOT NULL,
  `chapter_id` varchar(100) NOT NULL,
  `bolder` int(11),
  `fonts` varchar(100) DEFAULT NULL,
  `names` varchar(200) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=49 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_neuron`
--

LOCK TABLES `mathematics_neuron` WRITE;
/*!40000 ALTER TABLE `mathematics_neuron` DISABLE KEYS */;
INSERT INTO `mathematics_neuron` VALUES (2,'Discrete MC','1. Statement:\r\n<br>\r\nFor a discrete time $MC$:  $\\{X_n: n =0, 1, ...\\}$: the defining equation is,\r\nfor any $n \\geq 0$,\r\n\\begin{eqnarray*}\r\n	&& P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) \\\\\r\n	&=&\r\n	P(X_{n+1}=j |X_n=i).\r\n\\end{eqnarray*}\r\n2. Remark:  \r\n <br>\r\nDiscrete time $MC$: the time domain is $\\{0, 1, 2, ...\\}$. Continuous time $MC$:the time domain is $[0, \\infty)$, for example, Poisson process is continuous time $MC$.Brownian motion is a continuous time Markov process (but not $MC$).',1,1,1,1,'3',NULL,'1','1'),(3,'Markov Chain','1.  Statement:\r\n<br>\r\n	Markov chain ($MC$) is the Markov process  with discrete state space. Discrete state space is usually denoted by numbers $0, 1, 2, ....$.\r\n<br>\r\n2.  Remark: \r\n<br>\r\nDiscrete time $MC$: the time domain is $\\{0, 1, 2, ...\\}$. Continuous time $MC$:\r\nthe time domain is $[0, \\infty)$.',1,1,1,1,'3',NULL,'1','1'),(4,'Markov Process','1. Statement:\r\n<br>\r\nA stochastic process $\\{X_t\\}$ indexed by time $t$ such that, at each\r\ntime $t$, the future of the process $\\{X_s: s > t\\}$  is conditionally independent of\r\nthe past of the process $\\{X_s: s < t\\}$ given the present of the process $X_t$ taking any\r\nfixed value. Another interpretation is: at each\r\ntime $t$, the future of the process $\\{X_s: s > t\\}$  depends on the\r\nthe past of the process $\\{X_s: s < t\\}$ only through the present $X_t$.\r\n2. Remark:  \r\n<br>\r\n$\\{X_s: s> t\\}$ is in general not independent of $\\{X_s: s < t\\}$.',1,1,1,1,'3',NULL,'1','1'),(5,'N-step Transition','1. Statement:\r\n <br>\r\nDenote the $n$-step transition  probability as\r\n$P_{ij}^{(n)} = P(X_{n+k}=j|X_k=i)$\r\nand the $n$-step transition probability matrix as\r\n$$ \\bf{P}^{(n)}  = \\Biggl( \\quad P_{ij}^{(n)} \\quad \\Biggr) .$$\r\nFor notational convenience, we always let\r\n$$P^{(1)}_{ij} = P_{ij},   \\qquad P_{ij}^{(0)} = \\cases{ 1 & if $i=j$ \\cr\r\n	0 & if $i\\not=j$. \\cr\r\n}$$\r\n 2.  Remark:  \r\n<br>\r\n For all $0 \\leq m \\leq n$ and $n\\geq 0$,\r\n \\begin{eqnarray*}\r\n 		P_{ij}^{(n)} &=& \\sum_{l=0}^\\infty P_{il}^{(m)} P_{lj}^{(n-m)} \\\\\r\n 		\\bf{P}^{(n)} &=& \\bf{P}^n .\r\n \\end{eqnarray*}',1,1,1,1,'3',NULL,'1','1'),(6,'One-step Transition P','1.Statement:\r\n<br>\r\nLet $P_{ij} = P(X_{n+1}=j|X_n=i)$. The one-step transition probability matrix, or it transition matrix in brief,  is\r\n$$ \\qquad\\,\\, 0 \\qquad 1 \\qquad 2\\,\\,\\,\\,\\,\\,\\,\\,\\,\\, ... $$\r\n\\begin{eqnarray*}\r\n	{\\bf P} = \\matrix{0 \\cr 1 \\cr 2 \\cr\r\n		\\vdots} \\pmatrix{\r\n		P_{00} & P_{01} & P_{02} & ... \\cr\r\n		P_{10} & P_{11} & P_{12} & ...\\cr\r\n		P_{20} & P_{21} & P_{22} & ...\\cr\r\n		\\vdots & \\vdots & \\vdots & \\ddots}\r\n\\end{eqnarray*}\r\n \r\n   \r\n2.  Remark:  \r\n<br>\r\nIf one-step transition probability is irrelevant with $n$, i.e., it\'s the same for all $n$, we call the $MC$ $\\{X_t\\}$ a $MC$ with stationary transition probabilities.\r\nThroughout the course, we only consider $MC$ with stationary transition probabilities.',1,1,1,1,'1&2',NULL,'1','1'),(7,'Conditional Independence','1. Statement:\r\n <br>\r\n Two events A and B are conditionally independence given an event C with $P(C)>0$ if\r\n \r\n   $$P(A\\cup{B}|C) = P(A|B) P(B|C)$$\r\n \r\n   \r\n2.Remark:  \r\n <br>\r\nRecall that from the conditional probability,$$P(A|B)=\\frac{P(A\\cup{B})}{P(B)},$$\r\nif $P(B)>0$. By the conditioning on C, we obtain$$P(A|B,C)=\\frac{P(A\\cup{B}|C)}{P(B|C)},$$\r\nif $P(B|C)>0,P(C)\\not=0$.If A and B conditionally independent given on C, we obtain$$P(A|B,C)=\\frac{P(A\\cup{B}|C)}{P(B|C}=\\frac{P(A|C)P(B|C)}{P(B|C)}=P(A|C).$$\r\nThus,if A and B are conditionally independent given C, then$$P(A|B,C)=P(A|C),$$ which is the equivalent statement of the definition of conditional independence.',1,1,1,1,'1&2',NULL,'1','1'),(9,'First Step Analysis','1.Statement:\r\n<br>\r\n		The method of first-step analysis, is the method used in the example below, for the purpose of demonstrating a easier way of solving the problem.\r\n		For that particular problem, it is actually an easier method without invoking	the {\\it MC}  $\\{X_n, n\\geq 1 \\}$, but, rather, directly based on $\\{\\xi_i, i \\geq 1\\}$\r\n<br>\r\n2.Remark:  \r\n<br>\r\nRepeatedly tossing a fair coin.\r\nThere are 8 patterns of length 3:\r\n\r\n	$$HHH,  HHT, HTH, HTT, THH, THT, TTH, TTT$$\r\n\r\nTwo men, called  Yugong (Y) and Zhishou (Z),  are betting on whose choice\r\nof pattern would occur first. Mr Z  ``generously\"  allows\r\nMr Y to pick his favorite pattern first and, afterwards, he picks his own.\r\nWhatever Mr Y picks,  Mr Z, being ``Zhishou\", somehow\r\nalways beats him by a  chance at least 2/3! (This time, even infinite offspring\r\nto continue the game can\'t help Yugong.)\r\n<br>\r\n\r\n\r\nLet $X_n, n \\geq 2$ denote the pattern of length 2 of tosses\r\n$n-1$ and $n$.\r\n<br>\r\nThen $\\{X_n: n \\geq 2\\}$ is a $MC$  with state space $\\{$HH , HT, TH, TT$\\}$,\r\nwhich is accordingly denoted as $\\{0, 1, 2, 3\\}$, and transition matrix\r\n$$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n\\pmatrix{\r\n	1/2 &  1/2  &     &              \\cr\r\n	&     &  1/2  &  1/2            \\cr\r\n	1/2 &  1/2   &     &             \\cr\r\n	&      &  1/2   &   1/2\r\n}\r\n$$\r\nSuppose Mr Y\'s pick is HTH. Mr Z can pick HHT  to beat Mr Y\r\nby a chance $2/3$. The following is the proof using first-step analysis.\r\n<br>\r\nLet $p_i $ be the probability that pattern HTH occurs before HHT given\r\n$X_2=i$, $i=0, 1, 2, 3$.\r\n<br>\r\nThen\r\n\\begin{eqnarray*}\r\n	p_0 &=& 1/2p_0 + 1/2 \\times 0 \\\\\r\n	p_1 &=& 1/2 \\times 1 + 1/2 p_3 \\\\\r\n	p_2 &=& 1/2 p_0 + 1/2p_1 \\\\\r\n	p_3 &=& 1/2 p_2 + 1/2 p_3\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$p_0 = 0 \\quad p_1 = 2/3 \\quad p_2=p_3=1/3$$\r\nThen, the chance that Mr Y wins is\r\n$$ (p_0 + p_1 +p_2+p_3)/4 = 1/3,$$\r\nand Mr Z wins with chance $2/3$.',1,1,1,1,'1&2',NULL,'1','1'),(10,'$P^n$','1.Statement:\r\n<br>\r\n For all $0 \\leq m \\leq n$ and $n\\geq 0$,\r\n \\begin{eqnarray*}\r\n 		P_{ij}^{(n)} &=& \\sum_{l=0}^\\infty P_{il}^{(m)} P_{lj}^{(n-m)} \\\\\r\n 		\\bf{P}^{(n)} &=& \\bf{P}^n .\r\n \\end{eqnarray*} \r\n2. Remark:  \r\n<br>\r\nProof: Write\r\n \\begin{eqnarray*}\r\n 	P_{ij}^{(n)} &=& P(X_n=j|X_0=i) = P(X_n=j, X_0=i)/P(X_0=i)\r\n 	\\\\\r\n 	&=& \\sum_{l=0}^\\infty P(X_n=j, X_m=l, X_0=i)/P(X_0=i)\r\n 	\\\\\r\n 	&=& \\sum_{l=0}^\\infty P(X_n=j | X_m=l, X_0=i)P(X_m=l, X_0=i)/P(X_0=i)\r\n 	\\\\\r\n 	&=& \\sum_{l=0}^\\infty P(X_n=j | X_m=l, X_0=i)P(X_m=l| X_0=i)\r\n 	\\\\\r\n 	&=& \\sum_{l=0}^\\infty P(X_n=j | X_m=l) P(X_m=l, X_0=i)/P(X_0=i)\r\n 	\\\\\r\n 	&=& \\sum_{l=0}^\\infty P_{lj}^{(n-m)} P_{il}^{(m)}\r\n \\end{eqnarray*}\r\n Observe that\r\n $P_{ij}^{(n)}$ is the $(i+1, j+1)$-th entry of the matrix $\\bf{}^{(n)}$,\r\n $P_{il}^{(m)}, l=0,1,2,...$ are the $(i+1)$-th row of the matrix  $\\bf{P}^{(m)}$,\r\n and\r\n $P_{lj}^{(n-m)}, l=0,1,2,...$ are the $(j+1)$-th column of the matrix $\\bf{P}^{(n-m)}$.\r\n Hence,\r\n $$\\bf{P}^{(n)} = \\bf{P}^{(m)} \\bf{P}^{(n-m)}$$ for all $0 \\leq m \\leq n$.\r\n Since $\\bf{P}^{(1)} = \\bf{P}$, we have\r\n $$\\bf{P}^{(2)} = \\bf{P} \\bf{P} = \\bf{P}^2, \\quad\r\n \\bf{P}^{(3)} = \\bf{P} \\bf{P}^{(2)} = \\bf{P}^3 , \\quad ... \\quad\r\n \\bf{P}^{(n)} = \\bf{P}^n$$\r\n by induction.',1,1,1,1,'3',NULL,'1','1'),(12,'Calculate the Probability of an Event','1. Statement:\r\n<br>\r\n		The method of first-step analysis, can be used in the example below, for the purpose of demonstrating a easier way of solving the problem for the probability of an event.\r\n<br>\r\n2. Remark:  \r\n<br>\r\n(Mickey in Maze continued from Example 3.2).(i) Compute the probability Mickey goes to cell 2 before it reaches cell 6, beginning from cell 1.\r\n<br>\r\nSol: Set\r\n$$p_i = P(\\hbox{Mickey reaches cell 2 before reaching 6 } | \\hbox{ starting from i}).$$\r\nThen, for example,\r\n\\begin{eqnarray*}\r\n	p_0 &=& P_{01}p_1 + P_{03} p_3 = 1/2 p_1 + 1/2 p_3  \\\\\r\n	p_1 &=& 1/3p_0  + 1/3 p_4 + 1/3 p_2 = 1/3(p_0 +p_4) + 1/3 \\\\\r\n	p_2 &=& 1  \\\\\r\n	p_3 &=& 1/3 p_0 + 1/3 p_4 + 1/3p_6 = 1/3(p_0 +p_4) \\\\\r\n	p_4 &=& 1/4 p_1 + 1/4 p_3 + 1/4p_5 + 1/3p_7 \\\\\r\n	p_5 &=& 1/3 p_2 + 1/3 p_4 + 1/3p_8 = 1/3(p_0 +p_8) +1/3 \\\\\r\n	p_6 &=& 0 \\\\\r\n	p_7 &=& 1/3 p_4 + 1/3 p_6 + 1/3p_8 = 1/3(p_4 +p_8) \\\\\r\n	p_8 &=& 1/2 p_5 + 1/2 p_7\r\n\\end{eqnarray*}\r\nThe above linear equations can be solved for the answer\r\n$$ p_0=p_8=1/2, \\quad p_1=p_5 = 2/3, \\quad p_3=p_7=1/3 \\quad\r\np_4=1/2.$$\r\n\r\nIn fact, a short-cut to solving the eight equations is, by the\r\nsymmetry of the maze, to realize that $p_0=p_8$, $p_1=p_5$, $p_3=p_7$.\r\nTogether with the fact that $p_2=1$ and $p_6=0$, the number of\r\nequations can be immediately reduced to four about $p_0, p_1, p_3$\r\nand $p_4$.',1,1,1,1,'3',NULL,'1','1'),(13,'Calculate Mean Steps to a State','1.Statement:\r\n<br>\r\n		The method of first-step analysis, can be used in the example below, for the purpose of demonstrating a easier way of solving the problem, like the mean number of steps to a state. \r\n<br>\r\n2. Remark:  \r\n<br>\r\n(Mickey in Maze continued from Example 3.2).(ii). Compute the mean number of steps to reach cells 2 or 6, beginning from cell 4.\r\n<br>\r\nSol: Set $w_i$ the mean number of steps to reach 2 or 6, starting from\r\ncell $i$. Then, obviously, $w_2=w_6=0$. By symmetry, $w_0=w_8$, $w_1=w_3=w_5=w_7$.\r\n<br>\r\nMoreover,\r\n\\begin{eqnarray*}\r\n	w_0 &=& 1 + 1/2(w_1 + w_3) = 1 + w_1 \\\\\r\n	w_1 &=& 1+ 1/3(w_0+w_2+w_4)= 1 + 1/3(w_0 + w_4) \\\\\r\n	w_4 &=& 1+ 1/4(w_1+w_3+w_5+w_7) = 1+w_1\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=6\\quad \\hbox{and} \\quad w_1=w_3=w_5=w_7=5.$$',1,1,1,1,'3',NULL,'1','1'),(14,'Branching Process','1.Statement:\r\n<br>\r\n		Suppose every member of\r\n		a population produces its next generation, independent of\r\n		others, according to a common distribution  (i.i.d). Let $X_n$ be the size\r\n		of the $n$-th generation of the population, $n=0, 1, 2, ....$\r\n		Then, $\\{X_n: n \\geq 0\\}$ is a branching process.\r\n<br>\r\n\r\n2.Remark:  \r\n<br>\r\n		Let $\\xi$ be the random variable of the number of the next generation of\r\n		a typical individual in this population. Set\r\n		$$p_k = P(\\xi = k), \\qquad k=0, 1, 2, ...$$\r\n		Let $\\xi_i^{(n)}$ be the size of the next generation of the $i$-th  member\r\n		of the $n$-th generation of this population. Then,\r\n		$$ X_{n+1} = \\xi^{(n)}_0 + \\xi^{(n)}_1+ \\cdots+ \\xi^{(n)}_{X_n}.$$\r\n		Here, for notational convenience, we let   $\\xi^{(n)}_0\\equiv 0$.\r\n		Clearly, $X_{n+1}=0$ if  $X_n=0$.\r\n		The process $\\{X_n: n \\geq 0\\}$ is called a branching process.',1,1,1,1,'3',NULL,'1','1'),(15,'Calculate Mean Number of Visits to a State','1. Statement:\r\n<br>\r\n		The method of first-step analysis,can be used in the example below, for the purpose of demonstrating a easier way of solving the problem: the mean number of visits of a state.\r\n<br>\r\n2 Remark:  \r\n<br>\r\n(Mickey in Maze continued from Example 3.2).(iii). Compute the mean number of times Mickey visits cell 2 before reaching cell 6, starting from cell 4.\r\n<br>\r\nSol:Set $w_i$ the mean number of visits of cell 2 before reaching cell 6, starting\r\nfrom cell $i$ (including the starting state).\r\n<br>\r\nThen, obviously, $w_6=0$. By symmetry,\r\n$w_0=w_8$, $w_1=w_5$ and $w_3= w_7$.\r\n<br>\r\nMoreover,\r\n\\begin{eqnarray*}\r\n	w_0 &=&  1/2(w_1 + w_3)   \\\\\r\n	w_1 &=&  1/3(w_0+w_2+w_4)  \\\\\r\n	w_2 &=& 1 + 1/2(w_1 + w_5) = 1+ w_1 \\\\\r\n	w_3 &=& 1/3(w_0 + w_4 + w_6) = 1/3(w_0 +w_4) \\\\\r\n	w_4 &=&  1/4(w_1+w_3+w_5+w_7) = 1/2(w_1+w_3)\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=3/2\\quad w_1=w_5= 2 \\quad w_2= 3 \\quad \\hbox{and} \\quad  w_3=w_7=1.$$\r\nStarting from cell 4,  Mickey\'s  mean number of visits of cell 2 before reaching cell 6\r\nis $w_4=3/2$.',1,1,1,1,'1&2',NULL,'1','1'),(16,'Mean,Varance in Branching Process','1. Statement:\r\n<br>\r\n		Let\r\n		$$\\mu = E(\\xi) , \\sigma^2 = Var (\\xi), M_n = E(X_n), V_n = Var (X_n).$$ \r\n		The definition of a branching process implies the following proposition.\r\n		\r\n		{\\bf Proposition 3.1}. \\\r\n		$$M_n = \\mu^n M_0 \\quad \\hbox{and} \\quad\r\n		V_n = \\cases{ n \\sigma^2  M_0 & if $\\mu=1$ \\cr\r\n			\\sigma^2 M_0 \\mu^{n-1}{1- \\mu^n \\over 1- \\mu}\r\n			& if $\\mu \\not= 1$}\r\n		$$\r\n		Unless otherwise stated, we always set $X_0=1$. Then, $M_0=1$ and\r\n		$$M_n =  \\mu^n \\quad \\hbox{and} \\quad\r\n		V_n = \\cases{  n \\sigma^2 & if $\\mu=1$ \\cr\r\n			\\sigma^2  \\mu^{n-1}{1- \\mu^n \\over 1- \\mu}\r\n			& if $\\mu \\not= 1$}\r\n		$$\r\n\r\n\r\n2.Remark:  \r\n<br>\r\n		Properties:\r\n		$$\\,\\, \\cases{ M_n \\, \\, \\hbox{and}  \\,\\, V_n \\uparrow \\hbox{geometrically}\r\n			& if $\\mu > 1$; \\cr\r\n			M_n =1 \\,  \\hbox{and}  \\,\\, V_n \\uparrow \\hbox{linearlly}\r\n			& if $\\mu=1$; \\cr\r\n			M_n \\, \\, \\hbox{and}  \\,\\, V_n \\downarrow \\hbox{geometrically}\r\n			& if $\\mu <1$.\r\n		}\r\n		$$\r\n		Proof:\r\n		Write\r\n		\\begin{eqnarray*}\r\n			M_{n+1} &=& E(\\xi^{(n)}_0 + \\xi^{(n)}_1 + \\cdots + \\xi^{(n)}_{X_n})\r\n			=  \\sum_{k=0}^\\infty E(\\xi_0^{(n)} + \\xi_1^{(n)} + \\cdots + \\xi_k^{(n)}) P(X_n = k) \\\\\r\n			&=& \\sum_{k=0}^\\infty k \\mu P(X_n =k)\r\n			=  \\mu E(X_n) = \\mu M_n\r\n			.\r\n		\\end{eqnarray*}\r\n		Then, by induction\r\n		$$M_n= \\mu M_{n-1} = \\mu^2 M_{n-2} = \\cdots\r\n		= \\mu^n M_0.$$\r\n		By basically the same token,\r\n		\\begin{eqnarray*}\r\n			V_{n+1}&=& Var(X_{n+1})\r\n			= E(X_{n+1}^2)- \\{ E(X_{n+1}) \\}^2\r\n			=  E( \\sum_{k=0}^{X_n} \\xi^{(n)}_k )^2 - \\{ \\mu^{n+1}M_0\\}^2   \\\\\r\n			&=& \\sum_{k=0}^\\infty  E( \\sum_{j=0}^{k} \\xi^{(n)}_j )^2  P(X_n = k)\r\n			- \\mu^{2n+2} M_0^2  \\\\\r\n			&=&  \\sum_{k=0}^\\infty [Var( \\sum_{j=0}^{k} \\xi^{(n)}_j )\r\n			+ \\{ E( \\sum_{j=0}^{k} \\xi^{(n)}_j )\\}^2 ]   P(X_n = k)\r\n			- \\mu^{2n+2} M_0^2 \\\\\r\n			&=&   \\sum_{k=0}^\\infty [ k \\sigma^2 + (k \\mu)^2 ] P(X_n = k)\r\n			-\\mu^{2n+2} M_0^2\r\n			=   \\sigma^2 E(X_n) + \\mu^2 E(X_n^2) -\\mu^{2n+2} M_0^2\\\\\r\n			&=& \\sigma^2 \\mu^n M_0 + \\mu^2 V_n + \\mu^2 M_n^2 - \\mu^{2n+2}M_0^2\r\n			=\\mu^2V_n + \\mu^{n} \\sigma^2M_0\r\n		\\end{eqnarray*}\r\n		Therefore, by induction,\r\n		\\begin{eqnarray*}\r\n			V_{n} &=&  \\mu^2V_{n-1}  +  \\sigma^2 \\mu^{n-1} M_0\r\n			= \\mu^2( \\mu^2 V_{n-2}+ \\sigma^2 \\mu^{n-2}M_0   )   +  \\sigma^2 \\mu^{n-1}M_0   \\\\\r\n			&=& \\cdots \\\\\r\n			&=& \\sigma^2 ( \\mu^{n-1} + \\mu^{n} +\\cdots + \\mu^{2n-2} ) M_0\r\n			=\\sigma^2 \\mu^{n-1} ( 1+ \\mu + \\cdots + \\mu^{n-1} )M_0   \\\\\r\n			&=& \\cases{ n \\sigma^2  M_0  & if $\\mu=1$ \\cr\r\n				\\sigma^2 \\mu^{n-1} { 1- \\mu^n \\over 1- \\mu}M_0\r\n				& if $ \\mu \\not= 1$}\r\n		\\end{eqnarray*}',1,1,1,1,'3',NULL,'1','1'),(17,'$\\xi^{(n)} iid$','1.Statement:\r\n<br>\r\nThe defining characteristic is that $\\xi^{(n)}_i: i \\geq 1, n \\geq 0,$\r\nare $i.i.d.$. The numbers of the next generation of any member\r\nare $i.i.d.$.\r\n<br>\r\n2. Remark:  \r\n<br>\r\n		Let $\\xi$ be the random variable of the number of the next generation of\r\n		a typical individual in this population. Set\r\n		$$p_k = P(\\xi = k), \\qquad k=0, 1, 2, ...$$\r\n		Let $\\xi_i^{(n)}$ be the size of the next generation of the $i$-th  member\r\n		of the $n$-th generation of this population. Then,\r\n		$$ X_{n+1} = \\xi^{(n)}_0 + \\xi^{(n)}_1+ \\cdots+ \\xi^{(n)}_{X_n}.$$\r\n		Here, for notational convenience, we let   $\\xi^{(n)}_0\\equiv 0$.\r\n		Clearly, $X_{n+1}=0$ if  $X_n=0$.\r\n<br>\r\n		The process $\\{X_n: n \\geq 0\\}$ is called a branching process.',1,1,1,1,'3',NULL,'1','1'),(18,'$\\phi_{X_n}(s)$','1. Statement:\r\n<br>\r\nProbability generating function for $X_n$, defined by\r\n$$\\phi_n(s) = E(s^{X_n}).$$\r\n$\\phi_n(\\cdot)$ is the probability generating function for $X_n$, the\r\nsize of the $n$-th generation of a branching process.\r\n<br>\r\n2.Remark:  \r\n<br>\r\nSince, as a convention, we set $X_0=1$, we have $\\phi_0(s) =s $ and\r\n$$\\phi_1(s) = E(s^{X_1}) = E(s^{\\xi}) = \\phi(s).$$\r\nMoreover, for $n \\geq 0$.\r\n\\begin{eqnarray*}\r\n	\\phi_{n+1}(s) &=& E(s^{X_{n+1}}) = E(s^{\\xi_0^{(n)} + \\xi_1^{(n)} + \\cdots + \\xi_{X_n} ^{(n)} })\r\n	\\\\\r\n	&=& \\sum_{k=0}^\\infty E(s^{\\xi_0^{(n)} + \\xi_1^{(n)} + \\cdots + \\xi_k ^{(n)} }) P(X_n=k)\r\n	\\\\\r\n	&=& \\sum_{k=0}^\\infty \\{ E(s^\\xi)\\}^k P(X_n=k) = \\sum_{k=0}^\\infty \\{ \\phi(s) \\}^k P(X_n=k)\r\n	\\\\\r\n	&=& E\\{ \\phi(s)^{X_n} \\} = \\phi_n(\\phi(s))\r\n\\end{eqnarray*}\r\nBy induction, we have\r\n\\begin{eqnarray*}\r\n	\\phi_n(s) &=& \\underbrace{ \\phi(\\phi(\\phi( \\cdots (\\phi }(s)) )))   \\\\\r\n	&&\\qquad \\, \\, n\r\n\\end{eqnarray*}\r\nIn particular,\r\n$$\\phi_n\'(1) = E(X_n) \\quad {and} \\quad \\phi_n(0) = u_n =P(X_n=0).$$',1,1,1,1,'3',NULL,'1','1'),(19,'$\\phi_{\\xi}(s)$','1.Statement:\r\n<br>\r\n$\\phi(s)$ is defined to describe the probability generating function of\r\n$\\xi$:\r\n$$\\phi(s) = E(s^\\xi) = \\sum_{k=0}^\\infty s^kp_k, \\qquad s\\in [0, 1].$$\r\n2.Remark:  \r\n<br>\r\nThe following are some properties of $\\phi(\\cdot)$:\r\n<br>\r\n1). For $n \\geq 1$, the $n$-th derivative of $\\phi(\\cdot)$,\r\ndenoted as $\\phi^{(n)}(\\cdot)$, satisfies\r\n$$\\phi^{(n)}(s) = \\sum_{k=n}^\\infty k(k-1) \\cdots (k-n+1)p_k s^{k-n}\r\n=\\sum_{l=0}^\\infty {(l+n)!p_{l+n} \\over l!} s^l.$$\r\nAs a result,\r\n$$ \\phi^{(n)}(0) = n! p_n. $$\r\nThe fact that $\\phi^{(n)}(0) /n!$ produces $p_n$, the probabilities\r\nof $\\xi$, explains why $\\phi(\\cdot)$ is called the probability generating\r\nfunction of $\\xi$.\r\n<br>\r\n2). In particular,\r\n$$\\phi\'(s) = \\sum_{k=1}^\\infty k s^{k-1} p_k,\r\n\\quad \\phi\'(1) = \\sum_{k=1}^\\infty k p_k = E(\\xi)$$\r\n$$\\phi\'\'(1)= E(\\xi^2) - E(\\xi) \\quad {and} \\quad\r\nVar(\\xi)= \\phi\'\'(s)-\\{\\phi\'(1)\\}^2 + \\phi\'(1).$$\r\n\r\n3). $\\phi(\\cdot)$ is increasing and convex on $[0, 1]$.\r\n<br>\r\n\r\n4). Suppose $\\xi$ and $\\eta$ are independent, then the probability generating function\r\nof $\\xi+\\eta$ is the product of the those of $\\xi$ and $\\eta$.',1,1,1,1,'3',NULL,'1','1'),(20,'Probability Genarating Function','1.Statement:\r\n<br>\r\n		Suppose $\\xi$ is a nonnegative integer-valued\r\n		random variable. Let\r\n		$p_k = P(\\xi =k)$ for $k=0, 1, 2, ...$. The\r\n		probability generating function of\r\n		$\\xi$ is defined as\r\n		$$\\phi(s) = E(s^\\xi) = \\sum_{k=0}^\\infty s^kp_k, \\qquad s\\in [0, 1].$$\r\n2.Remark:  \r\n<br>\r\n		1). For $n \\geq 1$, the $n$-th derivative of $\\phi(\\cdot)$,\r\n		denoted as $\\phi^{(n)}(\\cdot)$, satisfies\r\n		$$\\phi^{(n)}(s) = \\sum_{k=n}^\\infty k(k-1) \\cdots (k-n+1)p_k s^{k-n}\r\n		=\\sum_{l=0}^\\infty {(l+n)!p_{l+n} \\over l!} s^l.$$\r\n		As a result,\r\n		$$ \\phi^{(n)}(0) = n! p_n. $$\r\n		The fact that $\\phi^{(n)}(0) /n!$ produces $p_n$, the probabilities\r\n		of $\\xi$, explains why $\\phi(\\cdot)$ is called the probability generating\r\n		function of $\\xi$.\r\n<br>\r\n2). In particular,\r\n		$$\\phi\'(s) = \\sum_{k=1}^\\infty k s^{k-1} p_k,\r\n		\\quad \\phi\'(1) = \\sum_{k=1}^\\infty k p_k = E(\\xi)$$\r\n		$$\\phi\'\'(1)= E(\\xi^2) - E(\\xi) \\quad {and} \\quad\r\n		Var(\\xi)= \\phi\'\'(s)-\\{\\phi\'(1)\\}^2 + \\phi\'(1).$$\r\n		\r\n		3). $\\phi(\\cdot)$ is increasing and convex on $[0, 1]$.\r\n<br>\r\n		4). Suppose $\\xi$ and $\\eta$ are independent, then the probability generating function\r\n		of $\\xi+\\eta$ is the product of the those of $\\xi$ and $\\eta$.',1,1,1,1,'3',NULL,'1','1'),(21,'Extinction Probability','1. Statement:\r\n<br>\r\n \r\nFor  a branching process\r\n$\\{X_n: n\\geq 0\\}$,\r\n$$X_{n+1} = \\sum_{j=0}^{X_n} \\xi^{(n)}_j$$\r\nis the size of the $n+1$-th generation, and\r\n$\\xi_j^{(n)}$ is the size of the next generation of\r\nthe $j$-th member of the $n$-th generation.\r\nFor simplicity, set $X_0=1$.\r\nLet $\\xi$ denote the random variable\r\nas the number of the next generation of a typical member of a population.\r\nLet $p_k = P(\\xi=k), k=0, 1, 2, ...$.\r\n\r\n\r\n\r\nDefine $N= \\min\\{ n \\geq 0: X_n=0\\}$. Then, $N$ is the time-to-extinction.\r\n<br>\r\nSet, for $n\\geq 0$,  $u_n= P(N\\leq n) =P(X_n=0)$, the chance that the population extinction at or\r\nbefore the $n$-th generation. Moreover,\r\n$$u_\\infty = P(N < \\infty) = \\lim_{n \\to \\infty} P(N \\leq n) $$\r\nis the chance of population eventual extinction. Since we assume $X_0=1$, Clearly,\r\n$$u_0 = 0, \\quad \\hbox{and} \\quad u_1 =P(X_1=0)=P(X_1=0|X_0=1) = P(\\xi=0) = p_0.$$\r\n\r\n\r\n\r\nIn general,\r\n\\begin{eqnarray*}\r\n	u_n &=& P(N\\leq n) = P(X_n=0|X_0=1) \\\\\r\n	&=& \\sum_{k=0}^\\infty P(X_n=0, X_1=k |X_0=1) \\\\\r\n	&=& \\sum_{k=0}^\\infty P(X_n=0| X_1=k, X_0=1)P(X_1=k|X_0=1) \\\\\r\n	&=& \\sum_{k=0}^\\infty P(X_n=0| X_1=k) p_k\r\n	=\\sum_{k=0}^\\infty P(X_n=0| X_1=1)^k p_k \\\\\r\n	&=& \\sum_{k=0}^\\infty P(X_{n-1}=0| X_0=1)^k p_k\r\n	=\\sum_{k=0}^\\infty u_{n-1}^k p_k \\\\\r\n	&=& E(u_{n-1}^\\xi)\r\n\\end{eqnarray*}',1,1,1,1,'3',NULL,'1','1'),(22,'Eventual Extinction','1. Statement:\r\n \r\n$$u_\\infty = P(N < \\infty) = \\lim_{n \\to \\infty} P(N \\leq n) $$\r\nis the chance of population eventual extinction.',1,1,1,1,'3',NULL,'1','1'),(23,'Events and Probability','1. Statement:\r\n <br>\r\n 1.1 Sample space: sample space $ S $ is a set which includes all possible outcomes in an experiment.\r\n <br>\r\n 1.2 Sample point: sample point $ \\omega $ is an element in the sample space $ S $.\r\n <br>\r\n 1.3 Event: the event $ A $ is a subset of the sample space $ S $.\r\n<br>\r\n1.4 Probability: the probability $ P $ on sample space $ S $ is to measure the event.\r\n <br>\r\n2. Remark:  \r\n <br>\r\n For any events $ A $, $ 0\\leq P(A) \\leq1 $.',1,1,1,1,'1&2',NULL,'1','1'),(24,'Random Variable','1.Statement:\r\n<br>\r\n		A random variable, usually written $X$, is a variable whose possible values are numerical outcomes of a random phenomenon.\r\n<br>\r\n2. Remark:\r\n<br>  \r\n		A discrete random variable is one which may take on only a countable number of distinct values such as $0,1,2,3,4...$ Discrete random variables are usually (but not necessarily) counts. If a random variable can take only a finite number of distinct values, then it must be discrete. Examples of discrete random variables include the number of children in a family, the Friday night attendance at a cinema, the number of patients in a doctor\'s surgery, the number of defective light bulbs in a box of ten.\r\n		The probability distribution of a discrete random variable is a list of probabilities associated with each of its possible values. It is also sometimes called the probability function or the probability mass function.',1,1,1,1,'1&2',NULL,'1','1'),(25,'Conditional Probability','1. Statement:\r\n<br>\r\n Let $ A $ and $ B $ be two events in a sample space $ S $ with $ P(B) >0$. The Conditional Probability of $ A $ given $ B $ is defined as\r\n $$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\r\n2. Remark\r\n <br>\r\n From the definition of conditional probability, we can get\r\n $$ P(A\\cap B) = P(A|B)P(B)$$',1,1,1,1,'1&2',NULL,'1','1'),(26,'Slice Universe','1. Statement:\r\n<br>\r\n Let $ B_1, B_2,\\cdots, B_n $ be a partition (both exhaustive and mutually exclusive) of the sample space $ S $ sich that $ P(B_i) \\neq 0$ for all $ i $. Then for any event $ A $, we have \r\n $$ P(A) = \\sum_{i=1}^{n} P(A\\cap B_i) = \\sum_{i=1}^{n} P(A|B_i)P(B_i)$$ \r\n2. Remark\r\n <br>\r\n Exhaustive means $ \\cup_{i=1}^n B_i = S $, where $S$ is sample space. Mutually exclusive means for different $ i $ and $ j $, $ B_i \\cap B_j = \\emptyset $.',1,1,1,1,'1&2',NULL,'1','1'),(27,'Bayes\' Rule','1. Statement:\r\n<br>\r\n Let $ B_1, B_2,\\cdots, B_n $ be a partition (both exhaustive and mutually exclusive) of the sample space  $ S $ sich that $ P(B_i) \\neq 0$ for all $ i $. Then for all $ i = 1, 2, 3, \\cdots,n $:\r\n $$ P(B_i|A) = \\frac{P(A|B_i)P(B_i)}{P(A)} = \\frac{{P(A|B_i)P(B_i)}}{ \\sum_{j=1}^{n} P(A|B_j)P(B_j)}$$\r\n2. Remark\r\n <br>\r\n Exhaustive means $ \\cup_{i=1}^n B_i = S $, where $S$ is sample space. Mutually exclusive means for different $ i $ and $ j $, $ B_i \\cap B_j = \\emptyset $.',1,1,1,1,'1&2',NULL,'1','1'),(28,'Independence of Events','1. Statement:\r\n<br>\r\nLet $ A $ and $ B $  be two events in a sample space $ S $. $ A $ and $ B $ are called independent if\r\n$$ P(A\\cap B) = P(A)P(B).$$\r\n2. Remark: \r\n<br>\r\nA sequence events $ E_1, E_2, \\cdots, E_n $ are called independent if and only if \r\n$$ P(E_{i_1}\\cap \\cdots \\cap E_{i_k}) = P(E_{i_1}) \\cdots P(E_{i_k}) ,$$\r\nwhere $\\{i_1,\\cdots, I_k \\}$ is any subset of $\\{1, 2, \\cdots, n\\}  $.\r\n<br>\r\nAnd later you will see the independence of events is just a special case of independence of random variables.',1,1,1,1,'1&2',NULL,'1','1'),(29,'Central Limit Theory','1.Statement:\r\n		<br>\r\nLet $X_1,X_2...,X_n$ be a random sample from a distribution with (finite) mean $\\mu$ with (finite) variance $\\sigma^2$. If the sample size $n$ is \"sufficently\" large, then:\r\n<br>\r\n(1) the sample mean $\\overline{X}$ follows an approximate distribution.\r\n<br>\r\n(2) with mean $E(\\overline{X})=\\mu_{\\overline{X}}=\\mu$.\r\n<br>\r\n(3) and variance $Var(\\overline{X})=\\sigma_{\\overline{X}}^2=\\frac{\\sigma^2}{n}$	\r\n<br>\r\nWe write:\r\n$$\\overline{X}\\stackrel{d}{\\rightarrow}{N}(\\mu, \\frac{\\sigma^2}{n}) \\quad as \\quad n\\rightarrow{\\infty}$$	\r\nor:\r\n$$Z=\\frac{\\overline{X}-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}=\\frac{\\sum_{i=1}^{n}X_i-n\\mu}{\\sqrt{n}\\sigma}\\stackrel{d}\\rightarrow{N}(0,1) \\quad as \\quad n\\rightarrow\\infty$$',1,1,1,1,'1&2',NULL,'1','1'),(30,'Joint Distribution','1.Statement:\r\n<br>\r\n1.1 Joint c.d.f\r\n<br>\r\nLet $ X $ and $ Y $  be two random variables, the joint cumulative distribution function (joint cdf) is defined as\r\n$$ F_{X,Y}(x,y) = P_{X,Y}(X\\leq x, Y \\leq y),$$\r\nfor $ -\\infty < x, y < \\infty,$ where $P_{X, Y}(\\cdot,\\cdot) $ is a probability function defined on the product space of $ X $ and $ Y $.\r\n<br>\r\n1.2 Marginal c.d.f of $ X $ and $ Y $ respectively are \r\n$$ F_X(x) = \\lim\\limits_{y \\rightarrow \\infty} F_{X,Y}(x,y) $$\r\n$$ F_Y(y) = \\lim\\limits_{x \\rightarrow \\infty} F_{X,Y}(x,y) $$\r\n1.3 Joint p.d.f\r\n<br> Let $ X $ and $ Y $  be two random variables, the joint probability distribution function (joint pdf) is defined as\r\n$$f_{X,Y}( x,y) = \\frac{d^2 F_{X,Y}(x,y)}{dxdy},$$\r\nwhere $  F_{X,Y}(x,y)  $ is joint cdf.\r\n<br>\r\n1.4 Marginal p.d.f of $ X $ and $ Y $ respectively are \r\n$$ f_X(x) =\\int_{-\\infty}^{\\infty} f_{X,Y}( x,y) dy$$\r\n$$ f_Y(y) =\\int_{-\\infty}^{\\infty} f_{X,Y}( x,y) dx$$\r\nwhere where $  f_{X,Y}(x,y)  $ is joint pdf.',1,1,1,1,'1&2',NULL,'1','1'),(31,'Mean and Variance','1. Statement:\r\n<br>\r\nIf $X$ is a random variable with probability density function $f(x)$, then we define the expected value of $X$ to be:\r\n$$E(X):=\\int_{-\\infty}^{\\infty}xf(x)dx$$\r\nWe define the variance of X to be:\r\n$$Var(X):=\\int_{-\\infty}^{\\infty}[x-E(X)]^2f(x)dx$$\r\n2. Remark:\r\n<br>\r\nAs with the variance of a discrete random variable, there is a simpler formula for the variance.\r\n \\begin{eqnarray*}\r\n 	Var(X):&=&\\int_{-\\infty}^{\\infty}[x-E(X)]^2f(x)dx\\\\\r\n 	       &=&\\int_{-\\infty}^{\\infty}X^2f(x)-2E(X)\\int_{-\\infty}^{\\infty}xf(X)dx+E(X)^2\\int_{-\\infty}^{\\infty}f(x)dx\\\\\r\n 	       &=&\\int_{-\\infty}^{\\infty}X^2f(x)dx-2E(X)E(x)+E(X)^2\\times{1}\\\\\r\n 	       &=&\\int_{-\\infty}^{\\infty}X^2f(x)d-E(X)^2\r\n \\end{eqnarray*}',1,1,1,1,'1&2',NULL,'1','1'),(32,'Independence of random variables','1.Statement:\r\n<br>\r\n1.1 For discrete r.v.s $ X $ and $ Y $, $ X $ and $ Y $ are independent if and only if\r\n$$ P(X = x, Y= y) = P(X = x)P(Y = y) , ~~ \\forall(x,y) $$\r\n1.2 For continuous r.v.s $ X $ and $ Y $, $ X $ and $ Y $ are independent if and only if\r\n$$ f_{X,Y}(x,y) = f_X(x)f_Y(y),$$\r\nwhere $ f_{X,Y}(x,y) $ is joint pdf of $ X $ and $ Y $, $ f_X(x) $ is pdf of $ X $ and $ f_Y(y) $ is pdf of $ Y $ .',1,1,1,1,'1&2',NULL,'1','1'),(33,'Correlation','1.Statement:\r\n<br>\r\n1.1 Covariance:\r\n<br>\r\nFor two random variables $ X $ and $ Y $, the covariance of $ X $ and $ Y $ is defined as:\r\n$$ Cov(X, Y) = E((X - EX)(Y - EY)) = E(XY) - EXEY$$\r\n1.2 Correlation:\r\n<br>\r\n$$ Corr(X, Y) = \\frac{Cov(X, Y)}{\\sqrt{Var(X)Var(Y)}}.$$\r\n2. Remark:\r\n<br>\r\n2.1 We can see that $ -1\\leq Corr(X, Y) \\leq 1 $.\r\n<br>\r\n2.2 If $ X $ and $ Y $ are independent, then $ Cov(X, Y) = 0 $, however, the converse is not true. But if $ X $ and $ Y $ are normal distribution, the independece of $ X $ and $ Y $ is equivalent to $ Cov(X, Y) = 0 $.',1,1,1,1,'1&2',NULL,'1','1'),(34,'Distribution and Density','1.Statement:\r\n<br>\r\nA (cumulative) distribution function of a random variable $X$ is defined by\r\n$$F_X(x)=P\\{\\omega\\in\\Omega;X(\\omega)\\leq{x}\\}$$\r\nwe shall typically write the less explicit expression:\r\n$$F_X(x)=P(X\\leq{x})$$\r\nfor distribution function.\r\nFor $X$ a random variable whose distribution function $F_X(x)$ has a derivative. The function $F_X(x)$ satisfying:\r\n$$P(a\\leq{x}\\leq{b})=F_X(b)-F_X(a)=\\int_{a}^{b}f_X(t)dt$$\r\n2. Remark:\r\n<br>\r\nThe density function has two basic properties that mirror\r\nthe properties of the mass function:\r\n\\begin{eqnarray}\r\nf_X(x)\\geq{0}~~ for~ all~ x ~in ~the ~state ~space\r\n\\end{eqnarray}\r\n$$\\int_{-\\infty}^{\\infty}f(x)dx=1$$',1,1,1,1,'1&2',NULL,'1','1'),(35,'Regular M.C','1. Statement: \r\n<br>\r\n(i). A transition probability matrix $ P $ is called a regular transition probability matrix, if there exists a $ k>0 $ such that all entries of $ P^k $ are positive.\r\n<br>\r\n  (ii). A MC is called regular if its transition probability matrix is regular.\r\n<br>\r\n2. Remark:\r\n<br>  \r\n In the definition of regular transition matrix, $ P^k $ has all positive elements for some $ k>0 $. Then, for all $ n>k $, $ P^n $ all have all positive element. In other word, an equivalent definition is there exists a $ k>0 $ such that, for all state $ i, j $ and all $ n\\geq k $, $ P_{ij}^{(n)}>0 $.',1,1,300,600,'4',NULL,'1','\\n\\n\\nregular M.C'),(36,'A sufficient condition for regular MC','1.  Statement:\r\n<br>\r\n Given a transition probability matrix, it is often not easy to identify whether it is regular. There is a sufficient condition:\r\n<br>\r\n  (i). for any $ i $ and $ j $ in the state space, there exists a $ k>0 $ (which depends on $ i $ and $ j $) such that $ P_{ij}^{(k)} >0$;\r\n  <br>\r\n  (ii) there exists a state $ i $ such that $ P_{ii} >0$. Then the transition matrix is regular.\r\n<br>\r\n2. Remark\r\n<br>\r\nPart (i) of the above sufficient condition, in which $ k $ depends on $ i $ and $ j $ is very different from the universal $ k $ in the definition of regular transition matrix.',1,1,700,600,'4',NULL,'1','\\n\\n\\na sufficient condition'),(37,'Long run behavior','1. Statement:\r\n <br>\r\n Let $ P $ be a regular transition matrix with states $\\{0, 1,\\ldots,N\\}  $. Then, for all states $ i $ and $ j $, as $ n\\rightarrow \\infty  $,\r\n$$ P_{ij}^{(n)} \\rightarrow \\pi_j >0 ~~~~and,~as~a~result, ~~P(X_n=j)\\rightarrow \\pi_j >0$$\r\nwhere $ (\\pi_0,\\pi_1,\\ldots,\\pi_N) $ is the unique solution to\r\n\\begin{equation}\r\n\\left\\{\r\n\\begin{array}{c}\r\n\\pi_j = \\sum_{k=0}^N \\pi_k P_{kj},\\\\\r\n\\sum_{i=0}^N \\pi_i = 1.  \\\\\r\n\\end{array}\r\n\\right.\r\n\\end{equation}\r\n2. Remark:  \r\n<br>\r\n The proof for the existence of the limit is technical and beyond the pursuit of this course.',1,1,500,400,'4',600,'1','\\n                                       long-run behavior'),(39,'$\\pi = \\pi P$','1. Statement:\r\n<br>\r\n From the statement in the long run behavior, we know how to compute $ \\pi_j $:\r\n \\begin{equation}\r\n \\left\\{\r\n \\begin{array}{c}\r\n \\pi_j = \\sum_{k=0}^N \\pi_k P_{kj},\\\\\r\n \\sum_{i=0}^N \\pi_i = 1.  \\\\\r\n \\end{array}\r\n \\right.\r\n \\end{equation}\r\n The first equation above is equivalent to $ \\pi = \\pi P $, where $\\pi= (\\pi_0,\\pi_1,\\ldots,\\pi_N) $ and $ P $ is transition probability matrix.\r\n <br>\r\n2. Remark:  \r\n<br>\r\nIt appears to be odd that there are N + 2 equations to solve for N + 1 variables. See more details in DIY.',1,1,200,300,'4',NULL,'1','\\n\\n\\n?=?p'),(40,'Space average and time average','1. Statement:\r\n<br>\r\n There are two average in the long run behavior of regular MC.\r\n The space average interpretation is, at any fixed time long into the future, the MC has about probability $ \\pi_j $ to be state $ j $. This gives the distribution of the $ X_n$ for a fixed large $ n $ in the state space.\r\n <br>\r\n The time avergae interpretation is, over a long period of time, the relative frequency of the MC staying in state $ j $ is about $ \\pi_j $.\r\n<br>\r\n 2. Remark:  \r\n<br>\r\nA little more elaboration on the time average. Suppose there is a payoff $ c_j $ whenever the $ MC $ visits state $ j $ once. Assume $ X_0 = k $, the long run average of payoff is\r\n$$ A_n \\equiv \\sum_{i=1}^n c_{X_i}/n = \\sum_{i=1}^n\\sum_{j=0}^{N}c_jI_{\\{X_i=j\\}}/n$$\r\nHeuristically, for large $ n $, $ A_n $, which is random, would be close to its mean, which is\r\n$$ E(A_n) = \\sum_{i=1}^n\\sum_{j=0}^{N}c_jE(I_{\\{X_i=j\\}})/n=\\sum_{i=1}^n\\sum_{j=0}^{N}c_jP(X_i = j|X_0 = k)/n = \\sum_{j=0}^{N}c_j[\\sum_{i=1}^n P_{kj}^{(i)}/n] \\approx  \\sum_{j=0}^{N}c_j\\pi_j$$\r\nNow, consider the special case with $ c_l = 1 $ for a particular $ l $ and the rest $ c_0,\\ldots,\\cap_{l-1},c_{l+1},\\ldots,c_N $ being all 0. Then $ A_n $ is fraction of the MC visits of state $ l $, which is indeed close to $ \\sum_{j=0}^{N}c_j\\pi_j = \\pi_l $.',1,1,500,200,'4',NULL,'1','space average\\n                                     and time average'),(41,'With states Rewards','Application',1,1,1100,100,'4',NULL,'1','\\n\\n          with state \\n      rewards'),(42,'Coupling','Application',1,1,1100,300,'4',NULL,'1','\\n\\n\\ncoupling'),(43,'Include History','Application',1,1,1100,500,'4',NULL,'1','\\n\\n\\nincluding history'),(44,'Conditional Independence','1. Statement:\r\n <br>\r\n Two events A and B are conditionally independence given an event C with $P(C)>0$ if\r\n \r\n   $$P(A\\cup{B}|C) = P(A|B) P(B|C)$$\r\n \r\n   \r\n2.Remark:  \r\n <br>\r\nRecall that from the conditional probability,$$P(A|B)=\\frac{P(A\\cup{B})}{P(B)},$$\r\nif $P(B)>0$. By the conditioning on C, we obtain$$P(A|B,C)=\\frac{P(A\\cup{B}|C)}{P(B|C)},$$\r\nif $P(B|C)>0,P(C)\\not=0$.If A and B conditionally independent given on C, we obtain$$P(A|B,C)=\\frac{P(A\\cup{B}|C)}{P(B|C}=\\frac{P(A|C)P(B|C)}{P(B|C)}=P(A|C).$$\r\nThus,if A and B are conditionally independent given C, then$$P(A|B,C)=P(A|C),$$ which is the equivalent statement of the definition of conditional independence.',1,1,1,1,'3',NULL,'1','1'),(45,'First step analysis','1.Statement:\r\n<br>\r\n		The method of first-step analysis, is the method used in the example below, for the purpose of demonstrating a easier way of solving the problem.\r\n		For that particular problem, it is actually an easier method without invoking	the {\\it MC}  $\\{X_n, n\\geq 1 \\}$, but, rather, directly based on $\\{\\xi_i, i \\geq 1\\}$\r\n<br>\r\n2.Remark:  \r\n<br>\r\nRepeatedly tossing a fair coin.\r\nThere are 8 patterns of length 3:\r\n\r\n	$$HHH,  HHT, HTH, HTT, THH, THT, TTH, TTT$$\r\n\r\nTwo men, called  Yugong (Y) and Zhishou (Z),  are betting on whose choice\r\nof pattern would occur first. Mr Z  ``generously\"  allows\r\nMr Y to pick his favorite pattern first and, afterwards, he picks his own.\r\nWhatever Mr Y picks,  Mr Z, being ``Zhishou\", somehow\r\nalways beats him by a  chance at least 2/3! (This time, even infinite offspring\r\nto continue the game can\'t help Yugong.)\r\n<br>\r\n\r\n\r\nLet $X_n, n \\geq 2$ denote the pattern of length 2 of tosses\r\n$n-1$ and $n$.\r\n<br>\r\nThen $\\{X_n: n \\geq 2\\}$ is a $MC$  with state space $\\{$HH , HT, TH, TT$\\}$,\r\nwhich is accordingly denoted as $\\{0, 1, 2, 3\\}$, and transition matrix\r\n$$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n\\pmatrix{\r\n	1/2 &  1/2  &     &              \\cr\r\n	&     &  1/2  &  1/2            \\cr\r\n	1/2 &  1/2   &     &             \\cr\r\n	&      &  1/2   &   1/2\r\n}\r\n$$\r\nSuppose Mr Y\'s pick is HTH. Mr Z can pick HHT  to beat Mr Y\r\nby a chance $2/3$. The following is the proof using first-step analysis.\r\n<br>\r\nLet $p_i $ be the probability that pattern HTH occurs before HHT given\r\n$X_2=i$, $i=0, 1, 2, 3$.\r\n<br>\r\nThen\r\n\\begin{eqnarray*}\r\n	p_0 &=& 1/2p_0 + 1/2 \\times 0 \\\\\r\n	p_1 &=& 1/2 \\times 1 + 1/2 p_3 \\\\\r\n	p_2 &=& 1/2 p_0 + 1/2p_1 \\\\\r\n	p_3 &=& 1/2 p_2 + 1/2 p_3\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$p_0 = 0 \\quad p_1 = 2/3 \\quad p_2=p_3=1/3$$\r\nThen, the chance that Mr Y wins is\r\n$$ (p_0 + p_1 +p_2+p_3)/4 = 1/3,$$\r\nand Mr Z wins with chance $2/3$.',1,1,1,1,'3',NULL,'1','1'),(46,'Application','Application',1,1,800,300,'4',NULL,'','\\n\\n\\napplication'),(47,'$\\pi_k=\\frac{1}{E(T|X_0=k)}$','Waiting',1,1,500,0,'4',NULL,'','$\\pi_k=\\frac{1}{E(T|X_0=k)}$'),(48,'Application','appliction',1,1,1,1,'3',NULL,'','application');
/*!40000 ALTER TABLE `mathematics_neuron` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_question`
--

DROP TABLE IF EXISTS `mathematics_question`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_question` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `code` varchar(100) NOT NULL,
  `category` int(11) NOT NULL,
  `problem` longtext NOT NULL,
  `problempicture1` varchar(100) DEFAULT NULL,
  `problempicture2` varchar(100) DEFAULT NULL,
  `problempicture3` varchar(100) DEFAULT NULL,
  `problempicture4` varchar(100) DEFAULT NULL,
  `problempicture5` varchar(100) DEFAULT NULL,
  `problempicture6` varchar(100) DEFAULT NULL,
  `choicesa` longtext,
  `choicesb` longtext,
  `choicesc` longtext,
  `choicesd` longtext,
  `choicese` longtext,
  `choicesf` longtext,
  `choicepicturea` varchar(100) DEFAULT NULL,
  `choicepictureb` varchar(100) DEFAULT NULL,
  `choicepicturec` varchar(100) DEFAULT NULL,
  `choicepictured` varchar(100) DEFAULT NULL,
  `choicepicturee` varchar(100) DEFAULT NULL,
  `choicepicturef` varchar(100) DEFAULT NULL,
  `answer` varchar(100) NOT NULL,
  `solutionspicture1` varchar(100) DEFAULT NULL,
  `solutionspicture2` varchar(100) DEFAULT NULL,
  `solutionspicture3` varchar(100) DEFAULT NULL,
  `solutions` longtext NOT NULL,
  `linkability1` double DEFAULT NULL,
  `linkability2` double DEFAULT NULL,
  `linkability3` double DEFAULT NULL,
  `linkability4` double DEFAULT NULL,
  `linkability5` double DEFAULT NULL,
  `linkability6` double DEFAULT NULL,
  `linkpersonaility` double DEFAULT NULL,
  `errors` longtext,
  `alternativesolutions` longtext,
  `messagefailure` longtext,
  `messagesuccess` longtext,
  `sensitivity` double DEFAULT NULL,
  `gussingparameter` double DEFAULT NULL,
  `difficulty` int(11) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=139 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_question`
--

LOCK TABLES `mathematics_question` WRITE;
/*!40000 ALTER TABLE `mathematics_question` DISABLE KEYS */;
INSERT INTO `mathematics_question` VALUES (1,'1.1.1',1,'aaaaaa','','','','','','','','','','','','','','','','','','','A','','','','aaaaaa',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(2,'3.1.1',4,'Bunny rabbit has three dens A, B and C. It likes A better than B and\r\nC. If it\'s in B or C on any night, it will always take chance 0.9 to go to A\r\n and chance $0.1$ to go to the other den for the following night. Once it reaches\r\n A, it will stay there for two nights and the third night will be in B or C\r\n with equal chance 1/2. Let $X_n$ be the den Bunny stays for night $n$.\r\n What is the state space of the $\\{X_n\\}$?\r\n Is $\\{X_n\\}$ a $MC$?','','','','','','','The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is a $MC$.','The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is not a $MC$.','None of the above is correct.','','','','','','','','','','B','','','','The state space of $\\{X_n\\}$ is $\\{\\hbox{Den A, Den B, Den C}\\}$.\r\n$\\{X_n\\}$ is not a $MC$,\r\nbecause, for example,\r\n $$P(X_{n+1}=A|X_{n}=A, X_{n-1}=B)=1 \\not=0=P(X_{n+1}=A|X_n=A, X_{n-1}=A).$$\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{  (MC definition)}} }$$',3,0,2,2,0,0,0,'','','Alas! Please check the definition of Markov chain.','Congratulations! You have mastered the definition of Markov chains.',1,NULL,1),(3,'3.1.3',4,'Suppose $X_n, n= 0, 1, 2, ...$ is\r\na discrete time $MC$. Let\r\n$0 \\leq n_0< n_1< n_2<   ...$ be a subsequence of the nonnegative integers\r\nand $Y_k = X_{n_k}$. Is $\\{Y_k: k =0,1, 2,...,\\}$\r\na $MC$?','','','','','','','$\\{Y_k: k =0,1, 2,...,\\}$ is a $MC$.','$\\{Y_k: k =0,1, 2,...,\\}$ is not a $MC$.','None of the above is correct.','','','','','','','','','','A','','','','The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n \\begin{eqnarray*}\r\n && P(X_{n+1}\\in A_1, X_{n+2} \\in A_2, ..., X_{n-1} \\in B_{n-1},   ..., X_0\r\n \\in B_0 | X_n = i)\r\n \\\\\r\n &=&  P(X_{n+1}\\in A_1, X_{n+2} \\in A_2, ...,|X_n=i)\r\n   \\times\r\n P(X_{n-1} \\in B_{n-1},  ..., X_0\r\n \\in B_0 | X_n = i)   \\qquad  (1)\r\n \\end{eqnarray*}\r\n$$\\qquad {\\textcolor[rgb]{1,0,0}{\\hbox{(MC definition and Conditional independence)}} }$$\r\n \r\n\r\n for any time $n$,\r\n any state $i$ and $any$ subsets of the state space, $A_1, A_2, ....,B_0, B_1, B_2, ...$; see the remark.\r\n Then,\r\n \\begin{eqnarray*}\r\n && P(Y_{k+1}\\in A_1, Y_{k+2} \\in A_2, ..., \\,\\,\\, Y_{k-1} \\in B_{k-1},  ..., Y_0\r\n \\in B_0 | Y_k=i  )\r\n \\\\\r\n &=&\r\n P(X_{n_{k+1}}\\in A_1, X_{n_{k+2}} \\in A_2, ...,\\,\\,\\, X_{n_{k-1}} \\in B_{k-1},   ..., X_{n_0}\r\n \\in B_0 | X_{n_k}=i  )\r\n \\\\\r\n && \\qquad \\hbox{(by definition of $Y_n$)}\r\n \\\\\r\n &=&\r\n P(X_{n_{k+1}}\\in A_1, X_{n_{k+2}} \\in A_2, ..., |X_{n_k}=i)\r\n  \\times P(X_{n_{k-1}} \\in B_{k-1},   ..., X_{n_0}\r\n \\in B_0 | X_{n_k}=i  )\r\n\\\\\r\n&& \\quad \\hbox{(by applying (1))}\r\n \\\\\r\n &=&\r\n P(Y_{k+1}\\in A_1, Y_{k+2} \\in A_2, ...,|Y_k=i)\r\n P(Y_{k-1} \\in B_{n-1}, Y_{k-2} \\in B_{n-2}, ..., Y_0\r\n \\in B_0 | Y_k = i).\r\n\\\\\r\n && \\quad \\hbox{(by definition of $Y_n$)}\r\n \\end{eqnarray*}\r\n \r\n<br>\r\n Remark. \r\n<br>\r\nThe mathematical expression that we often use as definition of $MC$ is\r\n $$P(X_{n+1}=j_1|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),\\qquad(2)$$\r\n for all time $n$, states $j_1,i, i_0,i_1,...,$\r\n which is a relatively simple version, but is actually equivalent to\r\n the seemingly more\r\n general version in (1). The following is to show the equivalency.\r\n Observe that (2) implies\r\n \\begin{eqnarray*}\r\n &&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i,  X_{n-1}=i_{n-1}, ..., X_0=i_0)\r\n \\\\\r\n &=& P_{i, j_1} P_{j_1, j_2} \\cdots P_{j_{k-1}, j_k}\r\n \\\\\r\n &=&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i),\r\n \\end{eqnarray*}\r\n which, by $DIY 3.1.2$, is equivalent to\r\n \\begin{eqnarray*}\r\n && P(X_{n+k}=j_k, ..., X_{n+1}=j_1,\r\n \\,\\,   X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i)\r\n \\\\\r\n &=&\r\n P(\\{X_{n+k}=j_k, ..., X_{n+1}=j_1\\} \\cap \\{   X_{n-1}=i_{n-1}, ..., X_0=i_0\\} |X_n=i)\r\n \\\\\r\n &=&\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i)\r\n P(X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i).\r\n \\end{eqnarray*}\r\n Then,\r\n \\begin{eqnarray*}\r\n &&\r\n P(X_{n+k}\\in A_k, ..., X_{n+1}\\in A_1,\r\n \\,\\,   X_{n-1}\\in B_{n-1}, ..., X_0\\in B_0 |X_n=i)\r\n \\\\\r\n &=&\r\n \\sum_{j_k \\in A_k} \\cdots \\sum_{j_1 \\in A_1} \\sum_{i_{n-1}\\in B_{n-1}} \\cdots\r\n \\sum_{i_0 \\in B_0}\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1, \\,\\,\\,   X_{n-1}=i_{n-1}, ..., X_0=i_0|X_n=i)\r\n \\\\\r\n &=&\r\n \\sum_{j_k \\in A_k} \\cdots \\sum_{j_1 \\in A_1} \\sum_{i_{n-1}\\in B_{n-1}} \\cdots\r\n \\sum_{i_0 \\in B_0}\r\n \\\\\r\n && \\qquad\r\n P(X_{n+k}=j_k, ..., X_{n+1}=j_1 | X_n=i)\r\n P(X_{n-1}=i_{n-1}, ..., X_0=i_0 |X_n=i)\r\n \\\\\r\n &=&\r\n P(X_{n+k}\\in A_k, ..., X_{n+1}\\in A_1 |X_n=i)\r\n P(  X_{n-1}=B_{n-1}, ..., X_0=B_0 |X_n=i)\r\n .\r\n \\end{eqnarray*}\r\n This equality fits precisely the statement of the conditional independence of\r\n the future and the past given a fixed state of the present.',3,1,2,0,0,0,0,'','','Alas! Please check the definition of Markov chain and conditional independence.','Congratulations! You have mastered the definition of Markov chains\r\nand conditional independence.',1,NULL,3),(4,'3.2.8',1,'(Mickey in Maze} continued from Example 3.2). \r\n(i) Compute the probability Mickey goes to cell 2 before it reaches cell 6, beginning\r\nfrom cell 1.\r\n<br>\r\n(ii). Compute the mean number of steps to reach cells 2 or 6, beginning\r\nfrom cell 4.\r\n<br>\r\n(iii). Compute the mean number of times Mickey visits cell 2 before reaching cell\r\n6, starting from cell 4.','theall/image/Expl3.2.8-1.png','','','','','','','','','','','','','','','','','','A','','','','(i) Set\r\n$$p_i = P(\\hbox{Mickey reaches cell 2 before reaching 6 } | \\hbox{ starting from i}).\r\n$$\r\nThen, for example,\r\n\\begin{eqnarray*}\r\np_0 &=& P_{01}p_1 + P_{03} p_3 = 1/2 p_1 + 1/2 p_3  \\\\\r\np_1 &=& 1/3p_0  + 1/3 p_4 + 1/3 p_2 = 1/3(p_0 +p_4) + 1/3 \\\\\r\np_2 &=& 1  \\\\\r\np_3 &=& 1/3 p_0 + 1/3 p_4 + 1/3p_6 = 1/3(p_0 +p_4) \\\\\r\np_4 &=& 1/4 p_1 + 1/4 p_3 + 1/4p_5 + 1/3p_7 \\\\\r\np_5 &=& 1/3 p_2 + 1/3 p_4 + 1/3p_8 = 1/3(p_0 +p_8) +1/3 \\\\\r\np_6 &=& 0 \\\\\r\np_7 &=& 1/3 p_4 + 1/3 p_6 + 1/3p_8 = 1/3(p_4 +p_8) \\\\\r\np_8 &=& 1/2 p_5 + 1/2 p_7\r\n\\end{eqnarray*}\r\nThe above linear equations can be solved for the answer\r\n$$ p_0=p_8=1/2, \\quad p_1=p_5 = 2/3, \\quad p_3=p_7=1/3 \\quad\r\np_4=1/2.$$\r\n\r\nIn fact, a short-cut to solving the eight equations is, by the\r\nsymmetry of the maze, to realize that $p_0=p_8$, $p_1=p_5$, $p_3=p_7$.\r\nTogether with the fact that $p_2=1$ and $p_6=0$, the number of\r\nequations can be immediately reduced to four about $p_0, p_1, p_3$\r\nand $p_4$.\r\n\r\n(ii). Set $w_i$ the mean number of steps to reach 2 or 6, starting from\r\ncell $i$. Then, obviously, $w_2=w_6=0$. By symmetry,\r\n$w_0=w_8$, $w_1=w_3=w_5=w_7$.\r\nMoreover,\r\n\\begin{eqnarray*}\r\nw_0 &=& 1 + 1/2(w_1 + w_3) = 1 + w_1 \\\\\r\nw_1 &=& 1+ 1/3(w_0+w_2+w_4)= 1 + 1/3(w_0 + w_4) \\\\\r\nw_4 &=& 1+ 1/4(w_1+w_3+w_5+w_7) = 1+w_1\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=6\\quad \\hbox{and} \\quad w_1=w_3=w_5=w_7=5.$$\r\n\r\n(iii).\r\nSet $w_i$ the mean number of visits of cell 2 before reaching cell 6, starting\r\nfrom cell $i$ (including the starting state).\r\n  Then, obviously, $w_6=0$. By symmetry,\r\n$w_0=w_8$, $w_1=w_5$ and $w_3= w_7$.\r\nMoreover,\r\n\\begin{eqnarray*}\r\nw_0 &=&  1/2(w_1 + w_3)   \\\\\r\nw_1 &=&  1/3(w_0+w_2+w_4)  \\\\\r\nw_2 &=& 1 + 1/2(w_1 + w_5) = 1+ w_1 \\\\\r\nw_3 &=& 1/3(w_0 + w_4 + w_6) = 1/3(w_0 +w_4) \\\\\r\nw_4 &=&  1/4(w_1+w_3+w_5+w_7) = 1/2(w_1+w_3)\r\n\\end{eqnarray*}\r\nSolving the three equations,\r\n$$w_0=w_4=w_8=3/2\\quad w_1=w_5= 2 \\quad w_2= 3 \\quad \\hbox{and} \\quad  w_3=w_7=1.$$\r\nStarting from cell 4,  Mickey\'s  mean number of visits of cell 2 before reaching cell 6\r\nis $w_4=3/2$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Alas! Please return to the graph and review related knowledge.','Congratulations! You have mastered the First Step Analysis method.',1,NULL,1),(5,'3.2.9',1,'( A Fecundity Model) \r\nThe life span of  women is divided into\r\nfive-year periods. In each period, a woman assumes\r\none of the following six states:\r\n$E_0$: prepuberty; $E_1$: single; $E_2$: married;\r\n$E_3$: divorced; $E_4$: widowed; $E_5$: out-of-population.\r\nAssume the transit process is a {MC}  with transition matrix\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\, E_0 \\quad E_1 \\quad E_2\\quad E_3\\quad E_4\\quad E_5\\\\\r\n {\\bf P}&=& \\matrix{E_0 \\cr E_1 \\cr E_2 \\cr E_3 \\cr E_4\\cr E_5 }\r\n  \\pmatrix{\r\n \\,\\,\\,\\,\\, &  0.9  &     &       &    & 0.1        \\cr\r\n &  0.5   &  0.4  &      &    &   0.1      \\cr\r\n  &       &  0.6   & 0.2 & 0.1  &   0.1    \\cr\r\n &    & 0.4   &  0.5  &     &     0.1     \\cr\r\n &     & 0.4  &      &  0.5   &  0.1     \\cr\r\n  &     &     &      &     &    1\r\n}\r\n\\end{eqnarray*}\r\nWhat is the mean marriage periods of a woman in her life time?','','','','','','','','','','','','','','','','','','','A','','','','Set $w_i$ the mean periods in marriage in the remaining life time,\r\nstarting from state $E_i$ for the current period (including\r\nthe current period.)\r\nThen,\r\n\\begin{eqnarray*}\r\nw_0 &=& 0.9 w_1 +0.1 w_5 \\\\\r\nw_1 &=& 0.5w_1 + 0.4 w_2 + 0.1w_5 \\\\\r\nw_2 &=& 0.6w_2 + 0.2 w_3 + 0.1 w_4 + 0.1w_5 + 1 \\\\\r\nw_3 &=& 0.4 w_2  + 0.5 w_3 + 0.1 w_5\\\\\r\nw_4 &=& 0.4 w_2  + 0.5 w_4 + 0.1 w_5\\\\\r\nw_5 &=& 0\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$w_0 = 4.5, \\quad w_1=5 \\quad w_2=5.25 \\quad \\hbox{and} \\quad w_3=w_4=5.$$\r\nThe mean marriage periods of a woman in her life time is $w_0 = 4.5$.\r\nIn other words, the mean marriage time of a woman in her life time\r\nis $4.5\\times 5 =22.5$ years.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Alas! Please return to the graph and review related knowledge.','Congratulations! You have mastered the First Step Analysis method.',1,NULL,2),(6,'3.3.12',1,'( The family tree of Confucius ) \r\n The following diagram shows the first few generations of the family tree\r\n of Confucius.','theall/image/Expl3.3.12-1.png','','','','','','','','','','','','','','','','','','A','','','','Note that $\\xi_i^{(n)} $ denotes the number of sons fathered by\r\nthe $i$-th member of the $n$-generation of Kong Zi(Confucius),\r\nand $X_n$ is the number of (male) descendants of Kong Zi at\r\nthe $n$-th generation.\r\n\r\nAllegedly, the 80-th generation of Kong Zi  is around\r\n1,300,000. If the process is indeed a branching process. Then\r\na reasonable estimate of $\\mu$, the mean number of sons produced by\r\nany male descendent of Kong Zi, can be obtained by\r\n$$ \\hat \\mu^{80} = 1,300,000,$$\r\nbased on the formula that $E(X_n) = \\mu^n$, to be shown in\r\nthe following Proposition 3.1.\r\nIt turns out $\\hat \\mu = 1.1924$, meaning that\r\nthe average number of sons produced by any  Kong father  is about\r\n1.2. Taking one step further, if the process is indeed\r\na branching process,  the data is trustworthy and\r\nthe male Kongs can be regarded as typical of\r\nChinese males, the one might claim, on average, each Chinese\r\nmale produces about 1.2 sons.\r\nIt is quite clear that the process cannot be really or even\r\napproximately a branching process---$\\xi^{(n)}_i$ cannot be\r\niid, not over good/bad historical periods, and at least\r\nnot over periods with/without birth control policies.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Alas! Please return to the graph and review related knowledge.','Congratulations! You have mastered the concept and fundamental characteristics of Branching Process.',NULL,NULL,2),(7,'3.4.1',4,'For coin tossing, is there a similar phenomenon for pattern of length 2 to\r\nthat for pattern of length 3 shown Example 3.7?','','','','','','','Yes.','No.','I am not sure.','','','','','','','','','','B','','','','No. Only four patterns of length 2: HH, HT, TH and TT.\r\nTH occurs before HH with chance 3/4, before HT and TT with 1/2 each.\r\n(Also by symmetry),  HT occurs before TT with chance 3/4, before\r\nTH and HH with 1/2 each. There is no loop of three or four patterns\r\nin which one occurs before another with larger\r\nthan 1/2 chance, unlike patterns of length 3.',3,2,2,2,0,0,0,'','','Oops, try again. Or you may review Example 3.7.','Great! You know the application of first step analysis.',1,0.333,3),(8,'3.4.3',4,'Based on the fact that the mean number of tosses till\r\n first HH is 6, can you calculate the mean number of tosses till the first\r\n HHH occur (by using one equation)?','','','','','','','14.','12.','10.','9.','None of the above is correct.','','','','','','','','A','','','','(This is a little tricky but still based only on the idea of first step analysis.)\r\nLet $w$ be the mean number till first  HHH  occur. Suppose  HH  occurs the first time\r\n at $n$-th toss. With 1/2 chance the $n+1$-th toss is H and it takes one addtional\r\n toss to have the first\r\n time HHH at $n+1$-th toss (remember, with 1/2 chance). The other 1/2 chance\r\n is for $n+1$-th toss being T. In this case, counting from $n+2$-th toss, it will take averagely\r\n w additional tosses to reach HHH. Hence,\r\n $$ w= 6 + 1/2 \\times 1 + 1/2 \\times (1+ w).$$\r\n Solve the equation, we have $w=14$.',3,1,1,3,0,0,0,'','','Oops, try again. Here is a hint: apply first step analysis.','Perfect! You have mastered the concept of first step analysis!',1,0.2,4),(9,'3.4.5',3,'A white rat is put into compartment 4 of the maze shown below.\r\nHe moves through the compartments at random; i.e., if there are k ways to\r\nleave a compartment, he chooses each of these with probability 1/k. What\r\nis the probability that the rat finds the food in compartment 3 before feeling\r\nthe electric shock in compartment 7?','','','','','','','$\\frac{7}{12}$','$\\frac{1}{4}$','$\\frac{5}{12}$','$\\frac{1}{3}$','$\\frac{1}{6}$','','','','','','','','C','','','','The question ask about some probability, probability $p_i$ have to be defined for each state.<br>\r\nNote: This probability is different from transition probability $P_{ij}$.Here $P_{ij}$ is $\\frac{1}{k}$.<br>\r\nSince the mouse have to reach 3 before 7, set\r\n\\begin{equation*}\r\np_3=1 \\qquad p_7=0\r\n\\end{equation*}\r\nThen, consider the probability of other states, now you need the transition probabilities\r\n\\begin{align*}\r\np_1&=\\frac{1}{2}p_2+\\frac{1}{2}p_4\\\\\r\np_2&=\\frac{1}{3}p_1+\\frac{1}{3}p_3+\\frac{1}{3}p_5\\\\\r\np_4&=\\frac{1}{3}p_1+\\frac{1}{3}p_5+\\frac{1}{3}p_7\\\\\r\np_5&=\\frac{1}{3}p_2+\\frac{1}{3}p_4+\\frac{1}{3}p_6\\\\\r\np_6&=\\frac{1}{2}p_3+\\frac{1}{2}p_5\\\\\r\n\\end{align*}\r\nThen do Gaussian elimination\r\n\\[\\left(\\begin{array}{ccccccc|c}\r\n1&-1/2&0&-1/2&0&0&0&0\\\\\r\n-1/3&1&-1/3&0&-1/3&0&0&0\\\\\r\n0&0&1&0&0&0&0&1\\\\\r\n-1/3&0&0&1&-1/3&0&-1/3&0\\\\\r\n0&-1/3&0&-1/3&1&-1/3&0&0\\\\\r\n0&0&-1/2&0&-1/2&1&0&0\\\\\r\n0&0&0&0&0&0&1&0\r\n\\end{array}\\right)\\]\r\n\r\nAfter tedious calculation, we have\r\n\\begin{align*}\r\np_1&=7/12\\\\\r\np_2&=3/4\\\\\r\np_4&=5/12\\\\\r\np_5&=2/3\\\\\r\np_6&=5/6\\\\\r\n\\end{align*}\r\n\r\nThe answer is 5/12.',NULL,NULL,NULL,NULL,NULL,0,NULL,'','','','',NULL,NULL,3),(11,'3.1.5',4,'$\\{X_n, n= 1, 2, ...\\}$ is\r\n a Markov chain with state space $ \\{-1,0,1\\} $.','','','','','','','$\\{sin(X_n):n=1,2,\\ldots\\} $ is a Markov chain.','$\\{cos(X_n):n=1,2,\\ldots\\} $ is a Markov chain.','$\\{|X_n|:n=1,2,\\ldots\\} $ is a Markov chain.','$\\{X_n^2:n=1,2,\\ldots\\} $ is a Markov chain.','None of the above is correct.','','','','','','','','A','','','','The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n\r\n 	 $$P(X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),$$\r\n \r\n for all time $n$, states $j_1,i, i_0,i_1,...$\r\n \r\n With the defintion of Markov chain, we check the (a) to (d) one by one:\r\n \r\n (a). It is easy to see under the state space ${-1,0,1} $, $ sin(X_n) $ is an one-to-one map on the state space, so \r\n \\begin{eqnarray*}\r\n 	&& P(sin(X_{n+1})=j|sin(X_n)=i,sin(X_{n-1})=i_{n-1}, ..., sin(X_0)=i_0)\r\n 	\\\\\r\n 	&=&\r\n 	P(X_{n+1}=arcsin(j)|X_n=arcsin(i), X_{n-1}=arcsin(i_{n-1}), ..., X_0=arcsin(i_0)\r\n 	\\\\\r\n 	&=&\r\n 		P(X_{n+1}=arcsin(j)|X_n=arcsin(i))\r\n 	\\\\\r\n 	&& \\qquad \\hbox{(by applying (1))}\r\n 	\\\\\r\n 	&=&P(sin(X_{n+1})=j|sin(X_n)=i)\r\n\\end{eqnarray*}  \r\n\r\n(b). Under the state space $ \\{-1,0,1\\} $, $ cos(X_n) $ is not an one-to-one map on the state space.\r\n<br>\r\n(c). Under the state space $\\{-1,0,1\\} $, $ |X_n| $ is not an one-to-one map on the state space.\r\n<br>\r\n(d). Under the state space $\\{-1,0,1\\} $, $ X_n^2 $ is not an one-to-one map on the state space.\r\nAnd the map in the (b),(c),(d) are somehow equivalent under the state space ${-1,0,1} $. So they should be both right or both wrong. So they are not correct answer.\r\n<br>\r\nRemark: The question is a type of question: given the $\\{X_n\\}$ is a Markov chain, and ask whether the transformation of $\\{X_n\\}$, such as $\\{cos(X_n)\\}$ is still a Markov chain. What we should do is to check whether the transformation is a one-to-one map.',3,1,2,1,0,0,0,'e','None','Oops! Please think about the definition of markov chain and conditional independence.','Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.',1,NULL,3),(12,'3.1.6',4,'$\\{X_n, n= 1, 2, ...\\}$ is\r\n a Markov chain with state space $ \\{1,2,\\ldots,10\\} $.','','','','','','','$ \\{X_n+X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain','$ \\{X_n-X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain','$ \\{X_nX_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain','$ \\{X_n/X_{n-1}: n=1,2,\\ldots\\} $ is a Markov chain','None of the above is correct.','','','','','','','','E','','','','In the answer of  DIY 3.1.5, the transformation in the state space should be one-to-one.\r\n \r\n With the answer in  DIY 3.1.5, we check the (a) to (d).\r\n In (a), the transformation is $ f(x,y) = x+y $ which is not one-to-one. So (a) is not a correct answer. The same is as transformations $ f(x,y) = x-y $, $ f(x,y) = xy $, $ f(x,y) = x/y $ which are in the (b), (c) and (d) . So the right answer is (e).',3,1,2,1,0,0,0,'A','','Oops! Please think about the definition of markov chain and conditional independence.','Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.',1,NULL,3),(13,'3.1.7',4,'Suppose we have three random variables $ X_0 $, $ X_1 $ and $ X_2 $ forming a Markov process with time domain $ \\{0,1,2\\} $. Then which following is right?','','','','','','','$ X_1, X_2,X_0 $ also form a Markov process.','$ X_2, X_0,X_1 $ also form a Markov process.','$ X_0, X_2,X_1 $ also form a Markov process.','$ X_2, X_1,X_0 $ also form a Markov process.','None of the above is correct.','','','','','','','','D','','','','The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present,   can be expressed as\r\n \r\n $$P(X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1}, ..., X_0=i_0)=P(X_{n+1}=j_1|X_n=i),$$\r\n \r\n for all time $n$, states $j_1,i, i_0,i_1,...$\r\n \r\n With the defintion of Markov chain, we have \r\n \\begin{eqnarray*}\r\n P(X_2 = j|X_1 = i_1, X_0 = i_0)&=& P(X_2=j|X_1 = i_1)\r\n \\\\\r\nthen ~~\\frac{P(X_2 = j,X_1 = i_1, X_0 = i_0)}{P(X_1 = i_1, X_0 = i_0)} &=& \\frac{P(X_2=j, X_1 = i_1)}{P(X_1 = i_1)}\r\n \\\\\r\n then~~\\frac{P(X_2 = j,X_1 = i_1, X_0 = i_0)}{P(X_2=j, X_1 = i_1)}&=&\\frac{P(X_1 = i_1, X_0 = i_0)}{P(X_1 = i_1)} \r\n \\\\\r\n namely~~P(X_0 = i_0|X_2=j, X_1 = i_1)&=&P(X_0=i_0|X_1 = i_1)\r\n  \\end{eqnarray*}\r\n which means $ X_2, X_1,X_0 $ is a Markov chain.',3,1,2,1,0,0,0,'A','','Oops! Please think about the definition of markov chain and conditional independence.','Congratulations! You have mastered the definition of markov chains\r\nand known how to use it.',1,NULL,3),(14,'3.1.8',4,'Mickey mouse and travels blindly independently in the maze in Example 3.2. And Donald duck follows Mickey\'s footstep.Let $ M_n $ be cell number of Mickey after n step transition and $ D_n $ be the cell number of Donald after n step transition. Then $ D_n = M_{n-1} $ for $ n\\geq1 $, with $ M_0 = D_1 = 4 $. Counting $ n = 1, 2, \\ldots $\r\n <br>\r\nThe maze is below:','theall/image/WX20170316-2301342x.png','','','','','','$ D_n^2 $ is a Markov chain.','$ 2M_n +D_n $ is a Markov chain.','$ M_nD_n $ is a Markov chain.','$ M_n - D_n $ is a Markov chain.','None of the above is correct.','','','','','','','','A','','','','In the remark of  DIY 3.1.5, the transformation in the state space should be one-to-one. The transformation as follow where we regard $ D_n = M_{n-1} $:\r\n <br>\r\n (a). $ f(x) = x^2 $\r\n <br>\r\n (b). $ f(x,y) = 2x + y $\r\n <br>\r\n (c). $ f(x,y) = xy $\r\n<br>\r\n (d). $ f(x,y) = x - y $\r\n <br>\r\n With the state space $ 0, 1, \\ldots, 8 $, $ f(x) $ in (a) is one-to-one. So the right answer is (a).',3,1,2,1,0,0,0,'E','','Oops! Please think about the definition of markov chain and conditional independence.','Congratulations! You have mastered the knowledge in the 3.1.',1,NULL,3),(15,'3.2.1',4,'The transition probability matrix is \r\n  \r\n  $$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n  $${ P} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n  \\pmatrix{\r\n  	0 &  2/3  &  1/3              \\cr\r\n  	1/2	&   0  &  1/2      \\cr\r\n  	1/2	 &   1/2  &   0          \\cr\r\n  }\r\n  $$\r\n Then, $ P_{00}^{(3)} $ is','','','','','','','1/2','1/3','1/4','1/5','None of the above is correct.','','','','','','','','C','','','','$ P_{00}^{(3)} $ means with initial point 0, the probability that it still at point 0 after 3 steps.\r\nWhen we are trying to know such kind probability, The easiest way is to compute $ P^{(3)} $. $ P^{(3)} = P\\times P \\times P$ .\r\n$$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n$${ P^2} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n\\pmatrix{\r\n	1/2 &  1/6  &  1/3              \\cr\r\n	1/4	&   7/12  &  1/6      \\cr\r\n	1/4	 &   1/3  &   5/12          \\cr\r\n}\r\n$$\r\n$$ \\qquad  \\,\\, \\,\\,\\, 0\\quad 1 \\,\\quad 2  $$\r\n$${ P^3} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n\\pmatrix{\r\n	1/4 &  *  &  *              \\cr\r\n	*	&   *  &  *      \\cr\r\n	*	 &   *  &   *       \\cr\r\n}\r\n$$\r\nWe only need the first component of $ P^3 $ which is $ P_{00}^{(3)} $ . So the right answer is c.',1,2,1,0,0,0,0,'','','Oops! Please check how to calculate transition matrix.','Congratulations! You have known how to calculate the transition matrix.',1,NULL,1),(16,'3.4.6',4,'The transition probability matrix is \r\n   $$ \\qquad  \\,\\, \\,\\,\\, 0\\qquad 1\\qquad 2  $$\r\n  $${ P} =\\matrix{ 0 \\cr 1 \\cr 2    }\r\n  \\pmatrix{\r\n  	0 &  2/3  &  1/3              \\cr\r\n  	1/2	&   0  &  1/2      \\cr\r\n  	1/2	 &   1/2  &   0          \\cr\r\n  }\r\n  $$\r\n \r\n Then, starting from state 0, the mean number of visits of state 2 before coming back to state 0 is','','','','','','','5/9','6/9','7/9','8/9','None of the above is correct.','','','','','','','','D','','','','The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i, the mean number of visits of state 2 before coming back to state 0. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 2/3w_1 + 1/3w_2\\\\\r\n w_1 &=& 1/2 w_2\\\\\r\n w_2 &=& 1 + 1/2w_1\r\n \\end{eqnarray*}\r\nSo$ w_0 = 8/9, w_1 = 2/3, w_2 = 4/3$. So the answer is d.\r\n<br>\r\n First step analysis is a very useful way to sovle this question. If we do not use this analysis, the question will be very difficult.',3,1,2,1,0,0,0,'','','Oops! Try to use first step analysis to deal with the question.','Congratulations! You have a right answer on this question about first step analysis.',1,NULL,3),(17,'3.4.7',4,'A Markov chain has one-step transition probability  matrix\r\n  \r\n  $$ \\qquad  \\,\\, ~~0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n $${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n \\pmatrix{\r\n 	0 &  1/3  &  1/3   &   1/3           \\cr\r\n 	1/3	&   1/3  &  1/3  &    0          \\cr\r\n 	1/2	 &   1/2  &   0  &   0          \\cr\r\n 	0 	&   1/2   &  1/2   &   0\r\n }\r\n $$\r\n Then, starting from state 0, the chance of visits of state 3 before visiting state 2 is','','','','','','','0','1/5','2/5','3/5','None of the above is correct.','','','','','','','','C','','','','The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i, the chance of visiting state 3 before visitng state 2. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/3w_1 + 1/3w_2 + 1/3w_3\\\\\r\n w_1 &=& 1/3 w_0 + 1/3 w_1 +1/3 w_2\\\\\r\n w_2 &=& 0\\\\\r\n w_3 &=& 1\r\n \\end{eqnarray*}\r\nSo $ w_0 = 2/5, w_1 = 1/5, w_2 = 0, w_3 = 1$. So the answer is c.',3,2,2,1,0,0,0,'','','Oops! Try to use first step analysis to deal with the question.','Congratulations! You have a right answer on this question about first step analysis.',1,NULL,3),(18,'3.4.8',4,'A Markov chain has one-step transition probability  matrix\r\n  \r\n $$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n\\pmatrix{\r\n	0 &  1/3  &  1/3   &   1/3           \\cr\r\n	1/3	&   1/3  &  1/3  &    0          \\cr\r\n	1/2	 &   1/2  &   0  &   0          \\cr\r\n	0 	&   1/2   &  1/2   &   0\r\n}\r\n$$\r\n Then, starting from state 0, the mean number of visits of state 2 from state 1 before visiting state 3, is','','','','','','','1/4','1/3','1/2','1','None of the above is correct.','','','','','','','','D','','','','The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the mean number of visits of state 2 from state 1 before visiting state 3. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/3w_1 + 1/3w_2 + 1/3w_3\\\\\r\n w_1 &=& 1/3 w_0 + 1/3 w_1 +1/3 (w_2+1)\\\\\r\n w_2 &=& 1/2w_0 + 1/2w_1\\\\\r\n w_3 &=& 0\r\n \\end{eqnarray*}\r\nSo $ w_0 = 1, w_1 = 4/3, w_2 = 5/3, w_3 = 0$. So the answer is d.',3,2,2,1,0,0,0,'','','Oops! Try to use first step analysis to deal with the question.','Congratulations! You have a right answer on this question about first step analysis.',1,NULL,4),(19,'3.4.9',4,'A Markov chain has one-step transition probability  matrix\r\n  $$ \\qquad  \\,\\,  0\\qquad 1\\qquad 2\\qquad 3  $$\r\n  $${\\bf P} =\\matrix{ 0 \\cr 1 \\cr 2 \\cr 3    }\r\n  \\pmatrix{\r\n  	1/4 &  1/4  &  1/4  &   1/4            \\cr\r\n  1/4 & 1/4  &  1/2  &    0     \\cr\r\n  0	 &   0  &   1/2  &   1/2         \\cr\r\n 0 	&   0   &  1/3   &   2/3    \r\n   }\r\n  $$\r\n  \r\n Starting from state 0, the mean number of visits of state 1 is','','','','','','','1/2','1/3','1/4','1/5','None of the above is correct.','','','','','','','','E','','','','The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the mean number of visits of state 1. So we have\r\n \\begin{eqnarray*}\r\n w_0 &=& 1/4w_0 + 1/4w_1 + 1/4w_2 + 1/4w_3\\\\\r\n w_1 &=& 1 + 1/4 w_0 + 1/4 w_1 +1/2 w_2\\\\\r\n w_2 &=& 1/2w_2 + 1/2w_3\\\\\r\n w_3 &=& 1/3w_2 + 2/3w_3\r\n \\end{eqnarray*}\r\nFirst we know $ w_2 = w_3 $. Then we can see that $ w_2 = w_3 = 0 $, since from stating 2 or 3, we can never back to state 1.\r\nSo $ w_0 = 1/8, w_1 = 3/8, w_2 = 0, w_3 = 0$. So the answer is e.',3,2,2,1,0,0,0,'','','Oops! Try to use first step analysis to deal with the question.','Congratulations! You have a right answer on this question about first step analysis.',1,NULL,3),(20,'3.4.10',4,'Toss a fair coin a number of times. You always bet on the head to make one dollor if it is a head and to lose one dollar if it is a tail. You are stopped whenever you make 3 dollar or you lose 2 dollar. Then,','','','','','','','you end up winning with 2/3 probability','you end up winning with 1/3 probability','you end up winning with 3/4 probability','you end up winning with 3/5 probability','None of the above is correct.','','','','','','','','E','','','','First we need to find the markov process in the question. We have 6 states. In order not to make you confused, we use state $ i $ to stand for the number of your money you win. Negative means you lose.\r\n<br>\r\n		So the one-step transition probability  matrix in this question is:\r\n		   $$ \\qquad \\,\\,\\quad-2 ~\\,\\quad -1 \\,~\\quad 0\\,\\,~\\quad 1 \\,\\,~\\quad 2\\,\\,~\\quad3  $$\r\n		$${\\bf P} =\\matrix{-2 \\cr -1 \\cr 0 \\cr 1 \\cr 2 \\cr 3    }\r\n		\\pmatrix{\r\n			1 &  0  &  0  &   0 & 0 &0           \\cr\r\n			1/2	&   0  &  1/2  &    0  & 0 & 0    \\cr\r\n			0	 &   1/2  &   0  &   1/2&0&  0        \\cr\r\n			0 	&   0   &  1/2   &   0&1/2&0       \\cr\r\n			0 & 0 & 0 & 1/2 &0 &1/2             \\cr\r\n			0 &0&0&0&0&1\r\n		}\r\n		$$\r\n		The key to solve this question is use first step analysis. We use $ w_i $ to stand for starting from state i,the chance we make 3 dollar before we lose 2 dollar. So we have\r\n		\\begin{eqnarray*}\r\n			w_{-2} &=& 0\\\\\r\n			w_{-1} &=& 1/2w_{-2} + 1/2w_0 \\\\\r\n			w_0 &=& 1/2w_{-1} + 1/2w_1\\\\\r\n			w_1 &=& 1/2w_0 + 1/2w_2\\\\\r\n			w_2 &=& 1/2w_1 + 1/2w_3\\\\\r\n			w_3 &=& 1\r\n		\\end{eqnarray*}\r\n	\r\n		So $ w_{-2} =0, w_{-1} = 1/5 , w_0 = 2/5, w_1 = 3/5, w_2 = 4/5, w_3 = 1$. So the answer is e.',3,2,2,2,0,0,0,'','','Oops! Try to use first step analysis to deal with the question.','Congratulations! You have a right answer on this question about first step analysis.',1,NULL,4),(21,'0.1',5,'Two events A and B each having probability 0.5 and 0.7, respectively. The probability for A and B happen at the same time should be','','','','','','','1','smaller than 0.2','at least 0.2','0','None of the above is correct.','','','','','','','','C','','','','The key to solve this question is  $$ P(A\\cap B) = P(A) +P(B) - P(A\\cup B)$$\r\n With  $ P(A) = 0.5  $ and $ P(B) = 0.7 $, $ P(A\\cap B) $ = $ 1.2 -P(A\\cup B) $. Although we do not know the value of $ P(A\\cup B) $. But it must between 0.7 and 1. Larger than $ P(B) $ but less than 1. So the value of $ P(A\\cap B) $ must between 0.2 and 0.5. So the answer is c.',1,1,2,2,0,0,0,'A','','Don\'t worry! Just try it again.','Congratulations! You have successfully finished your first question.',1,NULL,1),(22,'0.2',5,'There are three events: $ A $ and $ B $ and $ C $. We know $ P(A|B) = P(B|C) = 0.5$. Then $ P(A|C) $ should be','','','','','','','0','0.25','0.5','1','None of the above is correct.','','','','','','','','E','','','','Unfortunately, there is no answer for what is the value of $ P(A|C) $. In the different case, the value of $ P(A|C) $ is different. Think about the two later case.\r\n<br>\r\n The first one is $ A, B, C $ are independent with $ P(A) = P(B) = P(C) = 0.5$. We can see they satisfy the condition in the question.  $ P(A|C) $ there is 0.5.\r\n<br>\r\n The second one is $ A =C $ and they are independent of $ B $. We also have  $ P(A) = P(B) = P(C) = 0.5$. They satisfy the condition in the question.  $ P(A|C) $ there is 1.\r\n So there is no answer for what is the value of $ P(A|C) $.\r\n<br>\r\n Attention! $ P(A|B)  P(B|C) \\neq P(A|C)$.',2,1,2,2,0,0,0,'B','','Don\'t worry! Just try it again.','Congratulations! You have the right answer.',1,NULL,2),(23,'0.3',5,'For a random variable $ X $, which the statement below is correct?','','','','','','','If $ X  \\sim$ Binomial$(n,p)$, then $ Var X = np $.','If $ X  \\sim$ Poisson$(\\lambda)$, then $ Var X = \\lambda^2 $.','If $ X  \\sim$ $N(\\mu,\\sigma^2)$, then $ Var X = \\mu $.','If $ X \\sim $ Geometric$ (p) $, then $ Var X $ = $ \\frac{p}{(1-p)^2} $','None of the above is correct.','','','','','','','','D','','','','We just see it one by one.\r\n<br>\r\n(a). If $ X  \\sim$ Binomial$(n,p)$, then $ Var X = np(1-p) $.\r\n<br>\r\n(b). If $ X  \\sim$ Poisson$(\\lambda)$, then $ Var X = \\lambda $.\r\n<br>\r\n(c). If $ X  \\sim$ $N(\\mu,\\sigma^2)$, then $ Var X = \\sigma^2 $.\r\n<br>\r\n(d). It is the right answer.',2,1,1,0,0,0,0,'B','','Don\'t worry! Next time you will remember the variance.','Congratulations! You have the correct answer.',1,NULL,1),(24,'0.4',5,'Two random variables $ X $ and $ Y $ follow the same distribution. Then','','','','','','','The distribution of $ X-Y $ must be symmetric about 0.','The median of $ X-Y $ must be zero.','The median of $ X+Y $ is twice of the median of $ X $.','The mean of $ X-Y $, if finite, must be 0.','None of the above is correct.','','','','','','','','D','','','','The key to solve this question is that we do not know the relationship of $ X $ and $ Y $. $ X $ and $ Y $ may be independent or $ X = Y $, even $ X = -Y  $ in same speical case. So it is impossible to know the the distribution or the median of $ X - Y $ or $ X + Y $. However, we can know the mean of $ X + Y $. If the mean is finite, $ E[X-Y] = E[X]-E[Y]=0$. So the answer d is correct.\r\n<br>\r\nRemark: If you are still not sure even if you get correct answer, I suggest you think about whether a, b, c is wrong. Thinking about the counterexample will help you a lot.',2,1,2,0,0,0,0,'B','','Don\'t worry! Just try it again.  If you cannot exclude wrong answers, just choose the one which is most likely correct.','Congratulations! You have correct answer on this little difficult question.',1,NULL,2),(25,'0.5',5,'Toss a fair coin ten times. The chance there is neither consecutive heads nor consecutive tails is','','','','','','','$ 2^{-1} $','$ 2^{-5} $','$ 2^{-9} $','$ 2^{-10} $','None of the above is correct.','','','','','','','','C','','','','First we count the number of total case. It is $ 2^{10} $. Then we count the number of case in which there is neither consecutive heads nor consecutive tails. We use H to stand for head and T to stand for tail. There are only $ 2 $ cases. It is \"HTHTHTHTHT\" and \"THTHTHTHTH\". So the chance is  $ 2/2^{10} $, which is $ 2^{-9} $. So the right answer is c.',1,2,1,0,0,0,0,'D','','Don\'t worry! Just try it again.','Congratulations! You get the correct answer on this question.',NULL,NULL,1),(26,'0.6',5,'Two random variables have zero correlation. Then,','','','','','','','They are independent.','If they are normal random variables, they are independent.','If they are Poisson random variables, they are independent.','If they are random variables following uniform distributions, they are independent.','None of the above is correct.','','','','','','','','B','','','','The zero correlation may not be independence. But the independence must be zero correlation. Only in some special case, the zero correlation means independence. The normal distribution has that feature. But Poisson distribution and uniform distribution do not have that feature.',3,0,1,0,0,0,0,'E','','Don\'t worry! Just try it again.','Congratulations! You get the correct answer on this question.',1,NULL,1),(27,'0.7',5,'Let $ X = -10Y+10 $. Let $ r_1 $ be the correlation between $ X $ and $ Z $ and $ r_2 $ be the correlation between $ Y $ and $ Z $. Then,','','','','','','','$ r_1 = r_2 $','$ r_1 =10 r_2 $','$ r_1 = -10r_2 $','$ r_1 = -r_2 $','None of the above is correct.','','','','','','','','D','','','','We should know how to calculate the correlation between two random variables.\r\n$$ r_{xy} =\\frac{E(X-EX)(Y-EY)}{\\sqrt{VarX VarY}}$$\r\nW.L.O.G. We assume that $ EY=0, EZ=0 $, so \r\n\\begin{eqnarray*}\r\nr_{xz}  &=& \\frac{E(-10YZ)}{\\sqrt{Var(10Y) VarZ}}\\\\\r\n&=&-\\frac{E(YZ)}{\\sqrt{VarY VarZ}}\\\\\r\n&=&-r_{yz}\r\n\\end{eqnarray*}\r\nSo $ r_1 = -r_2 $, which is answer d.\r\n<br>\r\nRemark: The assumption $ EY=0, EZ=0 $ does not affect the final answer. If you are not sure, you can prove it bu yourself. It will help you a lot.',2,2,1,0,0,0,0,'','C','Don\'t worry! Just try it again.','Congratulations! You get the correct answer on this question.',1,NULL,2),(28,'0.8',5,'The density of a random variabel $ X $ is $$f(x)  \\propto x^{-1/2} , x\\in [0,1]$$\r\nand $ f(x) = 0 $ for $ x \\notin [0,1] $. Here $\\propto$ means proporional to. Then, the mean of $ X $ is','','','','','','','$ 1/2 $','$ 1/\\sqrt{2} $','$ 1/3 $','$ 1/4 $','None of the above is correct.','','','','','','','','C','','','','With the information in the question, we know that density function $ f(x) = Cx^{-1/2}, x\\in [0,1]$ with.\r\nSince $ \\int_{0}^{\\infty} f(x) dx=1$. So $ \\int_{0}^{1} Cx^{-1/2} dx=1$. So the $ C= 1/2 $.\r\n<br>\r\nNow we know the density function. We can calculate the expectation of X.\r\n$ EX = \\int_{0}^{1} f(x)x dx$ = $ \\int_{0}^{1} 1/2 \\cdot x^{-1/2} \\cdot x dx  $ = 1/3.\r\n<br>\r\nSo the right answer is (c).',2,2,1,0,0,0,0,'A','','Don\'t worry! Just try it again.','Congratulations! You get the correct answer on this question.',1,NULL,1),(29,'0.9',5,'Toss a fair coin n times. Let $ S_n $ be the total number of heads. If $ n $ is large enough, then,','','','','','','','$ S_n $ is close to the standard normal distribution.','$ S_n $ is close to a Poisson distribution.','$ S_n $ follows binomial distribution.','$ S_n $ follows uniform distribution over integers $ 0,\\ldots,n. $','None of the above is correct.','','','','','','','','C','','','','From the defintion of binomial distribution, it is easy to see that $ S_n $ follows binomial distribution.\r\n<br>\r\nRemark: Think about whether a, b are wrong.\r\n<br>\r\na. With central limit theorem, $ \\frac{S_n - \\frac{1}{2}n}{\\sqrt{n/4}}$ is close to standard normal distribution. Not  $ S_n $ is close to the standard normal distribution.\r\n<br>\r\nb. Poisson approximation only holds on the assumption that $ p $ is very small. Here $ p=1/2 $.',2,0,1,0,0,0,0,'A or B','','Don\'t worry! Just try it again.','Congratulations! You get the correct answer on this question.',1,NULL,2),(30,'0.10',5,'Suppose $ P(|X|<1) = 1$ and $ P(|Y| =2) = 1 $. Then,','','','','','','','The standard deviation of $ X $ is smaller than that of $ Y $.','The mean of $ X $ is smaller than that of $ Y $.','The variance of $ X $ is larger than that of $ Y $.','The median of $ X $ is equal to the median of $ Y $.','None of the above is correct.','','','','','','','','E','','','','It is not easy question. We need to check them one by one.\r\n<br>\r\na. We choose $ Y = 2 $. So the standard deviation of $ Y $ is 0.\r\n<br>\r\nb. We choose $ P(Y=2)=1/2 $ and $P(Y=-2)=1/2 $. And we choose $ P(X>0) =1 $. So the mean of $ X $ is smaller than that of $ Y $, which is 0 here.\r\n<br>\r\nc. We choose $ X = 1/2 $. So $ Var(X) = 0 $.\r\n<br>\r\nd. We choose the case in a. The median of $ Y $ is 2. But the meidian of $ X$ is smaller than 1.',3,0,2,1,0,0,0,'A, B, C or D','','Don\'t worry! Just try it again.','Congratulations! You get the correct answer on this question. And you have finished the part of quiz 0.',1,NULL,3),(31,'3.1.1',1,'Let $\\xi_0=0$ and, for $i \\geq 1$,\r\n$\\xi_i$ =\r\n $$\\cases{ 1 & if the $i$-th toss is a Head (with probability $p$) \\cr\r\n0 & if the $i$-th toss is a Tail (with probability $1-p$)}$$\r\nSet $X_n= \\sum_{i=0}^n \\xi_i $, $n \\geq 0$. $X_n$ is the random number which counts\r\nthe number of Heads up to the $n$-th toss.What\'s the value of\r\n$P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i)?$','','','','','','','= \\cases{ P(\\xi_{n+1}=1) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\  $','$ { P(\\xi_{n+1}=1)}=1$','None of the above is correct.','','','','','','','','','','A','','','','Just straight compute the probability\r\n\\begin{eqnarray*}\r\n&& P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) \\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1|X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0|X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ p & if $j=i+1$ \\cr\r\n1-p & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& \\cases{ P(\\xi_{n+1}=1|X_n=i) & if $j=i+1$ \\cr\r\nP(\\xi_{n+1}=0|X_n=i) & if $j=i$ \\cr\r\n0 & otherwise \\cr\r\n}\\\\\r\n&=& P(X_{n+1}=j |X_n=i)\r\n\\end{eqnarray*} <br>\r\n\r\n\r\nIt implies, once the present $X_n$ is fixed, the past history $X_0, ..., X_{n-1}$, shall not\r\naffect the future distribution of $X_{n+1}$.}\r\nHere, we are using time $n$ as present, time before $n$ is past, and time beyond $n$ is\r\nfuture.\r\n\r\nThe above statement is the same as saying that\r\nthe future $\\{X_k: k\\geq n+1\\}$ and the past $\\{X_k: k\\leq n-1\\}$ are\r\n$conditionally$ $independent$ given the present $X_n$ taking any fixed value.<br>\r\n$remark$   <br>Not only $n+1$ but, all future distribution $X_{n+1}, X_{n+2}, ....$ shall depend on\r\nthe past and now only through now.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Alas! Please check the definition of markov chain and conditional independence.','Congratulations! You have mastered the definition of markov chains\r\nand conditional independence.',1,3,1),(32,'3.1.2',1,'({ Mickey in Maze})\r\nMickey mouse travels in a maze with nine $3\\times 3$ cells. The cells are numbered\r\nas 0, 1, ..., 8 from left to right and top down. Each step Mickey travels\r\nfrom where it is to one\r\nof the surrounding connected cells with equal chance.Let $X_n$ denote the\r\ncell number of Mickey at step $n$. $(X_0=4)$.Is The process$\\{X_n: n=0, 1, 2, ...\\}$ a Markov chain?','theall/image/Example_3-1-2.PNG','','','','','','yes','no','None of the above is correct.','','','','','','','','','','A','','','','The definition of MC $\\{X_n\\}$, namely the past and future are\r\n independent given any fixed state of the present, <br>\r\n \\begin{eqnarray*}\r\n&& P(X_{n+1}=j | X_0=0, X_1 = i_1, ..., X_{n-1}= i_{n-1}, X_n = i) \\\\\r\n&=&\r\nP(X_{n+1}=j |X_n=i).\r\n\\end{eqnarray*}\r\nSuppose currently Mickey is in cell 5, for example,\r\nthe future movement or path of Mickey is irrelevant with the past movement\r\nor path of Mickey. In other words, how Mickey has got to cell 5 in the past\r\nhas nothing to do with how Mickey would move around in the future.\r\nThe process $\\{X_n: n=0, 1, 2, ...\\}$ is a Markov chain.<br>\r\n{ \\it Example for fun} (``First Blood.\") John Rambo only  obeys\r\nthe order from  Colonel Samuel Trautman,\r\nwho supposedly only  obeys the order from the Pentagon. Then\r\nPentagon $\\longrightarrow$ Trautman $\\longrightarrow$\r\nRambo forms a $MC$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','well,try again?','good job!',1,NULL,1),(33,'3.1.3',1,'({Mickey in Maze run more steps)\r\nMickey mouse travels in a maze with nine $3\\times 3$ cells. The cells are numbered\r\nas 0, 1, ..., 8 from left to right and top down. Each step Mickey travels\r\nfrom where it is to one\r\nof the surrounding connected cells with equal chance.Compute $P^{(3)}_{4\\, 8}$ and $P^{(3)}_{1\\, 8}.$','','','','','','','$P^{(3)}_{4\\, 8}$=$0.5$,$P^{(3)}_{1\\, 8}$=$0.5$','$P^{(3)}_{4\\, 8}$=$0$,$P^{(3)}_{1\\, 8}$=$0$','None of the above is correct.','','','','','','','','','','B','','','','Remember the basic definition and method of computing the transition  probability,\r\n  write\r\n$$P^{(3)}_{4\\, 8} = \\sum_{k=0}^\\infty P_{4\\, k} P_{k\\, 8}^{(2)}\r\n=\\sum_{k=1, 3, 5, 7} P_{4\\, k} P_{k\\, 8}^{(2)} = 1/4\\sum_{k=1, 3, 5, 7}   P_{k\\, 8}^{(2)}=0;$$\r\n$$P^{(3)}_{1\\, 8} = 1/3 \\times 1/2 \\times 1/3 + 1/3\\times 1/4 \\times 1/3 +\r\n1/3 \\times 1/4 \\times 1/3 =0.$$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',2,3,2),(34,'3.1.4',1,'( An Inventory Mode)\r\nLet $X_n$ be the number of TV sets at a store in the end of day $n$ with $X_0=2$. Let $\\xi_n$ be the\r\nsales of the number of TVs on day $n$. Assume $\\xi_1, \\xi_2, ...$ are iid (independent, identically distributed) such that\r\n$$P(\\xi_n=i) = \\cases{0.5 & $i=0 $\\cr 0.4 & $i =1$ \\cr  0.1 & $i=2$ }$$\r\nAt the end of any day $n$, if $X_n=0$ or $-1$, two TVs will be sent to the store overnight. Moreover,\r\nin the case of  $X_n=-1$,\r\nanother TV will be sent directly to the customer\'s house. If $X_n =1$ or $2$,\r\nnothing happens. With this inventory policy,\r\n$$X_{n+1}= \\cases{ X_n - \\xi_{n+1} & if $X_n=1, 2$ \\cr 2- \\xi_{n+1} & if $X_n=-1, 0$.}$$\r\nThen, $X_0, X_1, X_2, ..., $ is a $MC$ with state space $\\{-1, 0, 1, 2\\}$ .What\'s the one-step trasition probability from 1 to 2($p_{1\\,2}$)','','','','','','','1','0','None of the above is correct.','','','','','','','','','','B','','','','Remember the basic definition and method of computing the one-step transition  probability,<br>\r\n $X_0, X_1, X_2, ..., $ is a $MC$ with state space $\\{-1, 0, 1, 2\\}$ and with one-step\r\ntransition probability\r\n$$\\qquad \\,\\,\\,\\,\\,\\,\\, -1 \\qquad 0 \\qquad 1 \\qquad 2$$\r\n$$P=\\matrix{-1 \\cr 0 \\cr 1 \\cr 2}\\pmatrix{0 &0.1 &0.4 &0.5 \\cr 0 &0.1 &0.4 &0.5\\cr\r\n0.1 &0.4 &0.5 &  0 \\cr 0 &0.1 &0.4 &0.5 }.$$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(35,'3.1.5',1,'{ ({ The Ehrenfest Model}) <br>\r\nThere are $2N$ particles in a jar separated by a membrane into two chambers A and B. Let\r\n$Y_n$ be the number of particles in A after $n$ crossings. Each crossing is a particle from\r\nA to B or from B to A. Assume when any once crossing happens, it happens to any one of the\r\n$2N$ particles with\r\neach equal chance $1/2N$.Then $Y_n, n\\geq 0$ is a $MC$ with state space $\\{0, 1, 2, ..., 2N\\}$.Compute the\r\n$P_{ij}$','theall/image/Example_3-1-5.PNG','','','','','','0.5','$1/3$','None of the above is correct.','','','','','','','','','','C','','','','Write down the one-step transition,easily see that\r\n $$P_{ij}= P(Y_{n+1}=j|Y_n=i) = \\cases{ i/(2N) & if $j=i-1$ \\cr  1-i/(2N) & if $j=i+1$ \\cr\r\n0 & else; } $$\r\nfor $0 \\leq i \\leq j \\leq 2N$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','wrong,another try?','good job!',2,3,2),(36,'3.1.6',1,'Repeatedly toss a fair coin a number of times.  What\'s the expected number of\r\ntosses till the first two consecutive heads occur?','','','','','','','0','$1/2$','none of above is correct.','','','','','','','','','','C','','','','Let $\\xi_0=0$ and, for $i \\geq 1$,\r\n$$\\xi_i = \\cases{ 1 & if the $i$-th toss is a Head (with probability $1/2$) \\cr\r\n0 & if the $i$-th toss is a Tail (with probability $1/2$)}$$\r\nSet, for $n\\geq 1$,\r\n$$X_n= \\cases{ 0 & if the $n$-th toss is tail  \\cr\r\n1 & if $\\xi_{n-1}=0, \\xi_n=1$ \\cr\r\n2 & if $\\xi_{n-1}=1, \\xi_n=1$.}\r\n$$\r\nThen,\r\n $X_n$ is a $MC$ with state space $\\{0, 1, 2\\}$ and\r\n transition matrix\r\n \\begin{eqnarray*}\r\n && \\qquad \\,\\,\\, 0 \\qquad 1 \\qquad 2\\\\\r\n  P  &=& \\matrix{0\\cr 1 \\cr 2} \\pmatrix{\r\n 1/2 & 1/2 & 0 \\cr\r\n 1/2 & 0 & 1/2 \\cr\r\n 1/2 & 0 & 1/2\r\n }.\r\n \\end{eqnarray*}\r\nFor example,\r\n$$P_{00}= P(X_{n+1}=0|X_n=0)=P(\\xi_{n+1}=0|\\xi_n=0)=P(\\xi_{n+1}=0)=1/2.$$\r\nFor $k \\geq 1$, let $T_k= \\min\\{ n \\geq 0: X_{n+k}=2\\} $, which is the minimum number of\r\nadditional tosses till the first two consecutive heads occur, starting from (excluding)\r\nthe $k$-th toss. Then\r\n$$T_k=\\cases{0 &  if  $X_k=2$ \\cr\r\n T_{k+1} + 1 & if $X_k= 0$ or $1$ }.\r\n $$\r\n Let\r\n$$ w_0 = E( T_k|X_k=0) \\qquad w_1 = E(T_k |X_k=1).$$\r\nThen,\r\n\\begin{eqnarray*}\r\nw_0 &=& E(T_1 |X_1=0) = E(T_1 1_{\\{X_{1+1}=0 \\, {\\rm or} \\, 1  {\\rm or} \\, 2\\}} | X_1=0) \\\\\r\n&=& E(T_1 1_{\\{X_{2}=0 \\}} | X_1=0) + E(T_1\r\n1_{\\{X_{2}=  1\\}} | X_1=0) + E(T_1\r\n1_{\\{X_{2}=  2\\}} | X_1=0)\\\\\r\n&=&E(T_1|X_2=0, X_1=0)P(X_2=0|X_1=0)  \\\\\r\n&& + E(T_1|X_2=1, X_1=0)P(X_2=1|X_1=0)\r\n  + E(1 \\time 1_{\\{X_{2}=  2\\}} | X_1=0)\\\\\r\n  &=&E(T_2+1|X_2=0, X_1=0)P(X_2=0|X_1=0)  \\\\\r\n&& + E(T_2+1|X_2=1, X_1=0)P(X_2=1|X_1=0)\r\n  + P_{02}\\\\\r\n&=&(1+w_0)P_{00} + (1+w_1)P_{01} +  P_{02} \\\\\r\n&=& 1 + w_1P_{01}+ w_0P_{00}\r\n\\end{eqnarray*}\r\nLikewise,\r\n$$w_1 = 1 + P_{10} w_0 + P_{11} w_1. $$\r\nTogether, we have\r\n\\begin{eqnarray*}\r\n&& w_0= 1 + P_{00} w_0 + P_{01} w_1 = 1 + (1/2)(w_0+w_1) \\\\\r\n&& w_1 = 1 + P_{10} w_0 + P_{11} w_1 = 1 + (1/2) w_0 .\r\n\\end{eqnarray*}\r\nSolving the equation, we have $w_0=6$ and $w_1=4$.\r\nSince $X_1=0$ or $1$ with half chance.\r\nThe mean number of tosses till the first HH occur is\r\n$$ 1/2(w_0 + w_1) + 1 = 6.$$\r\n\r\n\r\n\r\nThe above example in fact, for the purpose of illustrating\r\n the method of first-step analysis, demonstrates a hard way of solving the problem.\r\n For this particular problem,\r\nthere is actually an easier method without invoking\r\nthe {\\it MC}  $\\{X_n, n\\geq 1 \\}$, but, rather, directly based on $\\{\\xi_i, i \\geq 1\\}$\r\n (Please DIY).\r\nThe idea is contained   in\r\nthe example called a dice game called craps presented in review and\r\nin the following example as well.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',2,3,3),(37,'3.1.1',2,'A Markov chain $X0$;$X1$; : : : on states $0, 1, 2$ has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.1& 0.2   &0.7    \\cr\r\n \r\n &    0.9 & 0.1  &0     \\cr\r\n  &  0.1   & 0.8       &    1\r\n}\r\n\\end{eqnarray*}\r\n\r\nand initial distribution $p_{0}$=Pr{$X_{0}$=0}=0.3,$p_{1}$=Pr{$X_{0}$=1}=0.4,and$p_{2}$=Pr{$X_{0}$=2}=0.3,Determine\r\nPr{$X_{0}$=9,$X_{1}$=1,$X_{2}$=2}.','','','','','','','1','0','none of above is correct','','','','','','','','','','B','','','','easily compute,Pr{$X_{0}$=$0$,$X_{1}$=$1$,$X_{2}$=$2$}=Pr{$X_{1}$=$1$,$X_{2}$=2|$X_{0}$=0}$*$Pr{$X_{0}$=$1$}=\r\nPr{$X_{2}$=2|$X_{0}$=0,$X_{1}$=$1$}$*$Pr{$X_{1}$=1|$X_{0}$=0}$*$Pr{$X_{0}$=$1$}=$p_{12}$$*$$p_{01}$$*$$p_{0}$=0$*$0.2$*$0.3=0.<br>',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(38,'3.1.2',2,'A Markov chain $X0$;$X1$;$...$on states $0, 1, 2$ has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.7& 0.2   &0.1    \\cr\r\n \r\n &    0 & 0.6  &0.4     \\cr\r\n  &  0.5   & 0       & 0.5\r\n}\r\n\\end{eqnarray*}\r\n\r\nDetermine the conditional probability,Pr{$X_{2}$=1,$X_{3}$=1|$X_{1}$=2} and Pr{$X_{1}$=1,$X_{2}$=1|$X_{0}$=0}','','','','','','','1;1','0.06;0.06','none of above is correct','','','','','','','','','','B','','','','we can compute by the definition,Pr{$X_{2}$=$1$,$X_{3}$=$1$|$X_{1}$=$0$}=Pr{$X_{3}$=$1$|$X_{1}$=$0$,$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=Pr{$X_{3}$=$1$|$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=$p_{1,1}$$*$$p_{0,1}$=0.1$*$0.6=0.06.We already us the markov property.<br>\r\nThe same way ,we can get  Pr{$X_{1}$=1,$X_{2}$=1|$X_{0}$=0}=0.06.Note the conditional probability doesn\'t denpend on \r\nthe initial time.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(39,'3.1.3',2,'A Markov chain $X0$;$X1$;$...$on states $0, 1, 2$ has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.6& 0.3   &0.1    \\cr\r\n \r\n &    0.3& 0.3  &0.4     \\cr\r\n  &  0.4   & 0.1       & 0.5\r\n}\r\n\\end{eqnarray*}\r\n\r\nIf it is known that the process starts in state $X_{0}$=1,Determine the  probability,Pr{$X_{0}$=1,$X_{1}$=0,$X_{2}$=2}.','','','','','','','0.3','0.03','none of above is correct','','','','','','','','','','B','','','','we can compute by the definition,Pr{$X_{0}$=$1$,$X_{1}$=$0$,$X_{2}$=$2$}=Pr{$X_{2}$=$2$|$X_{1}$=$0$,$X_{0}$=$1$}$*$Pr{$X_{1}$=$0$|$X_{0}$=$1$}=Pr{$X_{2}$=$2$|$X_{1}$=$0$}$*$Pr{$X_{1}$=$0$|$X_{0}$=$1$}=$p_{1 0}$$*$$p_{0 2}$=0.3$*$0.1=0.03.<br>\r\nIn the process,We already use the markov property.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(40,'3.1.4',2,'A Markov chain $X0$;$X1$;$...$on states $0, 1, 2$ has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.1& 0.1   &0.8   \\cr\r\n \r\n &    0.2& 0.2  &0.6    \\cr\r\n  &  0.3   & 0.3      & 0.4\r\n}\r\n\\end{eqnarray*}\r\n\r\nDetermine the conditional probability,Pr{$X_{1}$=1,$X_{2}$=1|$X_{0}$=0} and Pr{$X_{2}$=1,$X_{3}$=1|$X_{1}$=0}','','','','','','','0.02;0.03','0.02;0.03','none of above is correct','','','','','','','','','','B','','','','we can compute by the definition,Pr{$X_{2}$=$1$,$X_{3}$=$1$|$X_{1}$=$0$}=Pr{$X_{3}$=$1$|$X_{1}$=$0$,$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=Pr{$X_{3}$=$1$|$X_{2}$=$1$}$*$Pr{$X_{2}$=$1$|$X_{1}$=0}=$p_{1,1}$$*$$p_{0,1}$=0.1$*$0.2=0.02.We already use the markov property.<br>\r\nThe same way ,we can get  Pr{$X_{1}$=1,$X_{2}$=1|$X_{0}$=0}=0.02.Note the conditional probability doesn\'t denpend on initial time.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(41,'3.1.5',2,'A Markov chain $X0$;$X1$;$...$on states $0, 1, 2$ has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.3& 0.2   &0.5  \\cr\r\n \r\n &    0.5& 0.1  &0.4    \\cr\r\n  &  0.5   & 0.2      & 0.3\r\n}\r\n\\end{eqnarray*}\r\nand initial distribution $p_{0}$=0.5,$p_{1}$=0.5.\r\nDetermine the  probability,Pr{$X_{1}$=1,$X_{2}$=1,$X_{0}$=0} and Pr{$X_{2}$=1,$X_{3}$=1,$X_{1}$=0}','','','','','','','0.008;0.03','0.02;0.02','none of above is correct','','','','','','','','','','A','','','','we can only compute by the definition,using the idea of slice universe,Pr{$X_{2}$=$1$,$X_{3}$=$1$,$X_{1}$=$0$}=Pr{$X_{2}$=$1$,$X_{3}$=$1$,$X_{1}$=$0$,$X_{0}$=$0$}+Pr{$X_{2}$=$1$,$X_{3}$=$1$,$X_{1}$=$0$,$X_{0}$=$1$} =$p_{0}$$*$$p_{0 0}$$*$$p_{0 1}$$*$$p_{1 1}$+$p_{1}$$*$$p_{1 0}$$*$$p_{0 1}$$*$$p_{1 1}$=0.5$*$0.3$*$0.2$*$0.1+0.5$*$0.5$*$0.2$*$0.1=0.008.<br>\r\nPr{$X_{1}$=1,$X_{2}$=1,$X_{0}$=0}is easily get as $p_{0}$$*$$p_{0 1}$$*$$p_{1 1}$=0.03',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(42,'3.2.1',2,'A Markov chain{$X_{N}$}on the states 0;1;2 has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.1& 0.2   &0.7  \\cr\r\n \r\n &    0.2& 0.2  &0.6    \\cr\r\n  &  0.6 & 0.1      & 0.3\r\n}\r\n\\end{eqnarray*}\r\nWhat is the Pr{$X_{3}$=$1$|$X_{1}=0$}  and Pr{$X_{3}$=$1$|$X_{0}=0$} ?<br>\r\nhint: try compute the second step trasition matrix first.\r\nsolution:','','','','','','','0.13;0.16','0.16;0.13','none of above is correct','','','','','','','','','','A','','','','By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P^{2}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.47& 0.13  &0.4  \\cr\r\n \r\n &    0.42& 0.14  &0.44    \\cr\r\n  &  0.26 & 0.17      & 0.57\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{3}$=$1$|$X_{1}=0$}=$P_{0 1}^{2}$=0.13<br>\r\nPr{$X_{3}$=$1$|$X_{0}=0$} =$\\sum_{i=0}^{2}$Pr{$X_{3}$=$1$,$X_{1}$=$i$|$X_{0}=0$}=$\\sum_{i=0}^{2}$Pr{$X_{3}$=$1$|$X_{0}=0$,$X_{1}$=$i$}$*$\r\nPr{$X_{1}$=$i$|$X_{0}$=$0$} =$\\sum_{i=0}^{2}$Pr{$X_{3}$=$1$|$X_{1}$=$i$}$*$\r\nPr{$X_{1}$=$i$|$X_{0}$=$0$}=$\\sum_{i=0}^{2}$$P_{i 0}^{2}$$*$\r\nPr{$X_{1}$=$i$|$X_{0}$=$0$}=0.16.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(43,'3.2.2',2,'A particle moves among the states 0,1,2 according to a Markov process whosehas the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0& 0.5   &0.5  \\cr\r\n \r\n &    0.5& 0  &0.5    \\cr\r\n  &  0.5 & 0.5      & 0\r\n}\r\n\\end{eqnarray*}\r\nLet $X_{n}$ denote the position of the particle at the $n$th move,Calculate Pr{$X_{4}$=$0$|$X_{0}$=0}?<br>\r\nhint: try compute the n step trasition matrix first.','','','','','','','0.375','0.3125','none of above is correct','','','','','','','','','','A','','','','By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad\\quad1\\quad\\quad\\quad2\\\\\r\n {\\bf P^{4}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.375& 0.3125  &0.3125  \\cr\r\n \r\n &    0.3125& 0.375  &0.3125    \\cr\r\n  &  0.3125 & 0.3125      & 0.375\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{4}$=$0$|$X_{0}$=$0$}=0.375<br>',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(44,'3.2.3',2,'A Markov chain $X0$;$X1$; : : :  who has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.7& 0.2   &0.1  \\cr\r\n \r\n &    0.& 0.6  &0.4    \\cr\r\n  &  0.5 & 0      & 0.5\r\n}\r\n\\end{eqnarray*}\r\n\\Calculate Pr{$X_{4}$=$1$|$X_{0}$=0}andPr{$X_{3}$=$1$|$X_{0}$=0} ?<br>\r\nhint: try compute the n step trasition matrix first.','','','','','','','0.254;0.264','0;1','none of above is correct','','','','','','','','','','A','','','','By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad\\quad1\\quad\\quad\\quad2\\\\\r\n {\\bf P^{4}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.4636& 0.254  &0.2824  \\cr\r\n \r\n &    0.444& 0.2256  &0.3304    \\cr\r\n  &  0.524 & 0.222      & 0.254\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{4}$=$1$|$X_{0}$=$0$}=0.254<br>\r\nIn the same way we can get Pr{$X_{3}$=$1$|$X_{0}$=$0$}=0.264\r\nanother way is using the slicing universe,try it.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(45,'3.2.4',2,'A Markov chain $X0$;$X1$; : : :  who has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.6& 0.3   &0.1  \\cr\r\n \r\n &    0.3& 0.3  &0.4    \\cr\r\n  &  0.4 & 0.1     & 0.5\r\n}\r\n\\end{eqnarray*}\r\nIf it is known that the process starts in state $X_{0}$=1,determine the probability Pr{$X_{2}$=2}?','','','','','','','1','0.54','none of above is correct','','','','','','','','','','C','','','','By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P^{2}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.49& 0.28  &0.23  \\cr\r\n \r\n &    0.43& 0.22  &0.35    \\cr\r\n  &  0.47& 0.22      & 0.33\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{2}$=$1$}=Pr{$X_{2}$=$1$|$X_{0}$=$0$}=0.49<br>',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(46,'3.2.5',2,'A Markov chain $X0$;$X1$; : : :  who has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.1& 0.1   &0.8  \\cr\r\n \r\n &    0.2& 0.2  &0.6    \\cr\r\n  &  0.3 & 0.3    & 0.4\r\n}\r\n\\end{eqnarray*}\r\ndetermine the conditional probability Pr{$X_{3}$=$1$|$X_{1}$=$0$} and Pr{$X_{2}$=$1$|$X_{0}$=$0$} ?','','','','','','','1;1','0.27;0.27','none of above is correct','','','','','','','','','','B','','','','By the definition<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad\\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P^{2}}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.27& 0.27  &0.46  \\cr\r\n \r\n &    0.24& 0.24  &0.52    \\cr\r\n  &  0.21& 0.21      & 0.58\r\n}\r\n\\end{eqnarray*}\r\nSo that Pr{$X_{3}$=$1$|$X_{1}$=$0$} =Pr{$X_{2}$=$1$|$X_{0}$=$0$}=$p_{0 1}^{2}$=0.27<br>\r\nNote that we already use that trasition probability do not dependend on initial time!',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(47,'3.2.6',2,'A Markov chain $X0$;$X1$;$...$on states $0, 1, 2$ has the transition probability matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,0\\quad\\quad1\\quad\\quad2\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr 1 \\cr 2  }\r\n  \\pmatrix{\r\n&  0.3& 0.2   &0.5  \\cr\r\n \r\n &    0.5& 0.1  &0.4    \\cr\r\n  &  0.5   & 0.2      & 0.3\r\n}\r\n\\end{eqnarray*}\r\nand initial distribution $p_{0}$=0.5,$p_{1}$=0.5.\r\nDetermine the  probability,Pr{$X_{1}$=1} and Pr{$X_{2}$=1,$X_{3}$=$0$}?','','','','','','','0.42;0.416','0.42;0.181','none of above is correct','','','','','','','','','','A','','','','Use the idea of slicing universe,\r\nPr{$X_{3}$=$0$}=Pr{$X_{3}$=$0$,$X_{0}$=$0$}+Pr{$X_{3}$=$0$,$X_{0}$=$1$}=Pr{$X_{3}$=$0$|$X_{0}$=$0$}$*$Pr{$X_{0}$=$0$}+Pr{$X_{3}$=$0$|$X_{0}$=$1$}$*$Pr{$X_{0}$=$1$}=$p_{0 0}^{3}$$*$$p_{0}$+$p_{1 0}^{3}$$*$$p_{1}$=\r\n0.412$*$0.5+0.42$*$0.5=0.416.\r\nThe same way we get Pr{$X_{2}$=$0$}=$p_{0 0}^{2}$$*$$p_{0}$+$p_{1 0}^{2}$$*$$p_{1}$=\r\n0.44$*$0.5+0.4$*$0.5=0.42.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(48,'3.1.2',4,'For three events A, B and C,  we have the following three statements\r\n: (i) $P(A\\cap B|C)= P(A|C)P(B|C)$; (ii). $P(A|C \\cap B) = P(A|C)$;\r\nand (iii) $P(B|A\\cap C) = P(B|C)$. (Assume all quantities here are well defined.) Which of them are equivalent?\r\nNotice that statement (i) says that A and B are conditionally independent given C.','','','','','','','(i) and (ii)','(ii) and (iii)','(i) and (iii)','(i) and (ii) and (iii)','','','','','','','','','D','','','','(i) $\\Longrightarrow$ (ii). Write\r\n$$P(A|C\\cap B)={ P(A \\cap C \\cap B) \\over P(C \\cap B)}=\r\n{ P(A \\cap B|C)P(C)  \\over P(B|C)P(C)  }= {P(A|C)P(B|C) \\over P(B|C)}= P(A|C).$$\r\n (ii) $\\Longrightarrow$ (iii). Write\r\n$$P(B|A\\cap C)={ P(A \\cap C \\cap B) \\over P(A \\cap C)}=\r\n{ P(A | B \\cap C)P(B \\cap C)  \\over P(A|C)P(C)  }= {P(A|C)P(B|C)P(C) \\over P(A|C)P(C)}= P(B|C).$$\r\n(iii) $\\Longrightarrow$ (i). Write\r\n$$P(A\\cap B | C)={ P(A \\cap B \\cap C) \\over P( C)}=\r\n{ P(B| A\\cap C)P(A\\cap C)  \\over  P(C)  }= {P(B|C)P(A|C) P(C) \\over P( C)}= P(A|C)P(B|C).$$\r\n\r\n\r\nRemark. In (i), $A$ and $B$ are exchangeable. Therefore, if (i) is equivalent to (ii), it must\r\nbe equivalent to (iii).',3,0,2,0,0,0,0,'','','Alas! Please check the definition of conditional independence.','Congratulations! You have mastered the definition of conditional independence.',1,NULL,2),(49,'3.1.4',4,'Suppose $\\{X_n: n= \\cdots -2, -1, 0, 1, 2, \\cdots\\}$\r\nis a discrete time $MC$ with the time being  all integers from\r\n$-\\infty$ to $\\infty$. Let $Y_n = X_{-n}$ for all integers $n$.\r\nIs $\\{Y_n: n= \\cdots -2, -1, 0, 1, 2, \\cdots\\}$ a $MC$?','','','','','','','Yes','No','','','','','','','','','','','A','','','','Yes. Write\r\n\\begin{eqnarray*}\r\n && P(\\{Y_{n+1}\\in A_1, Y_{n+2}\\in A_2, ... \\} \\cap \\{ Y_{n-1} \\in B_1 , Y_{n-2} \\in B_2 ,...\\}\r\n|Y_n=j)\r\n\\\\\r\n&=& P(\\{X_{-n-1}\\in A_1, X_{-n-2}\\in A_2, ... \\} \\cap \\{ X_{-n+1} \\in B_1 , X_{-n+2} \\in B_2 ,...\\}\r\n|X_{-n}=j)\r\n\\\\\r\n&=& P(\\{X_{-n-1}\\in A_1, X_{-n-2}\\in A_2, ... \\} |  X_{-n}=j)\r\nP(\\{ X_{-n+1} \\in B_1 , X_{-n+2} \\in B_2 ,...\\}\r\n|X_{-n}=j)\r\n \\\\\r\n&=& P(\\{Y_{ n+1}\\in A_1, Y_{ n+2}\\in A_2, ... \\} |  Y_{n}=j)\r\nP(\\{ Y_{n-1} \\in B_1 , Y_{n-2} \\in B_2 ,...\\}\r\n|Y_{n}=j)\r\n\\end{eqnarray*}\r\nProving the conditional independence of the past and future given a fixed state of the present.',3,0,2,0,0,0,0,'','','Alas! Please check the definition of markov process.','Congratulations! You have mastered the definition of markov process.',1,NULL,2),(50,'3.4.2',4,'In Example 3.7,   a\r\n$MC$ based on patterns of length 2 with 4 states is constructed\r\nto solve the problem. If Mr Y picks\r\nthe pattern HHH, what is the chance of Mr Z to win\r\n by picking THH? You may construct a $MC$ based on patterns of\r\nlength 3 (rather than 2) with 8 states to\r\ncalculate.','','','','','','','5/8','6/8','7/8','1','','','','','','','','','C','','','','Let $\\{X_n: n\\geq 3\\}$ be the MC with state space being\r\n the eight patterns of length 3:\r\n $$\\hbox{\\{ HHH (0), THH (1),  HTH (2), TTH (3),\r\n HHT (4) , THT (5), HTT (6), TTT (7) \\}.}$$\r\n and transition probability matrix:\r\n \\begin{eqnarray*}\r\n&& \\qquad  \\qquad \\qquad  0\\quad \\, 1 \\quad \\, 2 \\quad\\,\r\n 3\\quad \\,\\,  4 \\quad \\,\\, 5 \\quad \\, 6 \\quad \\,\\, 7 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{HHH (0)}  \\cr\r\n\\hbox{THH (1)} \\cr  \\hbox{HTH (2)} \\cr \\hbox{TTH (3)}\\cr\r\n\\hbox{HHT (4)} \\cr  \\hbox{THT (5)} \\cr\r\n\\hbox{HTT (6)}\\cr \\hbox{TTT (7)} }\r\n\\pmatrix{\r\n.5   & 0   & 0 & 0 & .5 & 0 & 0 & 0   \\cr\r\n.5 & 0 & 0 & 0 & .5 & 0 & 0 &0  \\cr\r\n0 & .5 & 0 & 0 & 0 & .5 & 0 & 0   \\cr\r\n0 & .5 & 0 & 0 & 0 & .5 & 0 & 0  \\cr\r\n0 & 0 & .5&  0 & 0 & 0 & .5 & 0   \\cr\r\n0 & 0 & .5&  0 & 0 & 0 & .5 & 0   \\cr\r\n0 & 0 & 0 & .5 & 0 & 0 &0 & .5   \\cr\r\n0 & 0 & 0 & .5 & 0 & 0 &0 & .5\r\n}\r\n\\end{eqnarray*}\r\n\r\nLet $p_i$ be the chance that THH occur before HHH, beginning with state\r\n$i$, ($X_3=i$).\r\nThen,\r\n$$p_0=0, \\quad p_1=1, \\quad p_i =  \\sum_{j=0}^7 P_{ik}p_k, \\quad \\hbox{for $i\\not=0, 1$}.$$\r\nSolving the equations, we have\r\n$$p_2= p_3= p_4= p_5 =  p_6=  p_7=1.$$\r\nSince $P(X_3=i)=1/8$ for $i=0,...,7$, the chance that Mr Z wins is\r\n$$1/8\\times (p_0+p_1 + \\cdots+p_7)= 7/8.$$\r\n\r\n\r\nRemark. Directly solving the above 6 equations appears to be too much of\r\ncomputation. But a quick look shall find that $p_2=p_3$, $p_4=p_5$\r\nand $p_6=p_7$. Thus the number of equations is actually only three.\r\n(More astute observation further leads to $p_4=p_5=p_6=p_7$, which reduces the\r\nnumber of equations to two.)',3,2,3,2,0,0,0,'','','Reread Example 3.7 might be helpful!','Congratulations! You have solved a difficult problem of first step analysis!',1,NULL,4),(51,'3.4.4',4,'A rabbit has three dens A, B and C.  If it stays in A for a night,\r\nthe following night it\'s always in B. If it stays in B or C for a night,\r\nthe following day,  it takes equal chance 1/3 to\r\neither continue to stay or go to one of the other two dens for the night.\r\nA wolf, trying to hunt down the rabbit, has a different pattern.\r\nEvery day, it takes chance .8 to go clockwise to the next den\r\nand  chance .2 to go anti-clockwise the other den for the night-stay.\r\nThe rabbit would be eaten by the wolf the night they are in the same den.\r\nSuppose at the night of day 0, the rabbit is in Den A and the wolf\r\nin Den B.   What is the mean life time in days of the rabbit?','','','','','','','3.78','2.54','3.96','4.21','','','','','','','','','A','','','','Let $X_n $ represent the den the rabbit stays for $n$-th night\r\nfollowed by the den the wolf stays for the $n$-th night, assuming the wolf and rabbit\r\nalways stay in the den forever once they are in the same den.\r\nThen, $\\{X_n: n\\geq 0\\}$ is a {\\it MC} with nine states\r\n$$ \\{ \\hbox{ AA (0) , AB (1), AC (2), BA (3), BB (4), BC (5), CA (6), CB (7), CC (8)}, \\}$$\r\nand\r\nwith transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad\\quad AA \\quad \\,\\, AB\\quad \\,\\, AC\\quad \\,\\, BA\\quad \\,\\, BB\\quad \\,\\, BC\\quad \\,\r\n CA\\quad \\, CB\\quad \\,\\, CC \\\\\r\n{\\bf P} &=& \\matrix{AA \\cr AB \\cr AC \\cr BA \\cr BB \\cr BC\\cr CA \\cr CB \\cr CC}\r\n\\pmatrix{\r\n1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\cr\r\n0 & 0 & 0 & .8 & 0& .2 & 0 &0 & 0 \\cr\r\n0 & 0 & 0 & .2 & .8 & 0& 0 & 0 & 0 \\cr\r\n0 & .067 & .266 & 0 & .067 & .266 &0 & .067 & .266 \\cr\r\n0 & 0 & 0&  0 & 1 & 0 & 0 & 0 &0 \\cr\r\n.067&.266 & 0 & .067&.266 & 0  & .067&.266 & 0 \\cr\r\n0 & .067 & .266 & 0 & .067 & .266 &0 & .067 & .266 \\cr\r\n.266&0 &.067 & .266&0 & .067   & .266&0 & .067  \\cr\r\n0 & 0 & 0&  0 & 0 & 0 & 0 & 0 &1\r\n}\r\n\\end{eqnarray*}\r\nLet $w_i $ be the mean number of steps to reach AA, BB or CC, starting from\r\nstate $i$.\r\nThen\r\n$$w_0=w_4=w_8=0, \\qquad w_i = 1 + \\sum_{k=0}^8 P_{ik}w_i \\quad \\hbox{for $i\\not=0, 4, 8.$}$$\r\nSolve the equations,\r\n$$w_1= 3.78, \\,\\,\r\nw_2= 1.53, \\,\\,\r\nw_3= 2.69, \\,\\,\r\nw_5= 3.14, \\,\\,\r\nw_6= 2.69, \\,\\,\r\nw_7= 2.89 $$\r\nThe mean life time of rabbit is $w_1=3.78$ days ($X_0=1=$AB).',2,2,2,2,0,0,0,'','','You may have a look at example 3.7?','Congratulations! You mastered first step analysis!',1,NULL,3),(52,'3.9.1',4,'Is it possible that $E(X_n) \\uparrow \\infty$ geometrically\r\nfast, and the chance of extinction is still positive?','','','','','','','Yes, it is possible.','No, it is not possible.','','','','','','','','','','','A','','','','$E(X_n) = \\mu^n$. Choose $\\xi $ such that\r\n$P(\\xi=2)=3/4$ and $P(\\xi=0)=1/4$, so that $\\mu = 3/2 > 1$ and\r\n$E(X_n)   \\uparrow \\infty$ geometrically. But, on the other hand,\r\n$\\phi(s) = 1/4 + 3/4s^2$ and $s=\\phi(s)$ has a solution $1/3$.\r\nTherefore $\\mu_\\infty = 1/3$ which is positive.',3,0,2,0,0,0,0,'','','Oops! You may check the definition of extinction probability.','Congratulations! You mastered it!',1,NULL,4),(53,'3.9.3',4,'Suppose $P(\\xi\\geq 4)=0$, and  $P(\\xi=i)=p_i$ for\r\n$i=0,..., 3$. If we express $u_\\infty$ in terms of $p_0,...,p_3$, how many situations are there in total?','','','','','','','1','3','4','5','','','','','','','','','D','','','','Consider equation $s=\\phi(s)$, which can be written as\r\n$$ p_3s^3+p_2s^2 + (p_1-1)s +p_0=0.$$\r\nAs $1$ is always a solution. The equation is\r\n$$   (s-1)(p_3 s^2 +(p_2+p_3)s  -p_0)=0$$\r\nSolving the equations, we have\r\n$$u_\\infty = \\cases { 0 &  if $p_0=0$;\r\n\\cr\r\n1 & if $p_0>0, p_2=p_3=0$;\r\n\\cr\r\n \\min(1, p_0/p_2) & if $p_0>0, p_2>0, p_3=0$;\r\n \\cr\r\n(1/2p_3) \\{ -p_2-p_3 + \\sqrt{ (p_2+p_3)^2 + 4p_0p_3} \\}&\r\nif $p_0>0, p_3>0 $ and $p_0 \\leq 2p_3+p_2$ \\cr\r\n1 & if $p_0>0, p_3>0 $ and $p_0 > 2p_3+p_2$.}\r\n$$',3,0,2,0,0,0,0,'','','You may check the definition of extinction problem.','Great! You have considered all possibilities!',1,NULL,3),(54,'3.4.5',4,'{Mickey in Maze with an exit.}\r\nSuppose now there is an exit in cell 2 (See the graph below). Assume Mickey moves around in the same way\r\nas before, except that, once in cell 2, he has chance 0.5 to get out and never return\r\nand chance equal chance 1/4 to go to cell 1 or 5.\r\nSuppose Mickey begins in cell 6, what\'s the mean number of visits of cell 1?','theall/image/DIY-3.4.5-1.PNG','','','','','','6.','5.','4.','3.','None of the above is correct.','','','','','','','','D','','','','Let $s_i$ be the mean number of visits of cell 1 beginning from\r\ncell $i$, $including$ the beginning state if it is 1.\r\nThen,\r\n\\begin{eqnarray*}\r\n&&s_0 = 1/2(s_1+s_3) \\qquad  \\qquad\r\ns_1= 1+ 1/3(s_0+s_2+s_4) \\qquad \\quad\r\ns_2= 1/4(s_1+s_5) \\\\\r\n&&s_3=1/3(s_0+s_4+s_6) \\qquad s_4=1/4(s_1+s_3+s_5+s_7) \\qquad  \\,\\,  s_5=1/3(s_2+s_4+s_8) \\\\\r\n&&s_6=1/2(s_3+s_7) \\qquad \\qquad s_7=1/3(s_4+s_6+s_8 ) \\qquad \\qquad \\,\\, \\,  s_8=1/2(s_5+s_7)\r\n\\end{eqnarray*}\r\nSolve this equation, we have\r\n$$s_0=3.375,\\,\\, s_1= 3.625,\\,\\, s_2=1.5,\\,\\, s_3=3.125,\\,\\, s_4=3,\\,\\, s_5=2.375,\\,\\,s_6=3,\\,\\,\r\n s_7=2.876,\\,\\, s_8=2.625.$$\r\n The mean number of visits of state 1 beginning from state 6 is 3.\r\n<br><br>\r\nRemark. \r\n<br>\r\nThe above exercise is a little tedious in solving the nine linear equations.\r\nIn the exam, the number equations would be much smaller. For this problem, no symmetry\r\ncan be used to shorten number of equations. If, instead, the problem is to\r\n calculate the mean number of times the MC visits state 4, beginning from\r\nstate 6, symmetry can be used to eliminate three of the nine equations.',3,3,2,2,0,0,0,'','Let $a_i$ be the mean number of visits of cell 1 beginning from\r\ncell $i$, $excluding$ the beginning state if it is 1.\r\n Then,\r\n\\begin{eqnarray*}\r\n&&a_0 = 1/2(a_1+1 +a_3) \\qquad\r\n\\,\\, a_1=   1/3(a_0+a_2+a_4) \\qquad \\qquad \\qquad\r\na_2= 1/4(a_1+1 +a_5) \\\\\r\n&&a_3=1/3(a_0+a_4+a_6) \\qquad a_4=1/4(a_1+1+a_3+a_5+a_7) \\qquad a_5=1/3(a_2+a_4+a_8) \\\\\r\n&&a_6=1/2(a_3+a_7) \\qquad\\qquad  a_7=1/3(a_4+a_6+a_8 ) \\qquad\\qquad \\qquad a_8=1/2(a_5+a_7)\r\n\\end{eqnarray*}\r\nThe equations are the same as those of Method 1 with $a_1+1=s_1$ and $a_i=s_i$ for $i\\not=1$.\r\nWe get the same solutions, (except for $a_1=s_1-1=2.625$).','Oops, try again. Get yourself familiar with the application of first step analysis!','Well done! You have mastered the concept of first step analysis!',1,NULL,4),(55,'3.9.2',4,'How is $u_\\infty$ related to $s=\\phi(s)$?','','','','','','','The larger solution of $s=\\phi(s)$ on $[0, \\infty]$ is $u_\\infty$.','The smaller solution of $s=\\phi(s)$ on $[0, \\infty]$ is $u_\\infty$.','There is no certain answers.','','','','','','','','','','B','','','','Let $a$ be a solution of\r\n$s=\\phi(s)$ on\r\n$[0, 1]$. Since $1$ is always a solution, $a$ is well defined.\r\nObserve that $u_1=p_0=\\phi(0)$. Since $\\phi(\\cdot)$ is\r\nnondecreasing, we have\r\n$a =\\phi(a) \\geq \\phi(0) = u_1$. For any $n \\geq 1$, assume now\r\n$a \\geq u_n$. Then\r\n$a =\\phi(a) \\geq \\phi(u_n) = u_{n+1}$.\r\nBy induction, it follows that\r\n$a \\geq u_n$ for all $n \\geq 1$.\r\nTaking limit, we know\r\n$a \\geq u_\\infty$.\r\nConsequently, $u_\\infty$ must be the smallest solution\r\non $[0, 1]$ of\r\nthe equation $s=\\phi(s)$.',3,1,3,3,0,0,0,'','','Oops, try again. Get yourself familiar with probability generating function!','Well done! You have mastered the concepts of probability generating function and branching process!',1,NULL,5),(56,'3.1.1',3,'A simplified model for the spread of a disease goes this way: The\r\ntotal population size is $N=5$, of which some are diseased and the remainder\r\nare healthy. During any single period of time, two people are selected\r\nat random from the population and assumed to interact. The selection\r\nis such that an encounter between any pair of individuals in the\r\npopulation is just as likely as between any other pair. If one of these persons\r\nis diseased and the other not, then with probability $\\alpha=0.1$ the disease\r\nis transmitted to the healthy person. Otherwise, no disease transmission\r\ntakes place. Let $X_n$, denote the number of diseased persons in the\r\npopulation at the end of the $n$th period. Specify the transition probability\r\nmatrix.','','','','','','','\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&3&4&5\\\\\r\n0&1&0&0&0&0&0\\\\\r\n1&0&0.96&0.04&0&0&0\\\\\r\n2&0&0&0.94&0.06&0&0\\\\\r\n3&0&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0&0.96&0.04\\\\\r\n5&0&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]','\\[\r\n\\begin{array}{c||ccccc||}\r\n&1&2&3&4&5\\\\\r\n1&0.96&0.04&0&0&0\\\\\r\n2&0&0.94&0.06&0&0\\\\\r\n3&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0.96&0.04\\\\\r\n5&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]','\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&3&4&5\\\\\r\n0&0&0&0&0&0&0\\\\\r\n1&0&0.96&0.04&0&0&0\\\\\r\n2&0&0&0.94&0.06&0&0\\\\\r\n3&0&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0&0.96&0.04\\\\\r\n5&0&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]','\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&3&4&5\\\\\r\n0&0&0&0&0&0&0\\\\\r\n1&0&0.96&0.04&0&0&0\\\\\r\n2&0&0&0.94&0.06&0&0\\\\\r\n3&0&0&0&0.94&0.06&0\\\\\r\n4&0&0&0&0&0.96&0.04\\\\\r\n5&0&0&0&0&0&0.9\\\\\r\n\\end{array}\r\n\\]','','','','','','','','','A','','','','State i: Number of people infected<br>\r\nType of Interaction that more people get infected($i\\to i+1$): One infected and one uninfected<br>\r\nProbability of such kind of interaction:\r\n$\\binom{i}{1}\\binom{N-i}{1}/\\binom{N}{2}$<br>\r\nTherefore,\r\n\\begin{align*}\r\nP_{i(i+1)}=\\alpha\\binom{i}{1}\\binom{N-i}{1}/\\binom{N}{2}\\\\\r\nP_{ii}=1-\\alpha\\binom{i}{1}\\binom{N-i}{1}/\\binom{N}{2}\r\n\\end{align*}\r\nNote that 0 and 5 are two absorbing state, their diagonal entries is 1.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,4,2),(57,'3.1.4',3,'The random variables $\\xi_1,\\xi_2,...$ are independent and with the common probability mass function\r\n\\[\\begin{array}{ccccc}\\hline\r\nk=&0&1&2&3\\\\\r\n\\text{Pr}\\{\\xi=k\\}=&0.1&0.3&0.2&0.4\\\\\\hline\r\n\\end{array}\r\n\\] Set $X_0=0$, and let $X_n$=max\\{$\\xi_1,...,\\xi_n$\\} be the largest $\\xi$ observed to date. Determine the transition probability matrix for the Markov chain \\{$X_n$\\}','','','','','','','\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&0.1&0.3&0.2&0.4\\\\\r\n1&0.1&0.3&0.2&0.4\\\\\r\n2&0.1&0.3&0.2&0.4\\\\\r\n3&0.1&0.3&0.2&0.4\r\n\\end{array}\r\n\\]','\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&0.1&0.3&0.2&0.4\\\\\r\n1&0&0.3&0.2&0.4\\\\\r\n2&0&0&0.2&0.4\\\\\r\n3&0&0&0&0.4\r\n\\end{array}\r\n\\]','\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&0.1&0.3&0.2&0.4\\\\\r\n1&0&0.4&0.2&0.4\\\\\r\n2&0&0&0.6&0.4\\\\\r\n3&0&0&0&1\r\n\\end{array}\r\n\\]','','','','','','','','','','C','','','','$P_{ij}=0$ if $j\\neq i$ because we only consider the maximum.\r\nMaximum will remain unchanged when you roll a smaller value: those probabilities are added to $P_{ii}$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(58,'3.1.2',3,'Consider the problem of sending a binary message, 0 or 1, through a signal channel consisting of several stages, where transmission through each stage is subject to a fixed probability of error ?. Suppose that $ X_0 = 0 $ is the signal that is sent and let $ X_n $ be the signal that is received at the nth stage. Assume that$ \\{X_n\\} $ is a Markov chain with transition probabilities $ P_{00} = P_{11} = 1 - \\alpha $ and $ P_{01} = P_{10} = \\alpha $, where $ 0 < \\alpha < 1 $. <br>\r\n (a) Determine $ \\Pr\\{X_0 =0,X_1 =0,X_2 =0\\}$,the probability that no error occurs up to stage $ n = 2$.<br>\r\n (b) Determine the probability that a correct signal is received at stage 2. (Hint: This is $ \\Pr\\{X_0 = 0,X_1 = 0,X_2 = 0\\} + \\Pr\\{ X_0 = 0 ,X_1 = 1,X_2 =0\\}$.)','','','','','','','A. (a) $ 1 - \\alpha \\ $ ;  (b) $  1 - \\alpha $','B. (a) $ 1 - \\alpha \\ $ ;  (b) $ (1 - \\alpha)^2   $','C. (a) $ (1 - \\alpha)^2 \\ $ ; (b) $ (1 - \\alpha)^2 $','D. (a) $ (1 - \\alpha)^2 \\ $ ;  (b) $ 1 - (1 - \\alpha)^2 $','E. (a) $ (1 - \\alpha)^2 \\ $ ; (b) $  1 - 2\\alpha(1 - \\alpha)   $','','','','','','','','E','','','','(a)\r\n\\begin{align*}\r\n\\Pr\\{X_0 = 0,X_1 = 0,X_2 = 0\\} = P_{00} \\cdot P_{00} = \\left( 1 - \\alpha \\right)^2\r\n\\end{align*}\r\n(b) \r\n\\begin{align*}\r\n \\Pr\\{X_0 = 0,X_1 = 0,X_2 = 0\\} + \\Pr\\{ X_0 = 0 ,X_1 = 1,X_2 =0\\}\r\n&= P_{00} \\cdot P_{00} + P_{01} \\cdot P_{10}\\\\\r\n&= \\left( 1 - \\alpha \\right)^2 + \\alpha^2 = 1 - 2\\alpha + 2\\alpha^2 \\\\\r\n&= 1 - 2\\alpha(1-\\alpha)\r\n\\end{align*}',1,2,1,1,0,0,0,'','','Oops, your answer is not correct. You may not fully understand the concept of Markov chain and transition probability. You could try other similar problems.','Good job! You are quite familiar with the concept of Markov chain and transition probability.',NULL,0.2,2),(59,'3.3.11',1,'(The Confucius descendants) \r\nHypothetically, every male in the Kong family, since Confucius, produces his sons,\r\nindependent of everything else,\r\naccording to the same distribution\r\n<br>\r\n\r\nNumber of sons:  $0   ,    1   ,    2    ,   3    ,    4    ,    5    ,  6 or more $<br>\r\nProbability :      $ 0.17 , 0.50, 0.25 , 0.05 , 0.02 , 0 .01      ,  0 $\r\n\r\n<br>\r\n Then the mean number of sons of any Kong male is about 1.2.','','','','','','','','','','','','','','','','','','','A','','','','Let $X_n$ be the number of the $n$-th generation of descendants of\r\n Confucius. (According to the old Chinese tradition, only males carrying\r\n the last name are counted as family descendants.)\r\n Then $\\{X_n: n \\geq 1\\}$ is a so-called branching process.\r\n Here $X_0=1$ meaning that the $0$-th generation is Confucius himself, alone.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Congratulations! You have mastered the concept and fundamental characteristics of Branching Process.','Alas! Please return to the graph and review related knowledge.',1,NULL,1),(60,'3.2.7',1,'({\\sc Penney-ante}) Repeatedly tossing a fair coin.\r\nThere are 8 patterns of length 3:\r\n<br>\r\n$$HHH,  HHT, HTH, HTT, THH, THT, TTH, TTT.$$\r\n<br>\r\nTwo men, called  Yugong (Y) and Zhishou (Z),  are betting on whose choice\r\nof pattern would occur first. Mr Z  ``generously\"  allows\r\nMr Y to pick his favorite pattern first and, afterwards, he picks his own.\r\nWhatever Mr Y picks,  Mr Z, being ``Zhishou\", somehow\r\nalways beats him by a  chance at least 2/3! (This time, even infinite offspring\r\nto continue the game can\'t help Yugong.)','','','','','','','','','','','','','','','','','','','A','','','','Let $X_n, n \\geq 2$ denote the pattern of length 2 of tosses\r\n$n-1$ and $n$.\r\nThen $\\{X_n: n \\geq 2\\}$ is a $MC$  with state space $\\{$HH , HT, TH, TT$\\}$,\r\nwhich is accordingly denoted as $\\{0, 1, 2, 3\\}$, and transition matrix\r\n$$ \\qquad  \\,\\, 0 \\qquad 1 \\qquad 2\\qquad 3  $$\r\n$${\\bf P} =\\matrix{0 \\cr 1 \\cr 2 \\cr 3    }\r\n  \\pmatrix{\r\n  1/2 &  1/2  &     &              \\cr\r\n       &     &  1/2  &  1/2            \\cr\r\n  1/2 &  1/2   &     &             \\cr\r\n       &      &  1/2   &   1/2\r\n}\r\n$$\r\nSuppose Mr Y\'s pick is HTH. Mr Z can pick HHT  to beat Mr Y\r\nby a chance $2/3$. The following is the proof using first-step analysis.\r\n\r\nLet $p_i $ be the probability that pattern HTH occurs before HHT given\r\n$X_2=i$, $i=0, 1, 2, 3$.\r\nThen\r\n\\begin{eqnarray*}\r\np_0 &=& 1/2p_0 + 1/2 \\times 0 \\\\\r\np_1 &=& 1/2 \\times 1 + 1/2 p_3 \\\\\r\np_2 &=& 1/2 p_0 + 1/2p_1 \\\\\r\np_3 &=& 1/2 p_2 + 1/2 p_3\r\n\\end{eqnarray*}\r\nSolving the equations, we have\r\n$$p_0 = 0 \\quad p_1 = 2/3 \\quad p_2=p_3=1/3$$\r\nThen, the chance that Mr Y wins is\r\n$$ (p_0 + p_1 +p_2+p_3)/4 = 1/3,$$\r\nand Mr Z wins with chance $2/3$.\r\n\r\nGeneral results are:\r\n\r\nMr Y\'s preemptive Choice: $$ HHH , HHT , HTH , HTT , THH , THT , TTH , TTT $$\r\nMr Z\'s responsive Choice: $$ THH , THH , HHT , HHT ,  TTH , TTH , HTT , HTT $$\r\n $P$(Mr Z  wins):$$ 7/8\\quad  3/4\\quad  2/3\\quad 2/3\\quad 2/3 \\quad 2/3\\quad 3/4\\quad 7/8  $$\r\n%Mr Y\'s choice:\r\n % H & HHH &\r\n%Mr Z\'s choice:\r\n  % & THH &\r\n%Chance of Mr Z winning:\r\n  % & 7/8\r\n\\end{tabular}\r\n\\end{center}\r\n\\hfill $\\square$\r\n\r\nThe above problem may sound counter-intuitive in that Mr Y, with the right of\r\nchoosing first, is always in disadvantage. In other words, preemptive strike always\r\nloses to the counter-strike,   because there does NOT exist\r\nan optimal pattern superior to the rest seven patterns.\r\n It\'s essentially\r\nabout four random variables, numbers of tosses till the patterns of length 3, forming  a loop\r\nof one dominating another as follows:\r\n\r\nSuch a seemingly strange/bizarre  phenomenon is not uncommon. For a simpler occasion,\r\ntry to construct\r\nthree r.v.s, say, $X$, $Y$ and $Z$, such that\r\n$P(X>Y ) = P(Y> Z)= P(Z> X)=2/3$; which may appear equally bizarre at first\r\nglance. (Please DIY.\r\nHint:  ``TIAN GI SHAI MA\"---the horse racing strategy of the legendary Sun Bin suggested  to\r\nGeneral Tian in the kingdom\r\nof Qi.)\r\n\r\nSome more hints about an insightful explanation of Mr Z\'s strategy: HHH defeats THH only\r\nwhen the first three tosses turn  out HHH (why?). HHT defeats THH only when the first two tosses\r\nturn out HH (why?). For HTH vs HHT, the competition begins sometime with an H.\r\nIf the following one is H, Mr\r\nZ wins ($1/2$ chance already). If the following two is TH ($1/4$ chance) Mr Y wins,\r\nIf the following two is TT $(1/4$ chance), back to origin  waiting for the next H to\r\noccur. DIY: based on this reasoning, come up with one equation about the chance of Mr Z\'s HHT\r\ndefeating Mr Y\'s HTH. Work out the similar for the case of HTT vs HHT.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Alas! Please return to the graph and review related knowledge.','Congratulations! You have mastered the First Step Analysis method.',1,NULL,3),(61,'3.2.3',3,'Let $X_n$ denote the quality of $n$th item produced by a production system with $X_n=0$ meaning \"good\" and $X_n=1$ meaning \"defective.\" Suppose that $X_n$ evolve as a Markov chain whose transition probability matrix is \r\n\\[P=\r\n\\begin{array}{c||cc||}\r\n&0&1\\\\\r\n0&0.99&0.01\\\\\r\n1&0.12&0.88\r\n\\end{array}\r\n\\]\r\nWhat is the probability that the fourth item is defective given that the first item is defective','','','','','','','0.973731','0.026269','0.315228','0.684772','0.605752','0.88','','','','','','','D','','','','\\[P^{(4-1)}=P^{(3)}=\r\n\\begin{array}{c||cc||}\r\n&0&1\\\\\r\n0&0.973731&0.026269\\\\\r\n1&0.315228&0.684772\r\n\\end{array}\\]\r\nSince we know the first one is defective: original state is 1,\r\nwe want to calculate the probability the probability of defective (1), so we calculate\r\n$P_{11}^{(3)}=0.684772$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,5,2),(62,'3.3.1',3,'An urn contains six tags, of which three are red and three green.\r\nTwo tags are selected from the urn. If one tag is red and the other is green,\r\nthen the selected tags are discarded and two blue tags are returned to the\r\nurn. Otherwise, the selected tags are returned to the urn. This process repeats\r\nuntil the urn contains only blue tags. Let $X_n$, denote the number of red\r\ntags in the urn after the $n$th draw, with $X_0=3$. (This is an elementary\r\nmodel of a chemical reaction in which red and green atoms combine to\r\nform a blue molecule.) Give the transition probability matrix.','','','','','','','\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&1/15&14/15&0&0\\\\\r\n2&0&4/15&11/15&0\\\\\r\n3&0&0&3/5&2/5\\\\\r\n\\end{array}\\]','\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&14/15&1/15&0&0\\\\\r\n2&0&11/15&4/15&0\\\\\r\n3&0&0&2/5&3/5\\\\\r\n\\end{array}\\]','\\[\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&0&14/15&1/15&0\\\\\r\n2&0&0&11/15&4/15\\\\\r\n3&0&0&0&1\\\\\r\n\\end{array}\\]','','','','','','','','','','A','','','','Consider the possible state in the urn:<br>\r\n3 red, 3 green, 0 blue<br>\r\n2 red, 2 green, 2 blue<br>\r\n1 red, 1 green, 4 blue<br>\r\n0 red, 0 green, 6 blue<br>\r\n\r\nNow we know the transition probability \r\n\\[P_{i(i-1)}=X^2/\\binom{6}{2}\\]\r\n\\[P_{ii}=1-X^2/\\binom{6}{2}\\]\r\n\r\nSo we have the transition probability matrix is\r\n\\[P=\r\n\\begin{array}{c||cccc||}\r\n&0&1&2&3\\\\\r\n0&1&0&0&0\\\\\r\n1&1/15&14/15&0&0\\\\\r\n2&0&4/15&11/15&0\\\\\r\n3&0&0&3/5&2/5\\\\\r\n\\end{array}\\]',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(63,'3.2.1',3,'Consider the Markov chain whose transition probability matrix is given by \r\n\\[ \r\n\\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.1cm} & \\ \\ \\ 0 \\ & \\  \\ 1 \\ \\  & \\hspace{0.05cm} \\ 2 \\ \\  & \\hspace{0.05cm}  3 \\  \\ \\ \\ \\ \\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  0.4 & 0.3 & 0.2 & 0.1 \\\\ \r\n  0.1 & 0.4 & 0.3 & 0.2 \\\\ \r\n  0.3 & 0.2 & 0.1 & 0.4\\\\	\r\n  0.2 & 0.1 & 0.4 & 0.3 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nSuppose that the initial distribution is $ p_i = \\frac{1}{4} $ for $ i=0,1,2,3 $. Given that $ \\Pr\\{X_n = k\\} = \\frac{1}{4},k=0,1,2,3 $ for all $ n $, where $k$ is the state of the Markov chain. For $n \\in \\mathbb{N}$ (the set of all natural numbers), deduce a general result from this example.','','','','','','','A. $ \\Pr\\{X_n = k\\} = \\frac{1}{N + 1} $','B. $ \\Pr\\{X_n = k\\} = \\frac{1}{N} $','C. $\\Pr\\{X_n = k\\} = \\frac{N}{N + 1} $','D. $\\Pr\\{X_n = k\\} = \\frac{1}{k}$','E. No general result can be deduced. The result depends on the transition probabilities.','','','','','','','','A','','','','\\[p_i = \\Pr\\{X_0 = i\\} =  \\frac{1}{4} \\ \\text{for} \\ i = 0, 1, 2, 3\\]\r\nFor $k = 0,1,2,3$ :\r\n\\begin{align*}\r\n\\Pr\\{X_1 = k\\} \r\n&= \\Pr\\{X_1 = k | X_0 = 0\\} \\cdot \\Pr\\{X_0 = 0\\} + \\cdots + \\Pr\\{X_1 = k | X_0 = 3\\} \\cdot \\Pr\\{X_0 = 3\\}\\\\\r\n& \\ \\ \\  (\\text{Slicing the universe according to } X_0 ) \\\\\r\n&= \\sum_{j=0}^{3} \\Pr\\{X_1 = k | X_0 = j\\} \\cdot \\Pr\\{X_0 = j\\} \\\\\r\n&= \\sum_{j=0}^{3} \\textbf{P}_{jk} \\cdot p_j \\ (\\text{by the definition of transition probability})\\\\\r\n&= \\frac{1}{4} \\cdot \\sum_{j=0}^{3} \\textbf{P}_{jk} \\\\\r\n&= \\frac{1}{4} \\cdot 1 \\ (\\text{by the property of transition probability}) \\\\\r\n&= \\frac{1}{4} = \\Pr\\{X_0 = k\\} \r\n\\end{align*}\r\nBy induction, $\\Pr\\{X_n = k\\} = \\Pr\\{X_{n-1} = k\\} = \\cdots = \\Pr\\{X_1 = k\\} = \\Pr\\{X_0 = k\\} = \\frac{1}{4} \\ \\forall n \\in \\mathbb{N}$ ($\\mathbb{N}$ is the set of all natural numbers). <br>\r\nTherefore, for any $N \\in \\mathbb{N}$, \\[\\text{if} \\ \\Pr\\{X_0 = k\\} = \\frac{1}{N + 1} \\ \\text{for} \\  k = 0, 1, ..., N\\] where $k$ is the state of a Markov chain, then \\[\\Pr\\{X_n = k\\} = \\frac{1}{N + 1} \\ \\forall n \\in \\mathbb{N}\\]',3,1,3,1,0,0,0,'','','','',NULL,0.2,2),(64,'3.3.7',3,'A component in a system is placed into service, where it operates until its failure, whereupon it is replaced \\emph{at the end of the period} with a new component having statistically identical properties, and the process repeats. The probability that a component lasts for $k$ periods is $\\alpha_k$, for $k=1,2,...$ Let$X_n$ be the remaining life of the component in service \\emph{at the end of period n}. Then $X_n=0$ means that $X_{n+1}$ will be the total operating life of the next component. Give the transition probabilities for the Markov chain \\{$X_n$\\}.','','','','','','','\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&1&0&0&\\dots&0&\\dots\\\\\r\n1&1&0&0&\\dots&0&\\dots\\\\\r\n2&0&1&0&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]','\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&0&\\alpha_1&\\alpha_2&\\dots&\\alpha_k&\\dots\\\\\r\n1&1&0&0&\\dots&0&\\dots\\\\\r\n2&0&1&0&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]','\\[\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&0&\\alpha_1&\\alpha_2&\\dots&\\alpha_k&\\dots\\\\\r\n1&1-\\alpha_1&\\alpha_1&0&\\dots&0&\\dots\\\\\r\n2&0&1-\\alpha_2&\\alpha_2&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]','','','','','','','','','','B','','','','Note that the state always decrease by 1 ($i\\to i-1$) as times go by. Once it reaches 0, it is replaced ($0\\to k$).\r\nSo \r\n\\[P=\r\n\\begin{array}{c||cccccc||}\r\n&0&1&2&\\dots&k&\\dots\\\\\r\n0&0&\\alpha_1&\\alpha_2&\\dots&\\alpha_k&\\dots\\\\\r\n1&1&0&0&\\dots&0&\\dots\\\\\r\n2&0&1&0&\\dots&0&\\dots\\\\\r\n\\vdots&\\vdots&\\vdots&\\vdots&\\ddots&\\vdots&\\vdots\\\\\r\n\\end{array}\r\n\\]',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(66,'3.2.4',3,'Problem: \r\n Suppose $ X_n $ is a two-state Markov chain whose transition probability matrix is \r\n  \\[ \r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lll}\r\n \\hspace{0.1cm} & \\ \\ \\ \\ \\ 0 \\ & \\  \\  \\  \\  \\  \\  1 \\ \\ \r\n \\end{array} \\\\\r\n \\begin{array}{c}\r\n 0\\\\\r\n 1\\\\\r\n \\end{array}\r\n \\begin{Vmatrix}\r\n \\alpha & 1 - \\alpha   \\\\ \r\n 1 - \\beta & \\beta   \\\\ \r\n \\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nThen, $ Z_n =(X_{n-1},X_n) $ is a Markov chain having the four states $\\{0, 1, 2, 3\\} = \\{(0,0),(0,1), (1,0), (1,1)\\}$. Determine the transition probability matrix.','','','','','','','A. $ \\begin{pmatrix}\r\n 	\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n 	1 - \\beta & \\beta & 0 & 0 \\\\\r\n 	0 & 0 & \\alpha & 1 - \\alpha\\\\\r\n 	0 & 0 & 1 - \\beta & \\beta\\\\\r\n \\end{pmatrix} $','B. $ \\begin{pmatrix}\r\n0 & 0 & \\alpha & 1 - \\alpha\\\\\r\n0 & 0 & 1 - \\beta & \\beta\\\\\r\n\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n\\end{pmatrix} $','C. $ \\begin{pmatrix}\r\n	\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n0& 0& 1 - \\beta & \\beta \\\\\r\n	\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n0& 0& 1 - \\beta & \\beta  \\\\\r\n\\end{pmatrix} $','D. $ \\begin{pmatrix}\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n\\alpha & 1 - \\alpha & 0 & 0\\\\\r\n\\end{pmatrix} $','E. $ \\begin{pmatrix}\r\n0 & 0 &\\alpha & 1 - \\alpha\\\\\r\n1 - \\beta & \\beta & 0 & 0 \\\\\r\n0 & 0 & \\beta & 1 - \\beta  \\\\\r\n1 - \\alpha &  \\alpha & 0 & 0\\\\\r\n\\end{pmatrix} $','','','','','','','','C','','','','\\begin{align*}\r\n\\Pr\\{Z_0 = (0,0) = 0\\} \r\n&= \\textbf{P}_{00} = \\alpha\\\\\r\n\\Pr\\{Z_0 = (0,1) = 1\\} \r\n&= \\textbf{P}_{01} = 1 - \\alpha\\\\\r\n\\Pr\\{Z_0 = (1,0) = 2\\} \r\n&= \\textbf{P}_{10} = 1 - \\beta\\\\\r\n\\Pr\\{Z_0 = (1,1) = 3\\} \r\n&= \\textbf{P}_{11} = \\beta\\\\ \r\n\\\\\r\n\\Pr\\{Z_1 = 0 | Z_0 = 0\\} \r\n&= \\textbf{P}\'_{00} = \\textbf{P}_{00}= \\alpha\\\\\r\n\\Pr\\{Z_1 = 1 | Z_0 = 0\\} \r\n&= \\textbf{P}\'_{01} = \\textbf{P}_{01} = 1 - \\alpha\\\\\r\n\\Pr\\{Z_1 = 2 | Z_0 = 0\\} \r\n&= \\textbf{P}\'_{02} = 0 = \\Pr\\{Z_1 = 3 | Z_0 = 0\\} = \\textbf{P}_{03} \\\\\r\n\\end{align*}\r\nOther transition probabilities are computed similarly, and finally the transition probability matrix is:\r\n \\[\r\n\\mathbf{P}\' = \r\n\\begin{array}{l}\r\n\\begin{array}{lllll}\r\n\\hspace{0.075cm} & \\ \\ 0 \\ & \\hspace{0.1cm} \\ \\  1 \\ \\  & \\hspace{0.1cm} \\ \\ \\ \\ 2 \\ \\  & \\hspace{0.1cm} \\  3 \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n\\alpha & 1 - \\alpha & 0 & 0  \\\\ \r\n0 & 0 & 1 - \\beta & \\beta \\\\ \r\n\\alpha & 1 - \\alpha & 0 & 0  \\\\ \r\n0 & 0 & 1 - \\beta & \\beta \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]',1,3,2,1,0,0,NULL,'','','','',NULL,0.2,2),(67,'3.3.10',3,'Consider a discrete-time, periodic reciew inventory model and let $\\xi_n$ be the total demand in period $n$, and let $X_N$ be the inventory quantity on hand at the end-of-period $n$. An ($s,S$) inventory policy is used: If the end-of-oeriod stock is not greater than $s$, then a quantity is instantly procured to bring the level up to $S$. If the end-of-period stock exceeds s, then no replenishment takes place.<br>\r\nSuppose that $\\xi_1, \\xi_2,...$ are undependent random variables where Pr\\{$\\xi_n=0$\\}=0.1, Pr\\{$\\xi_n=1$\\}=0.3, Pr\\{$\\xi_n=2$\\}=0.3, Pr\\{$\\xi_n=3$\\}=0.2, Pr\\{$\\xi_n=4$\\}=0.1. Then $X_0,X_1,...$ is a Markov chain. Determine $P_{41}$ and $P_{04}$.','','','','','','','$P_{41}=0.2\\qquad P_{04}=0.4$','$P_{41}=0.3\\qquad P_{04}=0.3$','$P_{41}=0.1\\qquad P_{04}=0.2$','$P_{41}=0.2\\qquad P_{04}=0.1$','','','','','','','','','A','','','','The only possible way to achieve ($4\\to1$) is the demand is 3. Therefore $P_{41}=\\xi_3=0.2$.<br>\r\nAs for $P_{04}$, the possible route can be $\\xi=0$ or $\\xi=4$ or $\\xi=3$\r\n$P_{04}=0.1+0.1+0.2=0.4$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(68,'3.3.2',3,'Three fair coins are tossed, and we let $ X_1 $ denote the number of heads that appear. Those coins that were heads on the first trial (there were $ X_1 $ of them) we pick up and toss again, and now we let $ X_2 $ be the total number of tails, including those left from the first toss. We toss again all coins showing tails, and let $ X_3 $ be the resulting total number of heads, including those left from the previous toss. We continue the process.The pattern is,count heads,toss heads, count tails, toss tails, count heads, toss heads, etc., and $ X_0 = 3 $. Then, $ {Xn} $ is a Markov chain. What is the transition probability matrix?','','','','','','','A. $ \\begin{pmatrix}\r\n1 & 0 & 0 & 0\\\\\r\n1/2 & 1/2 & 0 & 0\\\\\r\n1/4 & 1/2 & 1/4 & 0 \\\\\r\n1/8 & 3/8 & 3/8 & 1/8\\\\\r\n\\end{pmatrix}  $','B. $ \\begin{pmatrix}\r\n1/8 & 3/8 & 3/8 & 1/8\\\\\r\n0 & 1/4 & 1/2 & 1/4 \\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 0 & 0 & 1\\\\\r\n\\end{pmatrix}  $','C. $ \\begin{pmatrix}\r\n0 & 0 & 0 & 1\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 1/4 & 1/2 & 1/4 \\\\\r\n1/8 & 3/8 & 3/8 & 1/8\\\\\r\n\\end{pmatrix}  $','D. $ \\begin{pmatrix}\r\n1/8 & 3/8 & 3/8 & 3/8\\\\\r\n1/4 & 1/2 & 1/4 & 0\\\\\r\n1/2 & 1/2 & 0 & 0 \\\\\r\n1 & 0 & 0 & 0 \\\\\r\n\\end{pmatrix}  $','E. $ \\begin{pmatrix}\r\n0 & 0 & 0 & 1\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 1/3 & 1/3 & 1/3 \\\\\r\n1/4 & 1/4 & 1/4 & 1/4\\\\\r\n\\end{pmatrix}  $','','','','','','','','C','','','','\\[\\Pr\\{X_0 = 0\\} = \\frac{1}{8}, \\Pr\\{X_0 = 1\\} = \\frac{3}{8}, \\Pr\\{X_0 = 2\\} = \\frac{3}{8}, \\Pr\\{X_0 = 3\\} = \\frac{1}{8} \\]\r\n\r\nIf 0 head appears in the previous toss, then 3 tails must appear in this toss:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 0\\} \r\n&= \\Pr\\{X_1 = 1| X_0 = 0\\} = \\Pr\\{X_1 = 2| X_0 = 0\\} = 0\\\\\r\n\\Pr\\{X_1 = 3| X_0 = 0\\} \r\n&= 1\\\\\r\n\\end{align*}\r\nIf 1 head appears in the previous toss, then 2 or 3 tails will appear in this toss where both probabilities are 1/2 :\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 2| X_0 = 1\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 3| X_0 = 1\\} \\\\\r\n\\Pr\\{X_1 = 0| X_0 = 1\\} \r\n&= 0 = \\Pr\\{X_1 = 1| X_0 = 1\\} \\\\\r\n\\end{align*}\r\nIf 2 head appear in the previous toss, then 1, 2 or 3 tail(s) will appear in this toss where the corresponding probabilities are 1/4, 1/2 and 1/4 respectively :\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 3| X_0 = 2\\} \r\n&= \\frac{1}{4} = \\Pr\\{X_1 = 1| X_0 = 2\\}\\\\\r\n\\Pr\\{X_1 = 2| X_0 = 2\\}  \r\n&= \\frac{1}{2} \\\\\r\n\\Pr\\{X_1 = 0| X_0 = 2\\} \r\n&= 0  \\\\\r\n\\end{align*}\r\nIf 3 head appear in the previous toss, then 0, 1, 2 or 3 tail(s) will appear in this toss where the corresponding probabilities are 1/8, 3/8, 3/8 and 1/8 respectively :\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 3| X_0 = 3\\} \r\n&= \\frac{1}{8} = \\Pr\\{X_1 = 0| X_0 = 3\\} \\\\\r\n\\Pr\\{X_1 = 2| X_0 = 3\\} \r\n&= \\frac{3}{8} = \\Pr\\{X_1 = 1| X_0 = 3\\} \r\n\\end{align*}\r\nthe transition probability matrix is:\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.125cm} & \\ \\  \\ 0 \\ & \\hspace{0.125cm} \\ \\ 1 \\ \\  & \\hspace{0.125cm}  \\ 2 \\ \\  & \\hspace{0.125cm} \\  3 \\  \\ \\ \\ \\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  0 & 0 & 0 & 1  \\\\ \r\n  0 & 0 & 1/2 & 1/2 \\\\ \r\n  0 & 1/4 & 1/2 & 1/4  \\\\ \r\n  1/8 & 3/8 & 3/8 & 1/8 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]',2,3,1,3,0,0,NULL,'','','','',NULL,0.2,2),(69,'3.3.5',3,'You are going to successively flip a quarter until the pattern $ HHT $ appears, that is, until you observe two successive heads followed by a tails. In order to calculate some properties of this game, you set up a Markov chain with the following states: 0,$ H $,$ HH $, and $ HHT $, where 0 represents the starting point, $ H $ represents a single observed head on the last flip, $ HH $ represents two successive heads on the last two flips, and $ HHT $ is the sequence that you are looking for. Observe that if you have just tossed a tails, followed by a heads, a next toss of a tails effectively starts you over again in your quest for the $ HHT $ sequence. Let the state $\\{0, H, HH, HHT\\}$ be state $\\{0, 1, 2, 3\\}$ of this Markov chain $X_n$. Set up the transition probability matrix.','','','','','','','A. $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n1/2 & 0 & 1/2 & 0\\\\\r\n1/2 & 0 & 0 & 1/2 \\\\\r\n1 & 0 & 0 & 0  \\\\\r\n\\end{pmatrix}  $','B. $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n0 & 1/2 & 1/2 & 0\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n1 & 0 & 0 & 0  \\\\\r\n\\end{pmatrix}  $','C.  $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n0 & 1/2 & 1/2 & 0\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 0 & 0 & 1  \\\\\r\n\\end{pmatrix}  $','D.  $ \\begin{pmatrix}\r\n1/2 & 1/2 & 0 & 0\\\\\r\n1/2 & 0 & 1/2 & 0\\\\\r\n1/2 & 0 & 0 & 1/2 \\\\\r\n0 & 0 & 0 & 1 \\\\\r\n\\end{pmatrix}  $','E. $ \\begin{pmatrix}\r\n1/2 & 0 & 0 & 1/2\\\\\r\n0 & 1/2 & 0 & 1/2\\\\\r\n0 & 0 & 1/2 & 1/2 \\\\\r\n0 & 0 & 0 & 1  \\\\\r\n\\end{pmatrix}  $','','','','','','','','D','','','','If a head appears in the this toss and no head appeared in the previous toss, then it will become state 1; otherwise it returns to state 0:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 0\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 1| X_0 = 0\\} \\\\\r\n\\Pr\\{X_1 = 2| X_0 = 0\\} \r\n&= 0 = \\Pr\\{X_1 = 3| X_0 = 0\\} \\\\\r\n\\end{align*}\r\nIf a head appears in this toss and one head appeared in the previous toss, then it will become state 2; otherwise it returns to state 0:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 1\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 2| X_0 = 1\\} \\\\\r\n\\Pr\\{X_1 = 1| X_0 = 1\\} \r\n&= 0 = \\Pr\\{X_1 = 3| X_0 = 1\\} \\\\\r\n\\end{align*}\r\nIf a tail appears in this toss and two heads appeared in the last two tosses, then it will become state 3; otherwise it returns to state 0:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 0| X_0 = 2\\} \r\n&= \\frac{1}{2} = \\Pr\\{X_1 = 3| X_0 = 2\\} \\\\\r\n\\Pr\\{X_1 = 1| X_0 = 2\\} \r\n&= 0 = \\Pr\\{X_1 = 2| X_0 = 2\\} \\\\\r\n\\end{align*}\r\nIf it is at state 3, you win the game which means you stay at state 3:\r\n\\begin{align*}\r\n\\rightarrow \\Pr\\{X_1 = 3| X_0 = 3\\} \r\n&= 1  \\\\\r\n\\Pr\\{X_1 = 0| X_0 = 3\\} \r\n&= 0 = \\Pr\\{X_1 = 1| X_0 = 3\\}  = \\Pr\\{X_1 = 2| X_0 = 3\\}\r\n\\end{align*}\r\nthe transition probability matrix is:\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.125cm} & \\ \\  \\ 0 \\ & \\hspace{0.125cm} \\ \\ 1 \\ \\  & \\hspace{0.1cm}  \\ 2 \\ \\  & \\hspace{0.125cm} \\  3 \\  \\ \\ \\ \\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1/2 & 1/2 & 0 & 0  \\\\ \r\n  1/2 & 0 & 1/2 & 0 \\\\ \r\n  1/2 & 0 & 0 & 1/2  \\\\ \r\n  0 & 0 & 0 & 1 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]',2,3,1,3,0,0,NULL,'','','','',NULL,0.2,2),(70,'3.3.8',3,'Two urns A and B contain a total of $ N $ balls. Assume that at time $ t $, there were exactly $ k $ balls in A. At time $ t+1 $, an urn is selected at random in proportion to its contents (i.e., A is chosen with probability $ k/N $ and B is chosen with probability $ (N - k)/N $). Then, a ball is selected from A with probability $ p $ or from B with probability $ q $ and placed in the previously chosen urn. Determine the transition matrix for this Markov chain. (Note that the states of this M.C. are $\\{0,1,2,...,N\\}$)','','','','','','','A. $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\np & q & 0 & \\cdots & 0 & 0\\\\\r\n0 & p & q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & p & q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $','B.$ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{1}{N} \\right) p & \\left(\\frac{N-1}{N} \\right)q & 0 & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{2}{N} \\right)p & \\left(\\frac{N-2}{N} \\right)q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{N-1}{N} \\right) p & \\left(\\frac{1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $','C.  $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{N-1}{N} \\right) p & \\left(\\frac{1}{N} \\right)q & 0 & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{N-2}{N} \\right)p & \\left(\\frac{2}{N} \\right)q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{1}{N} \\right) p & \\left(\\frac{N-1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $','D. $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{1}{N} \\right) p & \\left(\\frac{N-1}{N} \\right) p + \\left(\\frac{1}{N} \\right) q  & \\left(\\frac{N-1}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{2}{N} \\right) p & \\left(\\frac{N-2}{N} \\right) p + \\left(\\frac{2}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{1}{N} \\right) p + \\left(\\frac{N-1}{N} \\right) q  & \\left(\\frac{1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $','E. $ \\begin{pmatrix}\r\n1 & 0 & 0 & \\cdots & 0 & 0\\\\\r\n\\left(\\frac{N-1}{N} \\right) p & \\left(\\frac{1}{N} \\right) p + \\left(\\frac{N-1}{N} \\right) q  & \\left(\\frac{1}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n0 & \\left(\\frac{N-2}{N} \\right) p & \\left(\\frac{2}{N} \\right) p + \\left(\\frac{N-2}{N} \\right) q & \\cdots & 0 & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\r\n0 & 0 & 0 & \\cdots & \\left(\\frac{N-1}{N} \\right) p + \\left(\\frac{1}{N} \\right) q  & \\left(\\frac{N-1}{N} \\right) q\\\\\r\n0 & 0 & 0 & \\cdots & 0 & 1\\\\\r\n\\end{pmatrix}  $','','','','','','','','E','','','','Let $X_n$ be the number of balls in urn A at time $n$ for $n = 0,1,...,N$. Then, for $k=1,2,...,N-1$ and $n=0,1,...$,\r\n\\begin{align*}\r\nX_n \r\n&= k,\\\\\r\n\\Pr\\{X_{n+1} = k - 1 | X_n = k\\} \r\n&= (\\frac{N-k}{N}) \\cdot p,\\\\\r\n\\Pr\\{X_{n+1} = k|X_n = k\\} \r\n&= (\\frac{k}{N}) \\cdot p + (\\frac{N-k}{N}) \\cdot q, \\text{and} \\\\\r\n\\Pr\\{X_{n+1} = k + 1|X_n = k\\} \r\n&= (\\frac{k}{N}) \\cdot q \r\n\\end{align*}\r\nFor $k = 0$, $X_n = 0$, urn B must be chosen. Although urn A may be selected to take out one ball, urn A is empty and therefore number of balls in urn A and urn B will remain 0 and N respectively. It is like the end of the process, and we called this \'ending\' state ($X_n = 0$) as \'\\textit{absorbing state}\'. The corresponding transition probabilities are    \r\n\\begin{align*}\r\n\\Pr\\{X_{n+1} = - 1 | X_n = 0 \\} \r\n&= 0\\\\\r\n\\Pr\\{X_{n+1} = 0 | X_n = 0 \\} \r\n&= 1\\\\\r\n\\Pr\\{X_{n+1} = 1 | X_n = 0 \\} \r\n&= 0\r\n\\end{align*}\r\nSimilarly, for $k = N$, $X_n = N$ urn A must be chosen. It is another absorbing state with transition probabilities \r\n\\begin{align*}\r\n\\Pr\\{X_{n+1} = N - 1 | X_n = N \\} \r\n&= 0\\\\\r\n\\Pr\\{X_{n+1} = N | X_n = N \\} \r\n&= 1\\\\\r\n\\Pr\\{X_{n+1} = N + 1 | X_n = N \\} \r\n&= 0\r\n\\end{align*}\r\nTherefore, the desired transition probability matrix is:\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllllll}\r\n \\hspace{0.175cm} & \\ \\ \\ \\ \\ \\ \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 1 \\ \\  & \\hspace{0.2cm} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 2 \\ \\  & \\hspace{0.175cm} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\cdots & N-1 & N \\\\\r\n \\end{array}\\\\\r\n\\begin{array}[c]{c}\r\n0\\\\ \r\n1\\\\\r\n2\\\\\r\n\\vdots\\\\\r\n\\\\[-1em]\r\nN\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1 & 0 & 0  & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 0 \\ \\ \\\\ \r\n    (\\frac{N-1}{N}) \\cdot p & (\\frac{1}{N}) \\cdot p + (\\frac{N-1}{N}) \\cdot q & (\\frac{1}{N}) \\cdot q & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 0 \\ \\ \\\\ \r\n  0 & (\\frac{N-2}{N}) \\cdot p & (\\frac{2}{N}) \\cdot p + (\\frac{N-2}{N}) \\cdot q & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 0 \\ \\ \\\\\r\n  \\vdots & \\vdots & \\vdots   & \\ddots & \\ \\ \\ 0 & \\ \\ \\ \\ 0 \\ \\\\\\ \r\n  0 & 0 & 0 & \\cdots & \\ \\ \\ 0 & \\ \\ \\ \\ \\ 1 \\ \\ \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]',2,3,2,3,0,0,NULL,'','','','',NULL,0.2,2),(71,'3.4.6',3,'Consider the Markov chain whose transition matrix is\r\n\\[P=\r\n\\begin{array}{c||ccccc||}\r\n&0&1&2&3&4\\\\\r\n0&q&p&0&0&0\\\\\r\n1&q&0&p&0&0\\\\\r\n2&q&0&0&p&0\\\\\r\n3&q&0&0&0&p\\\\\r\n4&0&0&0&0&1\\\\\r\n\\end{array}\r\n\\]\r\nwhere $p+q=1$. Determine the mean time to reach state 4 starting from  state 0. That is, find $E[T|X_0=i]$ where $T=$min\\{$n\\geq0;X_n=4$\\}','','','','','','','$t_0=\\frac{1}{1-q-pq-p^2q-p^3q-p^4q}$','$t_0=\\frac{1+p+p^2+p^3}{1-q-pq-p^2q-p^3q-p^4q}$','$t_0=1+p+p^2+p^3$','$t_0=\\frac{1}{1-q-pq-p^2q-p^3q-p^4q}$','','','','','','','','','B','','','','Now we want to find the mean time, so the unknowns are the mean time $t_i$. For each state, you have to take one more step to reach the next state.\r\n\\begin{align*}\r\nt_0=qt_0+pt_1+1\\\\\r\nt_1=qt_0+pt_2+1\\\\\r\nt_2=qt_0+pt_3+1\\\\\r\nt_3=qt_0+pt_4+1\\\\\r\nt_4=qt_0+pt_5+1\\\\\r\nt_5=0\r\n\\end{align*}\r\n\r\n\\begin{align*}\r\n	t_3&=qt_0+p(qt_0+1)+1=qt_0+pqt_0+p+1\\\\\r\n	t_2&=qt_0+p(qt_0+pqt_0+p+1)+1=qt_0+pqt_0+p^2qt_0+p+1\\\\\r\n	t_1&=qt_0+p(qt_0+pqt_0+p^2qt_0+p+1)+1=qt_0+pqt_0+p^2qt_0+p^3qt_0+p^2+p+1\\\\\r\n	t_0&=qt_0+pqt_0+p^2qt_0+p^3qt_0+p^4qt_0+p^3+p^2+p+1\r\n\\end{align*}',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(72,'3.4.9',3,'An urn contains five red and three yellow balls. The balls are chosen\r\nat random, one by one, from the urn. Each ball removed is replaced in the\r\nurn by a yellow ball. The selection process continues until all of the red balls\r\nhave been removed from the urn. What is the mean duration of the game?','','','','','','','8','12','274/15','50/3','44/3','','','','','','','','C','','','','Define the mean duration $t_i$ of each state(number of red ball)<br>\r\nNow consider the transition probability \r\n\\begin{equation*}\r\nP_{i(i-1)}=i/8\r\n\\end{equation*}\r\nand \r\n\\begin{equation*}\r\nP_{ii}=1-i/8\r\n\\end{equation*}\r\n\r\nso the system of equations are\r\n\\begin{align*}\r\nt_5=\\frac{5}{8}t_4+\\frac{3}{8}t_5+1\\\\\r\nt_4=\\frac{1}{2}t_3+\\frac{1}{2}t_4+1\\\\\r\nt_3=\\frac{3}{8}t_2+\\frac{5}{8}t_3+1\\\\\r\nt_2=\\frac{1}{4}t_1+\\frac{3}{4}t_2+1\\\\\r\nt_1=\\frac{7}{8}t_1+1\\\\\r\n\\end{align*}\r\nso\r\n\\begin{align*}\r\nt_1=8\\\\\r\nt_2=12\\\\\r\nt_3=44/3\\\\\r\nt_4=50/3\\\\\r\nt_5=274/15\r\n\\end{align*}',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(73,'3.4.12',3,'A Markov chain $X_0, X_1, X_2,...$ has the transition probability matrix\r\n\\[P=\r\n\\begin{array}{c||ccc||}\r\n&0&1&2\\\\\r\n0&0.3&0.2&0.5\\\\\r\n1&0.5&0.1&0.4\\\\\r\n2&0&0&1\r\n\\end{array}\r\n\\]\r\nand is known to start in state $X_0=0$. Eventually, the process will end up\r\nin state 2. What is the probability that when the process moves into state\r\n2, it does so from state 1?','','','','','','','0.150943','0.528302','0.849057','0.471698','','','','','','','','','A','','','','Note that two events have to be defined:<br>\r\n($1\\to2$) with win probability 1 <br>\r\n($0\\to2$) with win probability 0 <br>\r\n\r\nSo we define the unknown:win probability to be $p_1$ and $p_2$<br>\r\n\r\n\\begin{align*}\r\np_0&=0.3p_0+0.2p_1\\\\\r\np_1&=0.5p_0+0.1p_1+0.4(1)\\\\\r\n\\end{align*}\r\nso\r\n\\begin{equation*}\r\np_0=0.150943\\qquad p_1=0.528302\r\n\\end{equation*}\r\nSo the probability is 0.150943',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(74,'3.4.15',3,'A simplified model for the spread of a rumor goes this way: There\r\nare $N=5$ people in a group of friends, of which some have heard the\r\nrumor and the others have not. During any single period of time, two people\r\nare selected at random from the group and assumed to interact. The selection\r\nis such that an encounter between any pair of friends is just as\r\nlikely as between any other pair. If one of these persons has heard the\r\nrumor and the other has not, then with probability $a = 0.1$ the rumor is\r\ntransmitted. Let $X_n$, denote the number of friends who have heard the\r\nrumor at the end of the nth period.\r\nAssuming that the process begins at time 0 with a single person knowing\r\nthe rumor, what is the mean time that it takes for everyone to hear it?','','','','','','','83.3333','58.3333','41.6777','25','','','','','','','','','A','','','','The transition probability for $X_n$ can be written as\r\n$P_{i(i+1)}=\\alpha i(5-i)/\\binom{5}{2}$<br>\r\n$P_{ii}=1-i(5-i)/\\binom{5}{2}$<br>\r\n\r\nNow define the mean time $t_i$ starting at each state\r\n\\begin{align*}\r\nt_1=0.96t_1+0.04t_2+1\\\\\r\nt_2=0.94t_2+0.06t_3+1\\\\\r\nt_3=0.94t_3+0.06t_4+1\\\\\r\nt_4=0.96t_4+1\\\\\r\n\\end{align*} \r\nso\r\n\\begin{align*}\r\nt_1=83.3333\\\\\r\nt_2=58.3333\\\\\r\nt_3=41.6667\\\\\r\nt_4=25\r\n\\end{align*}\r\n\r\nThe answer is $t_1=83.3333$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(75,'3.4.1',3,'A coin is being flipped. How many flips are needed to take, on average: successively flipping a quarter until the pattern $ HHT $ appears, i.e., until you observe two successive heads followed by a tails; or successively flipping a quarter until the pattern $ HTH $ appears?','','','','','','','A. $HHT$: 14 flips; $HTH$: 10 flips.','B. $HHT$: 13 flips; $HTH$: 11 flips.','C. $HHT$: 12 flips; $HTH$: 12 flips.','D. $HHT$: 11 flips; $HTH$: 13 flips.','E. $HHT$: 10 flips; $HTH$: 14 flips.','','','','','','','','A','','','','Let $u_i$ ,$v_j$ be the mean numbers of flip to reach $HHT$ and $HTH$ starting at state $i$ and $j$ respectively, for $i \\in \\{0, H, HH, HHT\\} = \\{0, 1, 2, 3\\}$ and $j \\in \\{0, H, HT, HTH\\} = \\{0, 1, 2, 3\\}$. Then, for the first case:\r\n	\\begin{align}\r\n	u_0 &= 1 + \\frac{1}{2} u_0 + \\frac{1}{2} u_1\\\\\r\n	u_1 &= 1 + \\frac{1}{2} u_0 + \\frac{1}{2} u_2\\\\\r\n	u_2 &= 1 + \\frac{1}{2} u_0 + \\frac{1}{2} u_3\\\\\r\n	u_3 &= 0\r\n	\\end{align}\r\n	By solving the above linear system of equations, we have $\\underline{u_0 = 14}$, $u_1 = 12$ and $u_2 = 8$. For the second case:\r\n	\\begin{align}\r\n	v_0 &= 1 + \\frac{1}{2} v_0 + \\frac{1}{2} v_1\\\\\r\n	v_1 &= 1 + \\frac{1}{2} v_1 + \\frac{1}{2} u_2\\\\\r\n	v_2 &= 1 + \\frac{1}{2} v_2 + \\frac{1}{2} u_3\\\\\r\n	v_3 &= 0\r\n	\\end{align}\r\n	By solving the above linear system of equations, we have $\\underline{v_0 = 10}$, $v_1 = 8$ and $v_2 = 6$. Therefore, the desired numbers of flips are 14 flips and 10 flips respectively.   \r\n<br> <br>\r\n	An intuitive way to think of the reason that the second case takes fewer flips is: it is harder to obtain $HH$ than $HT$ since you have to restart the steps from state 0 if you flipped a tail. In the second case, you stay at state of your previous step if you flipped a tail.',1,3,2,2,0,0,NULL,'','','','',NULL,0.2,2),(76,'3.4.4',3,'Consider the Markov chain whose transition probability matrix is given by\r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.075cm} & \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\  1 \\ \\  & \\hspace{0.125cm}   2 \\ \\  & \\hspace{0.125cm}  3 \\  \\ \\ \\ \\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1 & 0 & 0 & 0  \\\\ \r\n  0.1 & 0.2 & 0.5 & 0.2 \\\\ \r\n  0.1 & 0.2 & 0.6 & 0.2 \\\\ \r\n  0.2 & 0.2 & 0.3 & 0.3 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nStarting in state $ X_0 = 1 $, determine the probability that the process never visits state 2.','','','','','','','A. 0','B. 11/52','C. 41/52','D. 17/26','E. 9/26','','','','','','','','B','','','','Let $p_i = \\Pr\\{\\text{reaching state 2} \\ | \\ \\text{start from state} \\ i\\}$. Then, we have:\r\n	\\begin{align}\r\np_0 &= \\textbf{P}_{00} \\cdot p_0 + \\textbf{P}_{01} \\cdot p_1 + \\textbf{P}_{02} \\cdot p_2 + \\textbf{P}_{03} \\cdot p_3 = p_0 = 0\\\\\r\np_1 &= 0.2 p_1 + 0.5 p_2 + 0.2 p_3\\\\\r\np_2 &= 1\\\\\r\np_3 &= 0.2 p_1 + 0.3 p_2 + 0.3 p_3\\\\\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $p_1 = 41/52$, and $p_3 = 34/52$. The desired probability is\r\n\\begin{align*}\r\n1 - p_1 &= 11/52 \\\\\r\n		&\\approx 0.2115\r\n\\end{align*}',2,2,2,1,0,0,NULL,'','','','',NULL,0.2,2),(77,'3.4.7',3,'Let $ X_n $ be a Markov chain with transition probabilities $ P_{ij} $. We are given a discount factor $ \\beta $ with $ 0 < \\beta < 1 $ and a cost function $ c(i) $, and we wish to determine the total expected discounted cost starting from state $ i $, de?ned by \r\n \\[h_i = E \\left[ \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_0 = i\\right] \\]\r\n Using a first step analysis show that $ h_i $ is a solution of one of the following system of linear equations for all states i.','','','','','','','A. \\[c(i) + \\beta^i \\sum_{j} \\textbf{P}_{ij} h_j\\] \\','B. \\[c(i) + \\beta \\sum_{j} \\textbf{P}_{ij} h_j\\] \\','C. \\[c(0) + \\beta^j \\sum_{j} \\textbf{P}_{ij} h_j\\]','D. \\[c(0) + \\beta \\sum_{j} \\textbf{P}_{ij} h_j\\]','E. \\[c(i) + \\beta \\sum_{j}  h_j\\]','','','','','','','','B','','','','\\begin{align*}\r\nh_i\r\n&= E \\left[ \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_0 = i\\right]\\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) \\cdot 1_{ \\{X_1 = i \\} } | X_0 = i \\right] \\ \\ \\left( \\ \\text{where} \\ \\ 1_{ \\{X_1 = i \\} } = \\begin{cases}\r\n1 &  \\ \\ \\text{if} \\ \\ X_1 = i\\\\\r\n0 &  \\ \\ \\text{if} \\ \\ X_1 \\neq i\\\\\r\n\\end{cases} \\ \\ \\right) \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) \\cdot 1_{ \\{X_1 = 0 \\} } | X_0 = i \\right] + \r\n   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) \\cdot 1_{ \\{X_1 = 1 \\} } | X_0 = i \\right] + \\cdots \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 0, X_0 = i \\right] \\cdot \\Pr\\{X_1 = 0 | X_0 = i\\}  \\\\\r\n& \\ \\ + E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 1, X_0 = i \\right] \\cdot \\Pr\\{X_1 = 1 | X_0 = i\\} + \\cdots \\ \\left( \\ \\text{Conditional Independence} \\ \\right) \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 0, X_0 = i \\right] \\cdot \\textbf{P}_{i0} + \r\n   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 1, X_0 = i \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} + \r\n   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^n c(X_n) | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\ (\\text{property of Markov Chain})\\\\\r\n&= E \\left[  c(X_0) + \\beta c(X_1) + \\cdots | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n   E \\left[  c(X_0) + \\beta c(X_1) + \\cdots | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[ c(x_0) \\right] \\cdot (\\textbf{P}_{i0} + \\textbf{P}_{i1} + \\cdots) + \r\n   E \\left[ \\beta c(X_1) + \\cdots | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n   E \\left[ \\beta c(X_1) + \\cdots | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[ c(x_0) \\right] \\cdot 1 + \r\n   \\beta E \\left[  \\sum_{n = 1}^{\\infty} \\beta^{n-1} c(X_n) | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n   \\beta E \\left[  \\sum_{n = 1}^{\\infty} \\beta^{n-1} c(X_n)  | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots \\\\\r\n&= E \\left[ c(x_0) \\right] + \r\n\\beta \\cdot \\left( E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n+1}) | X_1 = 0 \\right] \\cdot \\textbf{P}_{i0} +\r\n				   E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n+1})  | X_1 = 1 \\right] \\cdot \\textbf{P}_{i1} + \\cdots\r\n		    \\right) \\ (\\text{shifting index})\\\\\r\n&= c(i) + \r\n\\beta \\cdot \\sum_{j} \\left( E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n+1}) | X_1 = j \\right] \\cdot \\textbf{P}_{ij} \\right) \\\\\r\n&= c(i) + \r\n\\beta \\cdot \\sum_{j} \\left( E \\left[  \\sum_{n = 0}^{\\infty} \\beta^{n} c(X_{n}) | X_0 = j \\right] \\cdot \\textbf{P}_{ij} \\right) \\ (\\text{it takes same number of steps from 0 to $n$ and from 1 to $n+1$}) \\\\\r\n&= c(i) + \r\n\\beta \\cdot \\sum_{j} \\left( h_j \\cdot \\textbf{P}_{ij} \\right) \r\n = \\underline{c(i) + \\beta \\sum_{j} \\textbf{P}_{ij} h_j} \\\\\r\n\\end{align*}',3,1,3,1,0,0,NULL,'','','','',NULL,0.2,2),(78,'3.4.16',3,'An urn contains five tags, of which three are red and two are green. A tag is randomly selected from the urn and replaced with a tag of the opposite color. This continues until only tags of a single color remain in the urn. Let $ X_n $ denote the number of red tags in the urn after the nth draw, with $ X_0 = 3 $. What is the probability that the game ends with the urn containing only red tags?','','','','','','','A. 1/2','B. 3/8','C. 5/8','D. 15/32','E. 17/32','','','','','','','','E','','','','Let \\[p_i = \\Pr \\{ \\text{reach state 5} \\ | \\ \\text{starting at state} \\ i \\}.\\] <br>\r\nThen, the transition probability matrix is \r\n \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{lllll}\r\n \\hspace{0.125cm} & \\ \\  \\ 0 \\ & \\hspace{0.125cm} \\ \\ 1 \\ \\  & \\hspace{0.125cm} \\  2 \\ \\  & \\hspace{0.125cm}  \\ 3 \\  \\ & \\hspace{0.125cm}  4 & \\hspace{0.125cm}  \\ \\ \\ 5\\\\\r\n \\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n4\\\\\r\n5\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n  1 & 0 & 0 & 0 & 0 & 0 \\\\ \r\n  1/5 & 0 & 4/5 & 0 & 0 & 0  \\\\ \r\n  0 & 2/5 & 0 & 3/5 & 0 & 0  \\\\ \r\n  0 & 0 & 3/5 & 0 & 2/5 & 0 \\\\ \r\n  0 & 0 & 0 & 4/5 & 0 & 1/5 \\\\ \r\n  0 & 0 & 0 & 0 & 0 & 1 \\\\ \r\n\\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\nThen, we have:\r\n\\begin{align}\r\np_0 &= 0\\\\\r\np_1 &= 1/5 p_0 + 4/5 p_2\\\\\r\np_2 &= 2/5 p_1 + 3/5 p_3\\\\\r\np_3 &= 3/5 p_2 + 2/5 p_4\\\\\r\np_4 &= 4/5 p_3 + 1/5 p_5\\\\\r\np_5 &= 1\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $p_1 = 3/8$, $p_2 = 15/32$, $\\underline{p_3 = 17/32}$ and $p_4 = 5/8$. The desired probability is \\[p_3 = 17/32 = 0.53125\\]',1,3,1,3,0,0,NULL,'','','','',NULL,0.2,2),(79,'3.8.3',3,'Families in a certain society choose the number of children that they will have according to the following rule: If the first child is a girl, they have exactly one more child. If the first child is a boy, they continue to have children until the first girl, and then cease childbearing. <br>\r\n (a) Let $\\xi$ denotes the number of children of a particular family. For $ k=0,1,2,... $, what is the probability that a particular family will have $ k $ children in total? <br>\r\n (b) Let $\\xi$ denotes the number of male children of a particular family. For $ k=0,1,2,... $, what is the probability that a particular family will have exactly $ k $ male children among their offspring?','','','','','','','A. <br>\r\n(a). $P(\\xi = 0) = 0, P(\\xi = 1) = \\frac{1}{2}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 2$  <br>\r\n(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k+1}$ for $k \\geq 1$  <br>','B. <br>\r\n(a). $P(\\xi = 0) = 0, P(\\xi = 1) = 0, P(\\xi = 2) = \\frac{3}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 3$  <br>\r\n	(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{2}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k+1}$ for $k \\geq 2$  <br>','C. <br>\r\n	(a). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 2$ <br>\r\n	(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{2}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k+1}$ for $k \\geq 2$  <br>','D. <br>\r\n	(a). $P(\\xi = 0) = 0, P(\\xi = 1) = 0, P(\\xi = 2) = \\frac{3}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 3$ <br>\r\n	(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k}$ for $k \\geq 2$  <br>','E. <br>\r\n		(a). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^k$ for $k \\geq 2$ <br>\r\n	(b). $P(\\xi = 0) = \\frac{1}{4}, P(\\xi = 1) = \\frac{1}{4}, P(\\xi = k) = \\left(  \\frac{1}{2}\\right)^{k}$ for $k \\geq 2$  <br>','','','','','','','','B','','','','(a).\r\n\\begin{align*}\r\nP(\\xi = 0) \r\n&= 0 = P(\\xi = 1), (\\text{all families will have at least 2 baby}) \\\\\r\nP(\\xi = 2) \r\n&= \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{3}{4} \\\\\r\n& \\ \\ (\\text{first term refers to the case where first child is a girl, and second  }\\\\ \r\n& \\ \\ \\text{ term refers to another case where the second child is a girl}) \\\\\r\nP(\\xi = k)\r\n&= \\left( \\frac{1}{2} \\right)^k \\ \\text{for} \\ k \\geq 3\\\\\r\n& (\\text{the families stop having more children when their $k$th child is a girl})\r\n\\end{align*}\r\n(b).\r\n\\begin{align*}\r\nP(\\xi = 0) \r\n&= \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4} \\ \\ (\\text{first 2 children are both female}) \\\\\r\nP(\\xi = 1) \r\n&= \\frac{1}{2} \\cdot \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{2} \\\\\r\n& \\ \\ (\\text{first term refers to the case where first child is a girl, and second child is a boy. }\\\\ \r\n& \\ \\ \\text{  The second term refers to another case where the second child is a girl}) \\\\\r\nP(\\xi = k)\r\n&= \\left( \\frac{1}{2} \\right)^{k+1} \\ \\text{for} \\ k \\geq 2\\\\\r\n& (\\text{the families stop having more children when their $k$th child is a girl, and so there} \\\\\r\n& \\text{ are (k-1) of boys} )\r\n\\end{align*}',3,2,3,3,0,0,NULL,'','','','',NULL,0.2,2),(80,'3.9.2',3,'One-fourth of the married couples in a far-off society have exactly three children. The other three-fourths of couples continue to have children until the first boy and then cease childbearing. Assume that each child is equally likely to be a boy or girl. Let $\\xi$ denotes the number of male offspring of a pair of married couple, and assumes all the married couple will have at least one child. What is the probability that the male line of descent of a particular husband $u_\\infty$ will eventually die out?','','','','','','','A. 1','B. $\\sqrt{2}/2$','C. $2(\\sqrt{2} - 1)$','D. $(\\sqrt{2} - 1)/2$','E. It cannot be determined.','','','','','','','','C','','','','Let $p_k = \\Pr\\{\\xi = k\\}$. Then,\r\n\\begin{align*}\r\np_0 &= \\frac{1}{4} \\cdot \\left( \\frac{1}{2} \\right)^3 + \\frac{3}{4} \\cdot \\left( \\frac{1}{2} \\right)^3 = \\frac{1}{8}\\\\\r\np_1 &= \\frac{1}{4} \\cdot \\left( \\frac{3}{8} \\right) + \\frac{3}{4} \\cdot \\left( \\frac{1}{2} + \\frac{1}{4} + \\frac{1}{8} \\right) = \\frac{3}{4}\\\\\r\np_2 &= \\frac{1}{4} \\cdot \\left( \\frac{3}{8} \\right) + 0 = \\frac{3}{32}\\\\\r\np_3 &= \\frac{1}{4} \\cdot \\left( \\frac{1}{8} \\right) + 0 = \\frac{1}{32}\\\\\r\n\\end{align*}\r\nThe first terms are the contribution from one-fourth of the married couples who have exactly three children. The second terms are from the other three-fourth of the married couple. Therefore, the probability generation function $\\phi(s)$ is given as\r\n\\[\\phi(s) =E \\left( s^\\xi \\right) = \\sum_{k = 0}^{3} s^k p_k = \\frac{1}{8} + \\frac{3}{4} s + \\frac{3}{32} s^2 + \\frac{1}{32} s^3 \\]\r\nSolving $\\phi(s) - s = 0$, we have:\r\n\\begin{align*}\r\n\\phi(s) - s &= 0 \\\\\r\n \\longrightarrow s^3 + 3s^2 - 8s + 4 &= 0\\\\\r\n (s-1)(s^2 + 4s - 4) &= 0\\\\\r\n s &= 1 \\ \\text{or} \\ \\frac{-4 \\pm \\sqrt{16 + 16}}{2}\\\\\r\n   &= 1 \\ \\text{or} \\ -2 \\pm 2 \\sqrt{2}\\\\\r\n   &= -2(\\sqrt{2} + 1) \\  \\text{or} \\ 2(\\sqrt{2} - 1) \\ \\text{or} \\ 1\\\\\r\n\\because 0 < 2(\\sqrt{2} - 1) < 1, \\\\\r\n\\therefore u_\\infty &= \\underline{2(\\sqrt{2} - 1)} \\approx 0.828\r\n\\end{align*}',2,3,1,3,0,0,NULL,'','','','',NULL,0.2,2),(81,'3.9.5',3,'At time 0, a blood culture starts with one red cell. At the end of 1 min, the red cell dies and is replaced by one of the following combinations with the probabilities as indicated:\r\n\\[\r\n\\begin{array}{rc}\r\n\\hline \r\n\\text{Two red cells} & \\frac{1}{4} \\\\ \r\n\\text{One red cell, One white cell} & \\frac{2}{3} \\\\\r\n\\text{Two white cells} & \\frac{1}{12} \\\\ \r\n \\hline\r\n\\end{array}\r\n\\]\r\nEach red cell lives for 1 min and gives birth to offspring in the same way as the parent cell. Each white cell lives for 1 min and dies without reproducing. Assume that individual cells behave independently.  <br>\r\n(a) Let $\\xi^{(n)}$ be the number of red cells after $n$th generation. At time $ n + \\frac{1}{2} $ min after the culture begins, what is the probability that no white cells have yet appeared? <br>\r\n(b) Let $\\xi$ be the number of red cells in the offspring of a red cell. What is the probability that the entire culture eventually dies out entirely?','','','','','','','A. <br>\r\n	 (a). \\[\\left( \\frac{1}{4} \\right)^{2^n} \\] <br>\r\n	 (b). \\[1\\]','B. <br>\r\n	 (a). \\[\\left( \\frac{1}{4} \\right)^{2^n} \\] <br>\r\n 	 (b). \\[ \\frac{1}{3}\\]','C. <br>\r\n	 (a). \\[\\left( \\frac{1}{4} \\right)^{2^n - 1} \\] <br>\r\n 	 (b). \\[1\\]','D. <br>\r\n 	(a). \\[\\left( \\frac{1}{4} \\right)^{2^n - 1} \\] <br>\r\n 	(b). \\[ \\frac{1}{3}\\]','E. <br>\r\n 	(a). \\[\\left( \\frac{1}{4} \\right)^{2n} \\] <br>\r\n 	(b). \\[ \\frac{1}{3}\\]','','','','','','','','D','','','','(a).\r\n\\begin{align*}\r\nP(\\xi^{(0)} =2) &= \\frac{1}{4}\\\\\r\nP(\\xi^{(1)} =2^2 = 4) &= \\left( \\frac{1}{4} \\right)^2\\\\\r\n\\vdots \\ \\ \\ \\ \\ \\ & \\ \\ \\ \\ \\ \\ \\ \\vdots\\\\\r\nP(\\xi^{(n-1)} = 2^{n}) &= \\left( \\frac{1}{4} \\right)^{2^{n-1}}\\\\ \r\n\\therefore \\ P( \\text{no white cells for} \\ n \\text{th generation})\r\n&= P(\\xi^{(0)} \\cap \\xi^{(1)} \\cap \\cdots \\cap \\xi^{(n-1)}) \\  (\\text{there are $n$ terms in total})\\\\\r\n&= \\left( \\frac{1}{4} \\right) \\left( \\frac{1}{4} \\right)^2 \\left( \\frac{1}{4} \\right)^4 \\cdots \\left( \\frac{1}{4} \\right)^{2^{n-1}}\\\\\r\n&=\\left( \\frac{1}{4} \\right)^{1 + 2 + 4 + \\cdots + 2^{n-1}}\\\\\r\n&= \\underline{\\left( \\frac{1}{4} \\right)^{2^n - 1}}\r\n\\end{align*}\r\n(b).\r\n\\[ P(\\xi = 0) = \\frac{1}{12}, \\ P(\\xi = 1) = \\frac{2}{3}, \\ P(\\xi = 2) = \\frac{1}{4} \\]\r\nThe probability generation function $\\phi(s)$ is given as\r\n\\[ \\phi(s) = \\frac{1}{12} + \\frac{2}{3} s + \\frac{1}{4} s^2 \\]\r\nSolving $\\phi(s) - s = 0$, we have:\r\n\\begin{align*}\r\n\\phi(s) - s &= 0 \\\\\r\n\\longrightarrow 3s^2 - 4s + 1 &= 0\\\\\r\n(s-1)(3s - 1) &= 0\\\\\r\ns &= 1 \\ \\text{or} \\ \\frac{1}{3}\\\\\r\n\\because 0 < \\frac{1}{3} < 1, \\ \\\r\n\\therefore u_\\infty &= \\underline{\\frac{1}{3}} \r\n\\end{align*}',2,3,2,3,0,0,NULL,'','','','',NULL,0.2,2),(82,'3.8.2',3,'Let $Z=\\sum_{n=0}^{\\infty}X_n$, be the total family size in a branching process\r\nwhose offspring distribution has a mean $\\mu = E[\\xi] < 1$. Assuming that\r\n$X_0 = 1$, what is $E[Z]$?.','','','','','','','$1/(1+\\mu)$','$1/(1-\\mu)$','$\\mu/(1-\\mu)$','','','','','','','','','','B','','','','\\begin{align*}\r\nE[z]&=\\sum_{n=0}^{\\infty}E[X_n]=\\sum_{n=0}^{\\infty}\\mu^n=1/(1-\\mu)\r\n\\end{align*}',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(83,'3.9.8',3,'Consider a branching process whose offspring follow the geometric distribution $p_k = ( 1 - c) \\ c^k$ for $k = 0,1,...$, where $0.5 < c < 1$. Determine the probability of eventual extinction.','','','','','','','A. \\[0\\]','B. \\[1\\]','C. \\[\\frac{2c - 1}{2c}\\]','D. \\[\\frac{1 -c}{1 + c}\\]','E. \\[\\frac{1 -c}{c}\\]','','','','','','','','E','','','','\\begin{align*}\r\np_k \r\n&= (1-c) \\ c^k \\ \\ \\text{for} \\ \\ k = 0,1,... , \\ \\text{where} \\ \\ c \\in (0,1) \\\\\r\n\\therefore \\ \\ \\phi(s)\r\n&= (1-c) + (1-c) \\ c \\cdot s + (1-c) \\ c^2 \\cdot s^2 + \\cdots \\\\\r\n&= (1-c) \\cdot [1 + cs + (cs)^2 + \\cdots]\\\\\r\n&= (1-c) \\ \\sum_{k = 0}^{\\infty} (cs)^k \\\\\r\n&= (1-c) \\cdot \\frac{1}{1 - cs} \\ \\text{(using Taylor Series Expansion to obtain the latter term from the infinite sum)}\\\\\r\n&= \\frac{1 - c}{1 - cs}\r\n\\end{align*}\r\nSolving $\\phi(s) - s = 0$, we have: \r\n\\begin{align*}\r\n\\frac{1 - c}{1 - cs}\r\n&= s \\\\\r\n\\rightarrow c \\cdot s^2 - s + (1 - c)\r\n&= 0\\\\\r\ns\r\n&= \\frac{1 \\pm \\sqrt{1^2 - 4c(1-c)}}{2c}\\\\\r\n&= \\frac{1 \\pm \\sqrt{4c^2 - 4c + 1}}{2c}\\\\\r\n&= \\frac{1 \\pm \\sqrt{(2c - 1)^2}}{2c} = \\frac{1 \\pm (2c -1)}{2c} = 1 \\ \\text{or} \\ \\frac{1-c}{c}\\\\ \\\\\r\n\\therefore \\ \\ u_\\infty \r\n&= \\begin{cases}\r\n1 & \\text{if} \\ \\ \\ c \\leq 0.5  \\\\\r\n\\frac{1 - c}{c} & \\text{if} \\ \\ \\ 0.5 < c < 1\\\\\r\n\\end{cases}\r\n\\end{align*}',3,2,1,1,0,0,NULL,'','','','',NULL,0.2,2),(84,'3.9.1',3,'One-fourth of the married couples in a far-off society have no children\r\nat all. The other three-fourths of families have exactly three children,\r\neach child equally likely to be a boy or a girl. What is the probability that\r\nthe male line of descent of a particular husband will eventually die out?','','','','','','','0','$\\frac{\\sqrt{69}-6}{3}$','1','$\\frac{-\\sqrt{69}-6}{3}$','none of the above','','','','','','','','B','','','','The probability of having\r\n0 boy:$1/4+3/4(1/8)=11/32$<br>\r\n1 boy:$3/4(3/8)=9/32$<br>\r\n2 boy:$3/4(3/8)=9/32$<br>\r\n3 boy:$3/4(1/8)=3/32$<br>\r\nApply $u_{\\infty}=\\phi(u_{\\infty})$\r\n\\begin{equation*}\r\nu_{\\infty}=11/32+(9/32)u_{\\infty}+(9/32)u_{\\infty}^2+(3/32)u_{\\infty}^3\r\n\\end{equation*}\r\nSo\r\n\\begin{equation*}\r\nu_{\\infty}=\\frac{-\\sqrt{69}-6}{3}\\text{  or  }\\frac{\\sqrt{69}-6}{3}\\text{  or  }1\r\n\\end{equation*}\r\nSince $u_{\\infty}$ converge to the smallest solution between 0 and 1, the answer is $\\frac{\\sqrt{69}-6}{3}$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(85,'3.9.7',3,'Families in a certain society choose the number of children that\r\nthey will have according to the following rule: If the first child is a girl,\r\nthey have exactly one more child. If the first child is a boy, they continue\r\nto have children until the first girl and then cease childbearing. Let $\\xi$ be\r\nthe number of male children in a particular family. Determine the mean of $\\xi$ directly or by differentiating the generating function.','','','','','','','2','0.5','1.75','2.5','','','','','','','','','C','','','','Consider the second rule, $\\xi$ obviously follow the geometric distribution.\r\nConsider the first rule, there is half the probability having one male child.\r\n\r\nSo \r\nPr\\{$\\xi=0$\\}$=\\frac{1}{2^2}$<br>\r\nPr\\{$\\xi=1$\\}$=\\frac{1}{2^2}$<br>\r\nPr\\{$\\xi=k>1$\\}$=\\frac{1}{2^k}$<br>\r\n\\begin{align*}\r\n\\phi(s)=\\frac{1}{2^2}+\\frac{1}{2^2}s+\\sum_{k=2}^{\\infty}\\frac{1}{2^k}s^k=\\frac{1}{2^2}+\\frac{1}{2^2}s+\\frac{s^2/4}{1-s/2}\r\n\\end{align*} \r\nOnce we differentiate the probability generating function, \r\n\\begin{align*}\r\n\\phi\'(s)=\\frac{1}{2^2}-\\frac{s(s-4)}{2(s-2)^2}\\\\\r\n\\phi\'(1)=\\frac{1}{2^2}+\\frac{3}{2}\r\n\\end{align*}\r\nIf we consider the mean of $\\xi$ directly, it is complicated as it involve clever shifting of index so that the $k$ is cancelled.\r\n\\begin{align*}\r\nE[\\xi]&=\\frac{1}{2^2}+\\sum_{k=2}^{\\infty}\\frac{k}{2^{k-1}}(1-\\frac{1}{2})\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=2}^{\\infty}\\frac{k}{2^{k-1}}-\\frac{k}{2^{k}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=1}^{\\infty}\\frac{k+1}{2^{k}}-\\frac{k+1}{2^{k+1}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=0}^{\\infty}\\frac{k}{2^{k+1}}+\\left(\\frac{2}{2^{k+1}}-\\frac{1}{2^{k+2}}\\right)-\\sum_{k=1}^{\\infty}\\frac{k}{2^{k+1}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=0}^{\\infty}\\frac{k}{2^{k+1}}+\\left(\\frac{2}{2^{k+1}}-\\frac{1}{2^{k+2}}\\right)-\\sum_{k=0}^{\\infty}\\frac{k}{2^{k+1}}\\\\\r\n&=\\frac{1}{2^2}+\\sum_{k=0}^{\\infty}\\left(\\frac{2}{2^{k+1}}-\\frac{1}{2^{k+2}}\\right)\\\\\r\n&=\\frac{1}{2^2}+\\frac{3}{2}\\\\\r\n\\end{align*}\r\nAnother way of doing it is to calculate the expectation value of a geometric distribution from 1 is $\\frac{1}{p}=2$. Now we start at $k=2$, so we subtract the contribution of $k=1$:$\\frac{1}{2}$. So the value of the second term is $2-\\frac{1}{2}=\\frac{3}{2}$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,4),(86,'3.9.10.a',3,'Suppose that in a branching process the number of offspring of an initial particle has a distribution whose generating function is $f(s)$. Each member of the first generation has a number of offspring whose distribution has generating function $g(s)$. The next generation has generating function $f$, the next has $g$ , and the distribution continue to alternate in this way from generation to generation. <br>\r\nDetermine the extinction probability of the process in terms of $f(s)$ and $g(s)$.','','','','','','','$(f+g)(u_{\\infty})$','$(fg)(u_{\\infty})$','$f\\circ g(u_{\\infty})$','$f\\ast g(u_{\\infty})$<br>\r\n$\\ast$ denote convolution operation','$g\\circ f(u_{\\infty})$','$f^g(u_{\\infty})$','','','','','','','C','','','','Here,  the key idea is to treat each two transition as one transition. First we have $\\xi_1\\to k_1$ and $\\xi_2\\to k_{2,i}$(i run from 1 to $k_1$, denoting the index of the second transition) for the two process $f$ and $g$.<br>\r\nThe outcome $k$ of each two transition(there are $k_1$ roll for $\\xi_2$, each $\\xi_2$ give out $k_2$) is then \r\n\\begin{equation*}\r\nk=\\sum_{i=0}^{k_1}k_{2,i}\r\n\\end{equation*}\r\nFor each outcome $\\sum_{i=0}^{k_1}k_{2,i}$,  the probability is\r\n\\begin{equation*}\r\nP(k)={\\xi}_1(k_1)\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})\r\n\\end{equation*}\r\nNow we have our generating function $\\phi(s)$ for the two steps transition, note to run over all the outcomes, the summation is $\\sum_k=\\sum_{k_1}\\sum_{i=0}^{k_1}$\r\n\\begin{align*}\r\n\\phi(s)=\\sum_kP(k)s^k&=\\sum_{k_1}\\sum_{i=0}^{k_1}s^{\\sum_{i=0}^{k_1}k_{2,i}}{\\xi}_1(k_1)\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})\\\\\r\n&=\\sum_{k_1}{\\xi}_1(k_1)\\sum_{i=0}^{k_1}\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})s^{k_{2,i}}\r\n\\end{align*}\r\nSince each term have element $\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})s^{k_{2,i}}$ and we have to run over all possible outcomes $k$ (usually come from multiplication of series), it is natural to think of $g^{k_1}$.<br>\r\nAfter identifying the $g^{k_1}$ term, everything is straight forward:\r\n\\begin{align*}\r\n\\phi(s)&=\\sum_{k_1}{\\xi}_1(k_1)\\sum_{i=0}^{k_1}\\prod_{i=0}^{k_1}{\\xi}_2(k_{2,i})s^{k_{2,i}}\\\\&=\\sum_{k_1}\\xi_1(k_1)g^{k_1}=f\\circ g(s)\r\n\\end{align*}\r\nTherefore, the extinction probability is $f\\circ g(u_{\\infty})$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,4),(87,'3.4.10',3,'You have five fair coins. You toss them all so that they randomly fall heads or tails. Those that fall tails in the first toss you pick up and toss again. You toss again those that show tails after the second toss, and so on, until all show heads. Let $ X $ be the number of coins involved in the last toss. Find $ \\Pr\\{X=1\\} $.','','','','','','','A. 2/3','B. 5/7','C. 29/105','D. 76/105','E. 157/217','','','','','','','','E','','','','Let $X_n$ be the number of tails at the end of period $n$. Then, $\\{X_n\\}$ is a Markov chain with state $ \\{0, 1, 2, 3, 4, 5\\} $. \\\\ \\\\\r\nIf 0 tail appears in the previous toss, the process will be ended. So it is an absorbing state:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 0\\} &= 1\\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0\\} &= 0\\\\\r\n\\end{align*}\r\nIf 1 tail appears in the previous toss, 0 or 1 tail will appear in this toss where both probabilities are 1/2:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 1\\} &= \\frac{1}{2} = \\Pr\\{X_1 = 1 | X_0 = 1\\} \\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0 \\ \\text{or} \\  1 \\} &= 0\\\\\r\n\\end{align*}\r\nIf 2 tails appear in the previous toss, 0, 1 or 2 tail(s) will appear in this toss where the probabilities are 1/4, 1/2 and 1/4 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 2\\} &= \\frac{1}{4} = \\Pr\\{X_1 = 2 | X_0 = 2\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 2\\} &= \\frac{1}{2}\\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0, 1 \\ \\text{or} \\  2 \\} &= 0\\\\\r\n\\end{align*}\r\nIf 3 tails appear in the previous toss, 0, 1, 2 or 3 tail(s) will appear in this toss where the probabilities are 1/8, 3/8, 3/8 and 1/8 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 3\\} &= \\frac{1}{8} = \\Pr\\{X_1 = 3 | X_0 = 3\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 3\\} &= \\frac{3}{8} = \\Pr\\{X_1 = 2 | X_0 = 3\\}\\\\\r\n\\Pr\\{X_1 = 0 | X_0 \\neq 0, 1, 2 \\ \\text{or} \\  3 \\} &= 0\\\\\r\n\\end{align*}\r\nIf 4 tails appear in the previous toss, 0, 1, 2, 3, 4 tail(s) will appear in this toss where the probabilities are 1/16, 1/4, 3/8, 1/4 and 1/16 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 4\\} &= \\frac{1}{16} = \\Pr\\{X_1 = 4 | X_0 = 4\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 4\\} &= \\frac{1}{4} = \\Pr\\{X_1 = 3 | X_0 = 4\\}\\\\\r\n\\Pr\\{X_1 = 2 | X_0 = 4\\} &= \\frac{3}{8}\\\\\r\n\\Pr\\{X_1 = 0 | X_0 = 5\\} &= 0\\\\\r\n\\end{align*}\r\nIf 5 tails appear in the previous toss, 0, 1, 2, 3, 4 or 5 tail(s) will appear in this toss where the probabilities are 1/32, 5/32, 5/16, 5/16, 5/32 and 1/32 respectively:\r\n\\begin{align*}\r\n\\Pr\\{X_1 = 0 | X_0 = 5\\} &= \\frac{1}{32} = \\Pr\\{X_1 = 5 | X_0 = 5\\} \\\\\r\n\\Pr\\{X_1 = 1 | X_0 = 5\\} &= \\frac{5}{32} = \\Pr\\{X_1 = 4 | X_0 = 5\\}\\\\\r\n\\Pr\\{X_1 = 2 | X_0 = 5\\} &= \\frac{5}{16} = \\Pr\\{X_1 = 3 | X_0 = 5\\}\\\\\r\n\\end{align*}\r\nTherefore, the transition probability matrix is given as\r\n \\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{lllllll}\r\n\\hspace{0.125cm} &   \\ \\ \\ \\ 0 \\ & \\hspace{0.15cm} \\ \\ \\ 1 \\ \\  & \\hspace{0.125cm}  \\ \\ \\ 2 \\ \\ & \\hspace{0.125cm}  \\  \\ \\ 3 \\ \\ \\ \\ & \\hspace{0.125cm} \\  4 \\  \\ \\ \\ & \\hspace{0.125cm}  \\ 5 \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n4\\\\\r\n5\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n1  	 & 0	& 0 	& 0 	& 0 	& 0    \\\\ \r\n1/2  & 1/2  & 0 	& 0 	& 0 	& 0     \\\\ \r\n1/4  & 1/2  & 1/4 	& 0 	& 0 	& 0   \\\\ \r\n1/8  & 3/8 	& 3/8 	& 1/8 	& 0 	& 0  \\\\ \r\n1/16 & 1/4 	& 3/8 	& 1/4 	& 1/16 	& 0  \\\\ \r\n1/32 & 5/32 & 5/16 	& 5/16 	& 5/32 	& 1/32  \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\nLet\r\n\\[ \\ p_i = \\Pr\\{ \\text{reaching state 1 without passing state 0} \\ | \\ \\text{begining from state} \\ i \\}\\]\r\n\\[ \\therefore p_0 = 0, p_1 = 1 \\]\r\n\\begin{align}\r\np_2 &= \\frac{1}{4} p_0 + \\frac{1}{2} p_1 + \\frac{1}{4} p_2 = \\frac{1}{2} + \\frac{1}{4} p_2 \\\\\r\np_3 &= \\frac{3}{8} + \\frac{3}{8} p_2 + \\frac{1}{8} p_3 \\\\\r\np_4 &= \\frac{1}{4} + \\frac{3}{8} p_2 + \\frac{1}{4} p_3 + \\frac{1}{16} p_4 \\\\\r\np_5 &= \\frac{5}{32} + \\frac{5}{16} p_2 + \\frac{5}{16} p_3 + \\frac{5}{32} p_4 + \\frac{1}{32} p_5\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $p_2 = 2/3$, $p_3 = 5/7$, $p_4 = 76/105$ and $\\underline{p_5 = 157/217}$. The desired probability is\r\n\\begin{align*}\r\n\\Pr\\{X = 1\\} &=  p_5 \\\\\r\n&= 157/217 \\approx 0.7235\r\n\\end{align*}',2,3,3,2,0,0,NULL,'','','','',NULL,0.2,2),(88,'3.4.19',3,'$\\textit{Computer Challenge}$. Let $ N $ be a positive integer and let $ Z_1,...,Z_N $ be independent random variables, each having the geometric distribution\r\n \\[\\Pr\\{ Z = k \\} = \\left( \\frac{1}{2} \\right) ^k , \\text{for} \\ k=1,2,....\\]\r\n  Since these are discrete random variables, the maximum among them may be unique, or there may be ties for the maximum. Let $ p_N $ be the probability that the maximum is unique. How does $ p_N $ behave when $ N $ is large? (Alternative formulation: You toss $ N $ dimes. Those that are heads you set aside; those that are tails you toss again. You repeat this until all of the coins are heads. Then, $ p_N $ is the probability that the last toss was of a single coin.)','','','','','','','A. It will converge to 0.','B. It will converge to a certain number between 0 and 1.','C. It will converge to 1.','D. It will diverge.','E. It cannot be determined. It behaves like a random variable independent of $N$.','','','','','','','','B','theall/image/P3.4.19.png','','','This problem is an extension of Problem 3.4.10. In other words, the setup in Problem 3.4.10 is a special case of this problem with $ N $ = 5. The transition probability matrix is given as\r\n \\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{lllllll}\r\n\\hspace{0.1cm} & \\ \\ \\ \\ \\  0 \\ & \\hspace{0.15cm} \\  1 \\ \\  & \\hspace{0.125cm} \\   2 \\ \\  & \\hspace{0.125cm}  \\ 3 & \\hspace{0.2cm}  \\cdots   & \\hspace{0.175cm}  N \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n\\vdots\\\\\r\nN\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n1  	 & 0	& 0 	& 0 	& \\cdots 	& 0    \\\\ \r\n1/2  & 1/2  & 0 	& 0 	& \\cdots 	& 0     \\\\ \r\n1/4  & 1/2  & 1/4 	& 0 	& \\cdots 	& 0   \\\\ \r\n1/8  & 3/8 	& 3/8 	& 1/8 	& \\cdots 	& 0  \\\\ \r\n\\vdots & \\vdots 	& \\vdots 	& \\vdots 	& \\ddots 	& \\vdots  \\\\ \r\n\\frac{C^N_0}{2^N} & \\frac{C^N_1}{2^N} & \\frac{C^N_2}{2^N} 	& \\frac{C^N_3}{2^N} 	& \\cdots 	& \\frac{C^N_N}{2^N}  \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\n\\[\r\n\\text{where} \\ C^n_r = \r\n\\begin{pmatrix}\r\nn\\\\ \r\nr\\\\\r\n\\end{pmatrix} = \\frac{n ! }{(n - r)! \\ r!} \\\r\n\\text{is called the \\textit{combination} or \\textit{binomial coefficient}.}\r\n\\]\r\nLet \\[ \r\np_i = \\Pr\\{\\text{reaching state 1 without passing through state 0} \\ | \\ \\text{begining from state} \\ i \\}.\r\n\\]\r\nThen, \\[\r\np_0 = 0, p_1 = 1, \\]\r\n\\[\r\np_i = \\sum_{j = 0}^{N} \\textbf{P}_{ij} p_j \\ \\text{for} \\ i \\neq 0 \\ \\text{or} \\ 1 \r\n\\]\r\nA matrix equation can be set up:\r\n\\[ \\left[\r\n\\mathbf{I}_N - \\begin{pmatrix}\r\n0 & 0 & 0 & \\cdots & 0\\\\\r\n0 & 0 & 0 & \\cdots & 0\\\\\r\n1/4 & 1/2 & 1/4 & \\cdots & 0\\\\\r\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\r\n\\frac{C^N_0}{2^N} & \\frac{C^N_1}{2^N} & \\frac{C^N_2}{2^N} 	& \\frac{C^N_3}{2^N} 	& \\cdots 	& \\frac{C^N_N}{2^N}  \\\\\r\n\\end{pmatrix}\r\n\\right] \\begin{bmatrix}\r\np_0\\\\\r\np_1\\\\\r\np_2\\\\\r\n\\vdots\\\\\r\np_N\\\\\r\n\\end{bmatrix}\r\n= \\begin{bmatrix}\r\n1\\\\\r\n0\\\\\r\n0\\\\\r\n\\vdots\\\\\r\n0\\\\\r\n\\end{bmatrix}\r\n\\]\r\n\\[\r\n\\text{where} \\ I \\ \\text{is the $N$ by $N$ identity matrix.}\r\n\\]\\\\\r\nBy solving the above matrix equation, we have:\r\n\\[\r\np_5 = \\frac{157}{217} \\approx 0.7235, \\ p_{10} = \\frac{1436}{1991} \\approx 0.7212, \\ p_{20} = \\frac{999}{1385} \\approx 0.7213,\\]\r\n\\[\r\np_{50} = \\frac{1429}{1981} \\approx 0.7214, \\ p_{100} = \\frac{919}{1274} \\approx 0.7214, \\ p_{150} = \\frac{409}{567} \\approx 0.7213\\]\r\nTherefore, we can see that the probability $p_N$ is converging to approximately 0.7213 when $ N $ is large. <br>\r\nYou can also see how $p_N$ varies from $N = 1$ to $N = 150$ in the following figure.',3,3,2,2,0,0,NULL,'','','','',NULL,0.2,2),(89,'3.4.13',3,'A Markov chain $ X_0,X_1,X_2,... $ has the transition probability matrix \r\n  \\[\r\n \\mathbf{P} = \r\n \\begin{array}{l}\r\n \\begin{array}{llll}\r\n \\hspace{0.075cm} & \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\  1 \\ \\  & \\hspace{0.125cm}   2 \\ \\  \\\\\r\n \\end{array}\\\\\r\n \\begin{array}{c}\r\n 0\\\\\r\n 1\\\\\r\n 2\\\\\r\n \\end{array}\r\n \\begin{Vmatrix}\r\n 0.3 & 0.2 & 0.5   \\\\ \r\n 0.5 & 0.1 & 0.4  \\\\ \r\n 0 & 0 & 1  \\\\ \r\n \\end{Vmatrix}\r\n \\end{array}\r\n \\]\r\n and is known to start in state $ X_0 =0 $. Eventually, the process will end up in state 2. What is the probability that the time $ T = \\min \\{n \\geq 0; X_n = 2 \\} $ is an odd number?','','','','','','','A. 43/133','B. 53/133','C. 80/133','D. 90/133','E. 100/133','','','','','','','','A','','','','Let\\[\r\nZ_n = (OE(T),X_n), \\\\ \\] \r\n\\[\\text{where} \\ OE(T) = \\begin{cases}\r\nO & \\text{if} \\ \\ \\frac{1 + (-1)^T}{2} = 1\\\\\r\nE & \\text{if} \\ \\ \\frac{1 + (-1)^T}{2} = 0\\\\\r\n\\end{cases}\r\n \\ \\text{is an odd/even number indicator function.}\r\n\\]\r\nThen, $Z_n$ is a Markov chain with state $ \\{0, 1, 2, 3, 4, 5 \\} = \\{(O,0), (O,1), (O,2), (E,0), (E,1), (E,2) \\} $. The transition probability matrix now becomes:\r\n \\[\r\n\\mathbf{P} = \r\n\\begin{array}{l}\r\n\\begin{array}{lllllll}\r\n\\hspace{0.075cm} & \\ \\ \\ 0 \\ & \\hspace{0.125cm} \\  1 \\ \\  & \\hspace{0.125cm}   2 \\ \\  & \\hspace{0.125cm}  3 \\  & \\hspace{0.15cm}  4 \\  \\ \\ & \\hspace{0.1cm}  5 \\  \\ \\ \\ \\\\\r\n\\end{array}\\\\\r\n\\begin{array}{c}\r\n0\\\\\r\n1\\\\\r\n2\\\\\r\n3\\\\\r\n4\\\\\r\n5\\\\\r\n\\end{array}\r\n\\begin{Vmatrix}\r\n0  	 	& 0		& 0 	& 0.3 	& 0.2 	& 0.5    \\\\ \r\n0  	 	& 0		& 0 	& 0.5 	& 0.1 	& 0.4    \\\\  \r\n0  	 	& 0		& 0 	& 0 	& 0 	& 1    \\\\ \r\n0.3 	& 0.2 	& 0.5  	& 0 	& 0 	& 0    \\\\ \r\n0.5 	& 0.1 	& 0.4	& 0 	& 0 	& 0    \\\\ \r\n0 		& 0 	& 1 	& 0 	& 0 	& 0    \\\\ \r\n\\end{Vmatrix}\r\n\\end{array}\r\n\\]\r\nLet \\[\r\np_i = \\Pr\\{\\text{reaching state 2 without passing state 5} \\ | \\ \\text{begining from state} \\ i \\}.\r\n\\]\r\nThen, we have:\r\n\\[\r\np_2 = 1, p_5 = 0,\r\n\\]\r\n\\begin{align}\r\np_0 &= 0.3p_3 + 0.2p_4 + 0.5p_5\\\\\r\np_1 &= 0.5p_3 + 0.1p_4 + 0.4p_5\\\\\r\np_2 &= 1\\\\\r\np_3 &= 0.3p_0 + 0.2p_1 + 0.5p_2\\\\\r\np_4 &= 0.5p_0 + 0.1p_1 + 0.4p_2\\\\\r\np_5 &= 0\r\n\\end{align}\r\nBy solving the above linear system of equations, we have $\\underline{p_0 = 43/133}$, $p_1 = 53/133$, $p_3 = 90/133$ and $p_4 = 80/133$. The desired probability is\r\n\\[\r\np_0 = 43/133\r\n\\]',2,3,2,2,0,0,NULL,'','','','',NULL,0.2,2),(90,'3.3.1',2,'Consider a spare parts inventory model in which either 0,1, or 2 repair parts \r\nare demanded in any period, with: $Pr(\\xi_n=0)=0.4; Pr(\\xi_n=1)=0.3; Pr(\\xi_n=2)=0.3;$\r\nand suppose $s=0$ and $S=3$. Determine the transition probability matrix for\r\nthe Markov chain$\\{X_n\\}$, where $\\{X_n\\}$ is defined to be the quantity on hand at the\r\nend-of-period n.','','','','','','','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n.3 & .3  & .4 & 0& 0   \\cr\r\n0 & .3  & .3 & .4 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & .4  & .3 & .3 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n.3 & .3  & .4 & 0& 0   \\cr\r\n0 & .3  & .3 & .4 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n0 & 0  & .3 & .3 & .4    \\cr\r\n0 & .3  & .4 & .3& 0   \\cr\r\n0 & .3  & .3 & .4 & 0    \\cr\r\n0 & 0  & .3 & .3 & .4\r\n}\r\n\\end{eqnarray*}','','','','','','','','','','A','','','','Consider that the each element(e.g. the element$(i,j)$) in the transition matrix shows the probability of transferring to state $j$ in the next step given the current state $i$.\r\n<br>Look at the first row, if the inventory today is $-1$ or $0$, then we\'ll get supplement . At the beginning of tomorrow, the number of inventory will be 3. The number of inventory sold could be$ 0,1,2$. So the remaining can only be $1,2,3$. So we can get the first row as:$ 0,0,0.3,0.3,0.4$.\r\nThe analysis to other rows is similar.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of transition matrix.',NULL,0.33,1),(91,'3.3.2',2,'Consider two urns A and B containing a total of $N$ balls. An experiment is\r\nperformed in which a ball is selected at random (all selections equally likely) at\r\ntime $t(t=1,2,...)$ from among the totality of $N$ balls. Then, an urn is selected\r\nat random (A is chosen with probability $p$ and B is chosen with probability $q$)\r\nand the ball previously drawn is placed in this urn.The state of the system at\r\neach trial is represented by the number of balls in A. Determine the transition\r\nmatrix for this Markov chain.','','','','','','','$P_{i,i}=(\\frac{i}{N})p+(\\frac{N-i}{N})q$<br>\r\n$P_{i,i+1}=(\\frac{i}{N})q$<br>\r\n$P_{i,i-1}=(\\frac{N-i}{N})p$','$P_{i,i}=(\\frac{i}{N})p+(\\frac{N-i}{N})q$<br>\r\n$P_{i,i+1}=(\\frac{N-i}{N})p$<br>\r\n$P_{i,i-1}=(\\frac{i}{N})q$','$P_{i,i}=(\\frac{N-i}{N})p+(\\frac{i}{N})q$<br>\r\n$P_{i,i+1}=(\\frac{N-i}{N})p$<br>\r\n$P_{i,i-1}=(\\frac{i}{N})q$','','','','','','','','','','B','','','','Given the',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of transition matrix.',NULL,0.33,1),(92,'3.3.3',2,'Consider the inventory model of Section 3.3.1. Suppose that $S=3$. Set up\r\nthe corresponding transition probability matrix for the end-of-period inventory\r\nlevel $X_n$. (Similar to Exer3.1.1, given $Pr(n=0)=0.5; Pr(n=1)=0.4; Pr(n=2)=0.1;$)','','','','','','','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & .1  & .4 & .5& 0   \\cr\r\n0 & .1  & .4 & .5 & 0    \\cr\r\n0 & 0  & .1 & .4 & .5\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n.1 & .4  & .5 & 0& 0   \\cr\r\n0 & 0  & .4 & .5 & .1    \\cr\r\n0 & 0  & .1 & .4 & .5\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n0 & 0  & .1 & .4 & .5    \\cr\r\n.1 & .4  & .5 & 0& 0   \\cr\r\n0 & .1  & .4 & .5 & 0    \\cr\r\n0 & 0  & .1 & .4 & .5\r\n}\r\n\\end{eqnarray*}','','','','','','','','','','C','','','','The analysis is similar to Exer3.3.1, just calculate the probability of going to every state tomorrow given today\'s state.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of transition matrix.',NULL,0.33,1),(93,'3.3.4',2,'Consider the inventory model of Section 3.3.1. Suppose that $S=3$ and that the\r\nprobability distribution for demand is $Pr(\\xi_n=0)=0.1;Pr(\\xi_n=1)=0.4;Pr(\\xi_n=2)=0.3, and Pr(\\xi_n=3)=0.2. $<br>\r\nSet up the corresponding transition probability\r\nmatrix for the end-of-period inventory level $X_n$.','','','','','','','\\begin{eqnarray*}\r\n&& \\qquad  \\,-2  \\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-2} \\cr \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .2 & .3 & .4 &.1  \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n.2 & .3 & .4 &.1& 0 & 0   \\cr\r\n0 & .2 & .3 & .4 &.1& 0 \\cr\r\n0 & 0 & .2 & .3 & .4 &.1\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,-2  \\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-2} \\cr \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .2 & .3 & .4 &.1  \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n0 & .1  & .2 & .3 & .4 & 0   \\cr\r\n.2 & .3 & .4 &.1& 0 & 0   \\cr\r\n0 & .2 & .3 & .4 &.1& 0 \\cr\r\n0 & 0 & .2 & .3 & .4 &.1\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,-2  \\, -1 \\quad \\, 0 \\quad \\, 1\\quad\\,\r\n2\\quad \\, 3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{-2} \\cr \\hbox{-1}  \\cr \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0 & 0  & .2 & .3 & .4 &.1  \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n0 & 0  & .2 & .3 & .4 &.1   \\cr\r\n.2 & .3 & .4 &.1& 0 & 0   \\cr\r\n0 & .2 & .3 & .4 &.1& 0 \\cr\r\n0 & .1 & .2 & .3 & .4 &0\r\n}\r\n\\end{eqnarray*}','','','','','','','','','','A','','','','The analysis is similar to Exer3.1.1. The difference is that this problem introduces one more state. But all steps are the same.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of transition matrix.',NULL,0.33,1),(94,'3.3.5',2,'An urn initially contains a single red ball and a single green ball.A ball is\r\ndrawn at random, removed, and replaced by a ball of the opposite color, and\r\nthis process repeats so that there are always exactly two balls in the urn. Let $X_n$\r\nbe the number of red balls in the urn after n draws, with $X_0=1$. Specify the\r\ntransition probabilities for the Markov chain $\\{X_n\\}$.','','','','','','','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n0 & 1 & 0    \\cr\r\n.5 & .5 & 0 \\cr\r\n0 & 0 & 1\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n0 & 1 & 0    \\cr\r\n0 & .5 & .5 \\cr\r\n1 & 0 & 0\r\n}\r\n\\end{eqnarray*}','\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n0 & 1 & 0    \\cr\r\n.5 & 0 & .5 \\cr\r\n0 & 1 & 0\r\n}\r\n\\end{eqnarray*}','','','','','','','','','','C','','','','If the $X_n=2$, which means the number of red balls in the urn is 2. So next time one of the two red balls will be replaced by one green ball. Then the number of red balls next time must be 1.<br>\r\nIf the $X_n=1$, which means the number of red balls in the urn is 1. So next time the red one has 50% chance to be replaced, then $P(X_{n+1}=0)=0.5$. There is the other 50% chance that the green one will be replaced. So $P(X_{n+1}=2)=0.5$.<br>\r\nIf the $X_n=2$, which means the number of red balls in the urn is 0. So next time one of the two green balls will be replaced by one red ball. Then the number of red balls next time must be 1.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,0.33,1),(95,'3.4.1',2,'Find the mean time to reach state 3 starting from state 0 for the Markov chain\r\nwhose transition probability matrix is<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad  \\quad1\\quad \\quad\r\n2\\quad \\quad3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n0.4 & 0.3  &0.2 & 0.1  \\cr\r\n0 & 0.7  &0.2 & 0.1 \\cr\r\n0 & 0  & 0.9 & 0.1 \\cr\r\n0 & 0  & 0 & 1 \r\n}\r\n\\end{eqnarray*}','','','','','','','7','8','9','10','','','','','','','','','D','','','','We denote $W_0,W_1,W_2,W_3$ as the expected steps of going to $state 3$ from $state0,1,2,3$. Obviously,$W_3=0$(means you are already at $state 3$) <br>\r\nFrom $state0$ to $state3$, we have to go forward at least one step. If you are lucky enough, you can reach $state 3$ directly after the first step. This probability is $0.1$ from the transition matrix. Otherwise, you must reach one of the other three states.<br>\r\nIf you go to $state2$ for example, since you haven\'t arrived at $state3$, you have to go further. Now the problem becomes: starting from $state2$, going to $state3$. The remaining expected steps are just $W_2$. The past first step has nothing to with the following. The probability of this scenario is 0.2<br>\r\nFrom the analysis above, we can derive the formula of $W_0$. It can be separated to 4 parts(corresponding to 4 scenarios)<br>\r\n$W_0=1+0.4*W_0+0.3*W_1+0.2*W_2+0.1*W_3$<br>\r\nThis is the equation in terms of starting from $state0$. Similarly we have other 3 equations according to the First Step Analysis.<br>\r\n$W_1=1+0*W_0+0.7*W_1+0.2W_2+0.1W_3$<br>\r\n$W_2=1+0*W_0+0*W_1+0.9W_2+0.1W_3$<br>\r\n$W_3=0$<br>\r\nBy solving these equations, we can get $W_0=10$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of First Step Analysis.',NULL,0.25,1),(96,'3.4.2',2,'Consider the Markov chain whose transition probablity matrix is given by<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad 1\\quad \r\n2\\quad3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n1 & 0  &0  \\cr\r\n0.1& 0.6 &0.3  \\cr\r\n0 & 0  & 1\r\n}\r\n\\end{eqnarray*}\r\n<br>\r\n(a) Starting in state 1, determine the probability that the Markov chain ends in\r\nstate 0.<br>\r\n(b) Determine the mean time to absorption.','','','','','','','0.25;2.5','0.6;2.5','0.25;3','0.6;3','','','','','','','','','A','','','','(a).<br>\r\nWe use $P_0,P_1,P_2$ to denote the probabilities that the Markov chain starts from $state0,1,2$ and ends at $state 0$.<br>\r\nStarting from $state0$, it has 1 chance to ends at $state0$.<br>\r\n$P_0=1$<br>\r\nStarting from $state2$, it has 0 chance to ends at $state0$.<br>\r\n$P_2=0$<br>\r\nStarting from $state1$, it can goes to any of the three states after the first step. Each state has its probability of going to $state0$, so:<br>\r\n$P_1=0.1*P_0+0.6*P_1+0.3*P_2$ <br>\r\nBy solving these 3 equations, we get $P_1=0.25$\r\n<br>\r\n(b).<br>\r\nSimilar analysis.<br>\r\nAbsorption means stopping at one fixed state forever. Here we can easily find that if we reach $state0$ or $state2$, we can get out of that state any more.So the problem is the expected steps we need to reach $state0$ or $state2$. Suppose  $W_0,W_1,W_2$  represent the expectation steps of reaching $state0$ or $state2$ starting from $state0,1,2$ respectively. <br>\r\n$W_0=W_2=1$, because they have already at one of the two states.<br>\r\n$W_1=1+0.1*W_0+0.6*W_1+0.3*W_2$, because  from $state1$, you have to at least go one step further. Then you may reach any of the three states. You\'ll have new starting point. The remaining expected steps are associated with your starting point. Fortunately, they are just $W_0,W_1,W_2$.\r\nBy solving the 3 equations related to $W_0,W_1,W_2$, we can get $W_1=2.5$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of First Step Analysis.',NULL,0.25,1),(97,'3.4.3',2,'Consider the Markov chain whose transition probability matrix is given by<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad  \\quad1\\quad \\quad\r\n2\\quad \\quad3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n1 & 0  &0 & 0  \\cr\r\n0.1 & 0.6  &0.1 & 0.2 \\cr\r\n0.2 & 0.3  & 0.4 & 0.1 \\cr\r\n0 & 0  & 0 & 1 \r\n}\r\n\\end{eqnarray*}<br>\r\n(a) Starting in state 1, determine the probability that the Markov chain ends in\r\nstate 0.<br>\r\n(b) Determine the mean time to absorption.','','','','','','','$\\frac{1}{3};\\frac{7}{3}$','$\\frac{1}{3};\\frac{10}{3}$','$\\frac{8}{21};\\frac{10}{3}$','$\\frac{8}{21};\\frac{7}{3}$','','','','','','','','','C','','','','The method is similar to Exer3.4.2 .<br>\r\n(a).<br>\r\nUse $P_0,P_1,P_2,P_3$ as the probabilities of ending at $state0$, starting from $state0,1,2,3$ .<bt>\r\nBy applying the First Step Analysis, we get 4 equations: <br>\r\n$P_0=1$<br>\r\n$P_1=0.1*P_0+0.6*P_1+0.1*P_2+0.2*P_3$<br>\r\n$P_2=0.2*P_0+0.3*P_1+0.4*P_2+0.1*P_3$<br>\r\n$P_3=0$<br>\r\nBy solving these 4 equations, we get $P_1=\\frac{8}{21}$<br>\r\n(b).<br>\r\nUse $W_0,W_1,W_2,W_3$ as the expected steps of going to absorption, starting from $state0,1,2,3$ .<br>\r\n$W_0=0$<br>\r\n$W_1=1+0.1*W_0+0.6*W_1+0.1*W_2+0.2*W_3$<br>\r\n$W_2=1+0.2*W_0+0.3*W_1+0.4*W_2+0.1*W_3$<br>\r\n$W_3=0$<br>\r\nBy solving these 4 equations, we get $W_1=\\frac{10}{3}$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of First Step Analysis.',NULL,0.25,1),(98,'3.4.4',2,'A coin is tossed repeatedly until two successive heads appear. Find the mean\r\nnumber of tosses required.<br>\r\nHint: Let Xn be the cumulative number of successive heads. The state space is\r\n0;1;2, and the transition probability matrix is<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad 1\\quad \r\n2\\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} }\r\n\\pmatrix{\r\n\\frac{1}{2} & \\frac{1}{2}  &0  \\cr\r\n\\frac{1}{2}& 0 &\\frac{1}{2}  \\cr\r\n0 & 0  & 1\r\n}\r\n\\end{eqnarray*}<br>\r\nDetermine the mean time to reach state 2 starting from state 0 by invoking a first\r\nstep analysis','','','','','','','3','4','5','6','','','','','','','','','D','','','','Let $Y_n$ be $1$ $(0)$ if the $n$-th toss is a head (tail).<br>\r\nSet $Y_0=Y_{-1}=0$ for notational convenience.<br>\r\nDefine, for $n \\geq 0$,\r\n$$ X_n= \\cases{ 0  & if $Y_n=0$; \\cr\r\n1 & if $Y_n=1, Y_{n-1}=0$; \\cr\r\n2 & if $Y_n=1, Y_{n-1}=1$.\r\n}$$\r\nAssume when HH occurs $(X_n=2)$, the coin toss stops and we\r\nlet $X_k=2$ for all $k \\geq n$ (see the following Remark for the irrelevancy\r\nof this assumption).\r\nThen, $\\{X_n: n \\geq 0\\}$ is a {\\it MC} with transition probability matrix\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\,  0 \\quad \\, 1 \\quad \\,  2 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2}  }\r\n\\pmatrix{\r\n.5 & .5 & 0    \\cr\r\n.5 & 0 & .5 \\cr\r\n0 & 0 & 1\r\n}\r\n\\end{eqnarray*}<br>\r\nLet $w_i$ be the mean time to reach state 2, beginning from state $i$.<br>\r\nThen,\r\n$$ w_2=0, \\quad w_0 = 1+ .5 w_0 +.5 w_1 \\quad w_1= 1 + .5w_0.$$\r\nSolving the equations gives $w_0=6$ and $w_1=4$.\r\nThe mean number of tosses till the first two consecutive heads occur is\r\n$w_0={6}$.\r\n\r\n<br>\r\nRemark. In fact, you might find that,  $w_0$ and $w_1$ are irrelevant\r\nwith the last row of ${\\bf P}$. In other words, transition probabilities\r\nfrom 2 to 0, 1 or 2 does not matter for computing $w_0$ and $ w_1$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of First Step Analysis.',NULL,0.25,2),(99,'3.4.5',2,'A coin is tossed repeatedly until either two successive heads appear or two successive tails appear. Suppose the first coin toss results in a head. Find the probability that the game ends with two successive tails.','','','','','','','$\\frac{1}{2}$','$\\frac{1}{3}$','$\\frac{1}{4}$','','','','','','','','','','B','','','','Analysis is the very similar to Exer3.4.4 .<br>\r\nLet $X_n$ be a random variable representing the  four states:$TT,HT,TH,HH$ where $X_n=0,1,2,3$ respectively. For example, $X_1=1$ means the first 2 tosses is:$HT$. We still use the transition matrix:<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad 1\\quad \r\n2\\quad 3\\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr  \\hbox{3} }\r\n\\pmatrix{\r\n1 & 0  &0 &0 \\cr\r\n0.5& 0 &0.5&0 \\cr\r\n0& 0.5 &0&0.5 \\cr\r\n0& 0 &0&1\r\n}\r\n\\end{eqnarray*}<br>\r\nNow the problem is the probability of going to $state0$ from $state2or3$. Since $state3$ is an absorption state, so we only need to consider the probability of reaching $state0$ from $state2$.<br>\r\nFrom the First Step Analysis, we can derive 4 equations.\r\n$P_0=1$<br>\r\n$P_1=0.5*P_0+0.5*P_2$ <br>\r\n$P_2=0.5*P_1+0.5*P_3$<br>\r\n$P_3=0$<br>\r\nHere $P_i$ means the probability that the Markov chain is started at $state/quad i$and absorbed at $state0$ . By solving these 4 equations, we get $P_2=\\frac{1}{3}$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point of First Step Analysis.',NULL,0.33,2),(100,'3.4.6',2,'Consider the Markov chain whose transition probability matrix is given by<br>\r\n\\begin{eqnarray*}\r\n&& \\qquad  \\,\\, 0 \\quad  \\quad1\\quad \\quad\r\n2\\quad \\quad3 \\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0} \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3} }\r\n\\pmatrix{\r\n1 & 0  &0 & 0  \\cr\r\n0.1 & 0.4  &0.1 & 0.4 \\cr\r\n0.2 & 0.1  & 0.6 & 0.1 \\cr\r\n0 & 0  & 0 & 1 \r\n}\r\n\\end{eqnarray*}<br>\r\n(a) Starting in state 1, determine the probability that the Markov chain ends in\r\nstate 0.<br>\r\n(b) Determine the mean time to absorption.','','','','','','','$\\frac{6}{23};\\frac{50}{23}$','$\\frac{9}{23};\\frac{50}{23}$','$\\frac{9}{23};\\frac{35}{23}$','','','','','','','','','','A','','','','This problem is very close to Exer3.4.3<br>\r\n(a). Let $u_i$ be the probability of ending at state 0, beginning from state i.\r\nThen,<br>\r\n\\begin{eqnarray*}\r\nu_1 &=& P_{10}+ P_{11}u_1 + P_{12}u_2 +0 = .1 + .4 u_1 + .1 u_2 \\\\\r\nu_2 &=& P_{20} + P_{21}u_1 + P_{22}u_2 + 0 = .2 + .1 u_1 + .6u_2.\r\n\\end{eqnarray*}<br>\r\nThe solutions of the equations  are $u_1=6/23$ and $u_2=13/23$.\r\nThe probability to compute is $u_1={6/23}$.\r\n<br>\r\n(b). Let $w_i$ be the mean to absorption starting from state $i$.\r\nThen,<br>\r\n$$w_1= 1+.4 w_1+ .1 w_2 \\qquad w_2= 1 + .1 w_1 + .6 w_2,$$\r\nand the solutions are $w_1=50/23$ and $w_2=70/23$.<br>\r\nThe mean time to absorption starting from state 1 is ${50/23}$.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','Oh, you\'d better think this problem again.','Congratulations, you get the point o',NULL,0.33,2),(101,'4.1.1',3,'Five balls are distributed between two urns, labeled A and B. Each period, an urn is selected at random, and if it is not empty, a ball from that urn is removed and placed into the other run. In the long run, what fraction of time is urn A is empty?','','','','','','','1/10','1/5','1/14','Cannot be determined','None of above','','','','','','','','A','','','','First we define the random variable $X$ be the number of ball in urn A. It takes value from 0 to 5.<br>\r\nTo consider the transitional matrix, we know $P_{01}=P_{54}$ must be 1 as one urn is empty. For other possible transition, we know $P_{i(i+1)}=P_{i(i-1)}=1/2$. Therefore, \r\n\\[P=\r\n\\begin{array}{c|cccccc|}\r\n&0&1&2&3&4&5\\\\\r\n0&0&1&0&0&0&0\\\\\r\n1&1/2&0&1/2&0&0&0\\\\\r\n2&0&1/2&0&1/2&0&0\\\\\r\n3&0&0&1/2&0&1/2&0\\\\\r\n4&0&0&0&1/2&0&1/2\\\\\r\n5&0&0&0&0&1&0\r\n\\end{array}\\]\r\nThen we consider $\\pi_j=\\sum_kP_{kj}\\pi_k$\r\n\\begin{align*}\r\n\\pi_0&=\\pi_1/2\\\\\r\n\\pi_1&=\\pi_0+\\pi_2/2\\\\\r\n\\pi_2&=\\pi_1/2+\\pi_3/2\\\\\r\n\\pi_3&=\\pi_2/2+\\pi_4/2\\\\\r\n\\pi_4&=\\pi_3/2+\\pi_5\\\\\r\n\\pi_5&=\\pi_4/2\r\n\\end{align*}\r\nand\r\n\\begin{equation*}\r\n\\pi_1+\\pi_2+\\pi_3+\\pi_4+\\pi_5=1\r\n\\end{equation*}\r\nSolving the equations we have $\\pi_0=0.1$\r\nTherefore, 1/10 of the time urn A is empty.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','1. A clearer way to understand the relation $\\pi_j=\\sum_kP_{kj}\\pi_k$ is the new state $\\pi_j$ is equal to all possible transition from old states $\\pi_k$. <br>\r\n2. A quicker way to solve the system is by symmetry. We know $\\pi_0=\\pi_5$, $\\pi_1=\\pi_4$, $\\pi_2=\\pi_3$. Then by the second equation we have $\\pi_1=\\pi_2$. As $\\pi_0+\\pi_1+\\pi_2=0.5$, it is then obvious that $\\pi_0=0.1$, $\\pi_1=\\pi_2=0.2$','','',NULL,NULL,2),(102,'4.1.4',3,'A finite state regular Markov chain has transition probability matrix $\\mathbf{P}=|P_{ij}|$ and limiting distribution $\\pi=|\\pi_i|$. In the long run, what fraction of the transitions are from the prescribed state $k$ to a prescibed state $m$?','','','','','','','$\\pi_k$','$\\frac{\\pi_mP_{mk}}{\\pi_k}$','$\\frac{\\pi_kP_{km}}{\\pi_m}$','$\\pi_m$','$\\frac{P_{km}}{\\sum_iP_{im}}$','None of the above','','','','','','','C','','','','Mathematically, we are considering\r\n\\begin{equation*}\r\nP(X_{n+1}=m\\cap X_n=k|X_{n+1}=m)\r\n\\end{equation*}\r\nas\r\n\\begin{equation*}\r\nP(X_{n+1}=m\\cap X_n=k)=P(X_{n+1}=m|X_n=k)P(X_n=k)=P_{km}\\pi_k\r\n\\end{equation*}\r\nand\r\n\\begin{equation*}\r\nP(X_{n+1}=m)=\\pi_m\r\n\\end{equation*}\r\nso\r\n\\begin{equation*}\r\nP(X_{n+1}=m\\cap X_n=k|X_{n+1}=m)=\\frac{\\pi_kP_{km}}{\\pi_m}\r\n\\end{equation*}',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','You can prove your answer with another method, which is related to Prob4.1.13.\r\nConsider\r\n\\begin{align*}\r\nP(X_{n+1}=m\\cap X_n=k|X_{n+1}=m)&=P(X_{n+1}=m|X_n=k\\cap X_{n+1}=m)P(X_n=k|X_{n+1}=m)\\\\\r\n&=1\\times P(X_n=k|X_{n+1}=m)\r\n\\end{align*}\r\nNow you can match the answer with Prob 4.1.13.','','',NULL,NULL,3),(103,'4.1.7',3,'Determine the limiting distribution for the Markov chain whose transition probability matrix is\r\n\\[P=\r\n\\begin{array}{c|cccc|}\r\n&0&1&2&3\\\\\r\n0&\\frac{1}{2}&0&0&\\frac{1}{2}\\\\\r\n1&1&0&0&0\\\\\r\n2&0&\\frac{1}{2}&\\frac{1}{3}&\\frac{1}{6}\\\\\r\n3&0&0&1&0\r\n\\end{array}\r\n\\]','','','','','','','\\begin{align*}\r\n\\pi_0=1/11\\\\\r\n\\pi_1=2/11\\\\\r\n\\pi_2=4/11\\\\\r\n\\pi_3=4/11\r\n\\end{align*}','\\begin{align*}\r\n\\pi_0=2/11\\\\\r\n\\pi_1=1/11\\\\\r\n\\pi_2=4/11\\\\\r\n\\pi_3=4/11\r\n\\end{align*}','The Markov chain is not regular.','\\begin{align*}\r\n\\pi_0=2/11\\\\\r\n\\pi_1=2/11\\\\\r\n\\pi_2=4/11\\\\\r\n\\pi_3=3/11\r\n\\end{align*}','None of the above','','','','','','','','A','','','','Consider the relation $\\pi_j=\\sum_kP_{kj}\\pi_k$, one can read off the system of equations(skim through the matrix column by column ):\r\n\\begin{align*}\r\n\\pi_0&=\\pi_0/2+\\pi_1\\\\\r\n\\pi_1&=\\pi_2/2\\\\\r\n\\pi_2&=\\pi_2/3+\\pi_3/6\\\\\r\n\\pi_3&=\\pi_2\r\n\\end{align*}\r\nSolving the equations we have\r\n\\begin{equation*}\r\n\\pi_0:\\pi_1:\\pi_2:\\pi_3=1:2:4:4\r\n\\end{equation*}\r\nRemember the conservation of probability\r\n\\begin{equation*}\r\n\\pi_0+\\pi_1+\\pi_2+\\pi_3=1\r\n\\end{equation*}\r\nTherefore, $\\pi_0=1/11$, $\\pi_1=2/11$, $\\pi_2=\\pi_3=4/11$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(104,'4.1.10',3,'Consider a Markov chain with transition probability matrix\r\n\\[P=\r\n\\begin{array}{|ccccc|}\r\np_0&p_1&p_2&\\dots&p_N\\\\\r\np_N&p_0&p_1&\\dots&p_{N-1}\\\\\r\np_{N-1}&p_N&p_0&\\dots&p_{N-2}\\\\\r\n\\vdots&\\vdots&\\vdots&&\\vdots\\\\\r\np_1&p_2&p_3&\\dots&p_0\r\n\\end{array}\r\n\\]\r\nwhere $0<p_0<1$ and $p_0+p_1+\\dots+p_n=1$. Determine the limiting distribution.','','','','','','','/','','','','','','','','','','','','A','','','','/',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,4),(105,'4.1.13',3,'A Markov chain has the transition probability matrix\r\n\\[P=\r\n\\begin{array}{c|ccc|}\r\n&0&1&2\\\\\r\n0&0.4&0.4&0.2\\\\\r\n1&0.6&0.2&0.2\\\\\r\n2&0.4&0.2&0.4\r\n\\end{array}\r\n\\]\r\nAfter a long period of time, you observe the chain and see that it is in state 1. What is the conditional probability that the previous state was 2? That is, find\r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X_{n-1}=2|X_n=1\\}\r\n\\end{equation*}','','','','','','','0.2','12/70','6/7','1/4','None of the above','','','','','','','','B','','','','The transition matrix is about $P(X_{n}|X_{n-1})$, so the obvious choice is to apply Bayes\' theorem.\r\n\\begin{align*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X_{n-1}=2|X_n=1\\}&=\\lim_{n\\to\\infty}\\text{Pr}\\{X_n=1|X_{n-1}=2\\}\\frac{\\text{Pr}\\{X_{n-1}=2\\}}{\\text{Pr}\\{X_n=1\\}}\\\\\r\n&=P_{21}\\frac{\\pi_2}{\\pi_1}\r\n\\end{align*}\r\nSolving the long time behaviour, we have $\\pi_0=11/24$, $\\pi_1=7/24$, $\\pi_2=1/4$. Therefore, \r\n\\begin{equation*}\r\n\\lim_{n\\to\\infty}\\text{Pr}\\{X_{n-1}=2|X_n=1\\}=12/70\r\n\\end{equation*}',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(106,'4.2.3',3,'Suppose that a production process changes state according to a Markov process whose transition probability matrix is given by \r\n\\[P=\r\n\\begin{array}{c|cccc|}\r\n&0&1&2&3\\\\\r\n0&0.2&0.2&0.4&0.2\\\\\r\n1&0.5&0.2&0.2&0.1\\\\\r\n2&0.2&0.3&0.4&0.1\\\\\r\n3&0.1&0.2&0.4&0.3\r\n\\end{array}\r\n\\]\r\nSuppose that states 0 and 1 are \"in-control\", while states 2 and 3 are deemed \"out-of-control\", In the long run, what fraction of time is the process out-of-control?','','','','','','','11/20','26/51','14/51','The  MC is irregular','None of the above','','','','','','','','B','','','','So we want to calculate $\\pi_2+\\pi_3$\r\nThe system of equation can be written as \r\n\\begin{align*}\r\n\\pi_0&=\\pi_0/5+\\pi_1/2+\\pi_2/5+\\pi_3/10\\\\\r\n\\pi_1&=\\pi_0/5+\\pi_1/5+3\\pi_2/10+\\pi_3/5\\\\\r\n\\pi_2&=2\\pi_0/5+\\pi_1/5+2\\pi_2/5+2\\pi_3/5\\\\\r\n\\pi_3&=\\pi_0/5+\\pi_1/10+\\pi_2/10+3\\pi_3/10\r\n\\end{align*}\r\nwith \r\n\\begin{equation*}\r\n\\pi_0+\\pi_1+\\pi_2+\\pi_3=1\r\n\\end{equation*}\r\n$\\pi_0=13/51$? $\\pi_1=4/17$, $\\pi_2=6/17$, $\\pi_3=8/51$ and $\\pi_2+\\pi_3=26/51$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(107,'4.2.6',3,'Consider a computer system that fails on a given day with probability $p$ and remains \"up\" with probability $1-p$. Suppose the repair time is a random variable $N$ having the probability mass function $p(k)=\\beta(1-\\beta)^{k-1}$ for $k=1,2,...,$ where $0<\\beta<1$. Let $X_n=1$ if the computer is operating on day $n$ and $X_n=0$ if not. Determine the long run probability that the computer is operating in terms of $\\beta,p$.','','','','','','','$\\pi_0=\\pi_1=1/2$','$\\pi_0=p(1-\\beta) \\qquad \\pi_1=p\\beta$','$\\pi_0=p \\qquad \\pi_1=q$','$\\pi_0=\\frac{p}{\\beta+p} \\qquad \\pi_1=\\frac{\\beta}{\\beta+p}$','None of the above','','','','','','','','D','','','','Observe that the repair day follow a geometric distribution, it is clearly memoryless. We can conclude that when the computer is down, it has $\\beta$ chance getting repaired and $\\alpha=1-\\beta$ chance not getting repaired. When the computer is up, it has $q$ chance remain up, and $p$ chance being down. The transition matrix is then\r\n\\[\r\n\\begin{array}{c|cc|}\r\n&0&1\\\\\r\n0&1-\\beta&\\beta\\\\\r\n1&p&1-p\r\n\\end{array}\r\n\\]\r\nThe long run probability then have the relation $\\beta\\pi_0=p\\pi_1$ and $\\pi_0+\\pi_1=1$\r\n\\begin{equation*}\r\n\\pi_0=\\frac{p}{\\beta+p} \\qquad \\pi_1=\\frac{\\beta}{\\beta+p}\r\n\\end{equation*}',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(108,'4.1.5',1,'English premiere league football team Chelsea  recently fired the coach\r\n  Scolari  and hired Mr. N. (not Hiddink), who, known for his firmness with rules,\r\n  declares to strictly employ a naive\r\n  tactic on using for center forward\r\n  one of the two equally good strikers: Didier Drogba or Nicholas Anelka.\r\n  On every match, the player (D or A) has chance $1-p$ to  be performing (up to\r\n  Mr. N.\'s standard), in which case\r\n  he continues to play for the next match, and chance $p$ to be non-performing, in\r\nwhich case, he is suspended   for the next two matches (as a kind of penalty or distrust)\r\n  and afterwards waits for his turn to play when the other  is suspended.\r\n  Mr. N. lets Drogba play the first match.\r\n  Let $X_n$ be the number of ``ready\" players at match $n$, playing\r\n  or in bench\r\n  $(X_1=2)$.\r\n  Let $Y_n$ be the number of players that is suspended in\r\n   match $n$\r\n  but would be ``ready\" for the $n+1$-th match $(Y_1=0)$.\r\n  To illustrate the situation, consider the following\r\n  possible path:\r\n\r\nRight now Mr. N. worries about the chance of\r\nhis nightmare match: both Drogba and Anelka are suspended.','theall/image/example4-1-5.PNG','','','','','','no','','','','','','','','','','','','A','','','','Note first that $\\{X_n\\}$ is NOT a {MC}, since, for example,\r\n$$P(X_{n+1}=1|X_n=1, X_{n-1}=2) = 1-p \\quad \\hbox{but}\r\n\\quad P(X_{n+1}=1|X_n=1, X_{n-1}=1, X_{n-2}=2)=p.$$\r\n  But   $\\{(X_n, Y_n): n \\geq 0\\}$ is a {\\it MC} with\r\n  state space of $(X_n, Y_n)$\r\n  is $\\{(2, 0), (1,1), (1,0), (0, 1)\\}$ and\r\n  transition probability matrix:\r\n  \\begin{eqnarray*}\r\n && \\qquad \\qquad \\,\\, ({\\rm 2, 0})\\,\\,\\,\\,\\,\\, \\,\\,({\\rm 1, 0})\r\n \\,\\,\\,\\,\\,\\,\\,\r\n({\\rm 1, 1})\\,\\,\\,\\,\\,\\, ({\\rm 0, 1}) \\\\\r\n  P &=&  \\matrix{\r\n  ({\\rm 2, 0}) \\cr ({\\rm 1, 0}) \\cr\r\n({\\rm 1, 1}) \\cr ({\\rm 0, 1}) }\r\n\\pmatrix{\r\n\\,\\,\\,\\, 1-p \\,\\,\\, & \\,\\,\\,\\, p \\,\\,\\,\\,\\,\r\n& \\,\\,\\,\\,\\,\\,\\,     &\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  \\cr\r\n & & 1-p & \\,\\,\\, p \\,\\,\\cr\r\n1-p & p & & \\cr\r\n&  &   1 & \\,\\,\\, \\,\\, }\r\n\\end{eqnarray*}\r\nLet 0, 1, 2 and 3 denote ({2, 0}), ({1, 0}),\r\n({1, 1}) and ({0, 1}), respectively.\r\nEquations (4.2)-(4.3) becomes\r\n$$\\cases{(\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )\r\n= (\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )P & \\cr\r\n\\pi_0 + \\pi_1 +\\pi_2 + \\pi_3 = 1 }\r\n$$\r\nSolving the equations gives\r\n$$\\pi_0 = {1-p  \\over 1+p+p^2}, \\quad \\pi_1=\\pi_2= {p \\over 1+p+p^2},\r\n  \\quad\\hbox{and}\\quad\r\n \\pi_3={p^2 \\over 1+p+p^2}.$$\r\nThe long run fraction of the Chelsea nightmare matches\r\nis\r\n$$ \\lim_{n \\to \\infty} P(X_n  =0 )\r\n= \\lim_{n\\to \\infty} P((X_n, Y_n) =(0, 1))\r\n =\\pi_3 = p^2/(1+p+p^2)\r\n$$\r\nIf Mr. N. has the mind of\r\ntrusting whoever employed and un-employing whoever distrusted,\r\n(Yong Ren Bu Yi, Yi Ren Bu Yong--a old Chinese saying)  he sets $p$ low.\r\nThe fraction of nightmare match is at the order\r\nof $p^2$ rather than $p$.\r\nFor example, if $p=1/10$, meaning that a player is nonperforming\r\nin one match is 1 out of 10.\r\nHaving two players, the nightmare matches would be only\r\n1 out of 111, which is relatively small.\r\nIf, instead, Mr. N.   is too harsh and demanding,\r\n  he could set $p$ as 0.4 for example.\r\nThen, his chance of nightmare match is a little over\r\n1 out of 10.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,5),(109,'4.1.6',1,'({ Stock Price Movement})<br>\r\nSuppose, on each trading day, the HSBC stock is either\r\nup (U) or down (D). And today\'s up or down depends on the\r\nhistorical movement only through   that of the\r\nprevious two days in the following\r\nway:(figure)<br>\r\n  What\'s the long run fraction of days the stock price is up?','theall/image/example4-1-6.PNG','','','','','','','','','','','','','','','','','','A','','','','At first glance, it appears that both down and up trends tend to persist, with\r\n  the downs only slightly more so. One might expect that a little over\r\n  50% of the days would be down. It turns out the down days are nearly\r\n  twice as often as up days! Surprising, isn\'t it?\r\n\r\n  Let $Y_n={\\rm U}$ of D if the stock price on day $n$ is\r\n  up or down, respectively. Then, $\\{Y_n\\}$ is NOT a {$MC$}, because\r\n  $$P(Y_{n+2}={U} | Y_{n+1} = {U}, Y_{n}={U}) = 0.8 \\not=\r\n  0.6= P(Y_{n+2}={U} | Y_{n+1} = {\\rm U}, Y_{n}={\\rm D}). $$\r\n  As a result, the limit theorem of regular {$MC$} does not\r\n  apply to $\\{Y_n\\}$. Looks like a dead end.\r\n\r\n% shan chong shui fu yi wu lu,\r\n% Liu An hua ming yiu yi chun. ---Lu Yiu.\r\n\r\nRecall the tricks of constructing {MC} using\r\npattern of length 2 or 3 in Examples  and exercises\r\nabout coin tossing. The idea applies here.\r\nSet\r\n$ X_n= (Y_{n-1 }, Y_{n})$. Then $\\{X_n\\}$ is a {MC}\r\nwith state space $\\{({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}),   ({\\rm D, D})\\}$ and transition probability\r\nmatrix\r\n\\begin{eqnarray*}\r\n && \\qquad \\qquad ({\\rm U, U})\\,\\,\\,\\, ({\\rm D, U})\\,\\,\\,\r\n({\\rm U, D})\\,\\, ({\\rm D, D}) \\\\\r\n  P &=&  \\matrix{\r\n  ({\\rm U, U}) \\cr ({\\rm D, U}) \\cr\r\n({\\rm U, D}) \\cr ({\\rm D, D}) }\r\n\\pmatrix{\r\n\\,\\,\\,\\, 0.8 \\,\\,\\, & \\,\\,\\,\\,   \\,\\,\\,\\,\\,\r\n& \\,\\,\\,\\,0.2 \\,\\,\\,     &\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  \\cr\r\n0.6 &  & 0.4& \\cr\r\n & 0.4&  & \\,\\,\\, 0.6 \\,\\,\\cr\r\n& 0.1 &   & \\,\\,\\,0.9\\,\\, }\r\n\\end{eqnarray*}\r\nLet 0, 1, 2 and 3 denote ({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}) and ({\\rm D, D}), respectively.\r\nEquations (4.2)-(4.3) becomes\r\n$$\\cases{(\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )\r\n= (\\pi_0 \\,\\, \\pi_1 \\,\\, \\pi_2 \\,\\, \\pi_3 )P & \\cr\r\n\\pi_0 + \\pi_1 +\\pi_2 + \\pi_3 = 1 }\r\n$$\r\nSolving the equations gives\r\n$$\\pi_0 = 3/11, \\qquad \\pi_1=\\pi_2=1/11 \\qquad \\pi_3=6/11.$$\r\nThe long run fraction of   the up days\r\nis\r\n\\begin{eqnarray*}\r\n&& \\lim_{n \\to \\infty} P(Y_n={\\rm U})\r\n= \\lim_{n\\to \\infty} [P(Y_n={\\rm U}, Y_{n+1}={\\rm U}) +\r\nP(Y_n={\\rm U}, Y_{n+1}={\\rm D})]\\\\\r\n&=& \\lim_{n\\to \\infty} [P(X_{n+1}=({\\rm U, U}) )+ P(X_{n+1}=({\\rm U, D}))]\r\n= \\lim_{n\\to \\infty} [P(X_{n+1}=0) + P(X_{n+1}=2)]\\\\\r\n&=& \\pi_0 + \\pi_2\r\n= 1/11 + 3/11 = 4/11.\r\n\\end{eqnarray*}\r\nOn average and over a long time, four out of eleven trading days\r\nthe HSBC stock is up and seven out of eleven trading days it\'s down.\r\n $\\square$\r\n\r\n\r\nThe so-called age replacement policies in the following example\r\n are very common in industry.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,5),(110,'4.1.7',1,'{\\bf Example 4.7} \\ {\\textcolor[rgb]{1, .5,0} {$\\star\\star\\star\\star$}}\r\n \\ ({\\sc Age replacement policies}) \\\r\n To ensure flight safety,\r\nOasis Hong Kong Airline  has a policy to retire aging Boeing 777 with new replacement: as long as\r\nthe airplane fails a regular year-end   examination or, even\r\nif it passes the examination, it has served for $N$ years.\r\nShould there be a replacement, it always takes place in New Year\'s eve.\r\nSuppose the life time (till failure of examination)\r\nof a Boeing 777 is $Y$ . Let\r\n$a_j = P(Y=j), j=1,2,...$, and\r\n$$p_k=P(Y=k+1|Y>k)={a_{k+1}\\over a_{k+1}+a_{k+2} + \\cdots},\r\n\\quad k=0, 1, 2, ...$$\r\nThen, $p_k$ is the chance the airplane fails next year\'s test\r\nafter it has serviced $k$ years. What\'s the long run\r\nfraction of replacements?','theall/image/example4-1-7.PNG','','','','','','no','','','','','','','','','','','','A','','','','To ensure flight safety,\r\nOasis Hong Kong Airline  has a policy to retire aging Boeing 777 with new replacement: as long as\r\nthe airplane fails a regular year-end   examination or, even\r\nif it passes the examination, it has served for $N$ years.\r\nShould there be a replacement, it always takes place in New Year\'s eve.\r\nSuppose the life time (till failure of examination)\r\nof a Boeing 777 is $Y$ . Let\r\n$a_j = P(Y=j), j=1,2,...$, and\r\n$$p_k=P(Y=k+1|Y>k)={a_{k+1}\\over a_{k+1}+a_{k+2} + \\cdots},\r\n\\quad k=0, 1, 2, ...$$\r\nThen, $p_k$ is the chance the airplane fails next year\'s test\r\nafter it has serviced $k$ years. What\'s the long run\r\nfraction of replacements?\r\n\r\nDefine $X_n$ as  the number of years of service of\r\n the airplane servicing on Jan 1 of year $n$.  ($X_0=0$.)\r\n $\\{X_n\\}$ is a {\\it MC} with state space $\\{0, 1, ..., N-1\\}$\r\nand transition probability matrix\r\n\\begin{eqnarray*}\r\n&&  \\qquad \\quad \\qquad 0 \\qquad \\quad 1 \\qquad \\quad 2 \\qquad \\quad \\,\\, 3\r\n\\qquad \\, \\cdots \\quad  N-1 \\\\\r\nP &=& \\matrix{0 \\cr 1 \\cr 2 \\cr 3 \\cr \\vdots \\cr N-2 \\cr N-1}\r\n\\pmatrix{ p_0 & 1-p_0 & 0 &  0 & \\cdots & 0 \\cr\r\n          p_1 & 0     &1-p_1& 0& \\cdots & 0 \\cr\r\n          p_2 & 0     &0    &1-p_2& \\cdots & 0 \\cr\r\n          p_3 & 0     &0    & 0 & \\cdots & 0 \\cr\r\n          \\vdots&\\vdots & \\vdots &\\vdots & \\vdots & \\vdots \\cr\r\n          p_{N-2} & 0& 0& 0& \\cdots & 1-p_{N-2} \\cr\r\n          1 & 0 & 0 & 0 & \\cdots & 0 }.\r\n          \\end{eqnarray*}\r\nFor illustration, consider a typical path with $N=3$ as follows.\r\n(figure)\r\nFor example, for $k\\leq  N-2$,\r\n\\begin{eqnarray*}\r\nP_{k,0}&=& P(X_{n+1}=0|X_n=k) \\\\\r\n&=&  P(\\hbox{the plane fails\r\nthe  year-end examination at year $n$} \\, \\\\\r\n&& \\quad\r\n| \\,\\, \\hbox{the plane in service on Jan 1st of year $n$ has served $k$ years}) \\\\\r\n&=& p_k, \\\\\r\nP_{k, k+1} &=& P(X_{n+1}=k+1|X_n=k) = 1-p_k;\r\n\\end{eqnarray*}\r\n and $P_{N-1, 0} = P(X_{n+1}=0|X_n=N-1)=1.$\r\nThe equation (4.2)-(4.3) can be rewritten as\r\n$$\\cases{ \\pi_0=p_0\\pi_0 +p_1 \\pi_1 + \\cdots +p_{N-2}\r\n\\pi_{N-2} +\\pi_{N-1} & \\cr\r\n\\pi_1=(1-p_0)\\pi_0 & \\cr\r\n\\pi_2=(1-p_1)\\pi_1 & \\cr\r\n\\vdots & \\cr\r\n\\pi_{N-1}=(1-p_{N-2})\\pi_{N-2} & \\cr\r\n\\pi_0 + \\cdots + \\pi_{N-1} = 1 & \\cr\r\n}\r\n$$\r\nIt follows that $\\pi_k = (1-p_{k-1})\\cdots (1-p_0)\\pi_0$\r\nfor $1 \\leq k \\leq N-1$.\r\nHence,\r\n$$\\pi_0 \\{ 1 + (1-p_0) + (1-p_0)(1-p_1) + \\cdots\r\n+ \\prod_{i=0}^{N-2} (1-p_i) \\} = 1.$$\r\nAnd, finally, we have\r\n\\begin{eqnarray*}\r\n\\pi_0 &=& { 1\\over 1 + (1-p_0) + (1-p_0)(1-p_1) + \\cdots\r\n+ \\prod_{i=0}^{N-2} (1-p_i)  } \\\\\r\n&=&{1 \\over P(Y> 0) +P(Y>1)+ \\cdots + P(Y > N-1)} \\\\\r\n&=&{1 \\over E[\\min(Y, N) ]}.  \\qquad \\hbox{(See {\\it Exercise 4.2})}\r\n\\end{eqnarray*}\r\n$\\pi_0$ is the long run fraction of\r\nreplacements. In other words, by our\r\n{\\it time average} interpretation,\r\n$\\pi_0 \\approx   R_n/n$ for a large $n$,\r\nwhere $R_n$ is number of replacements over\r\n$n$ years. As a result,\r\n$$ R_n \\times E[\\min(Y, N) ] \\approx n$$\r\nSince $E[\\min(Y, N)]$ is the mean length of\r\nservice of an airplane, the above approximation\r\nindeed makes sense.\r\n $\\square$\r\n\r\nRemark. Suppose $\\xi_i$ is the time of service of the $i$-th Boeing 777.\r\nThen $\\xi_1, \\xi_2, ...$ are iid positive r.v.s following the common distribution\r\nas that of $\\min(Y, N)$.  This is the key element in the definition\r\nof a renewal process   $R(t)$,   the number of\r\nreplacements by time $t$, to be specified  in Chapter 7.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,5),(111,'4.1.8',1,'Consider transition probability matrix:\r\n\\begin{eqnarray*}\r\n&&\\qquad \\,\\,\\, 0 \\qquad 1 \\qquad 2 \\qquad 3 \\\\\r\nP &=& \\matrix{0 \\cr 1 \\cr 2 \\cr 3} \\pmatrix{\r\n1/2 & 1/2 & 0 & 0 \\cr\r\n1/2 & 0 & 1/2 & 0 \\cr\r\n0 & 0 & 1/4 & 3/4 \\cr\r\n0 & 0 & 1/3 & 2/3\r\n}\r\n\\end{eqnarray*}\r\n\r\nState $j$ is called {accessible} from state\r\n$i$, denoted as $i \\to j$,\r\n if $P_{ij}^{(k)} > 0$ for some $k > 0$.\r\nIn other words, from $i$ there is a positive chance\r\nthe $MC$ can reach state $j$. In the above\r\nexample, $0 \\to 1$, $0 \\to 2$, but $2 \\nrightarrow 0$.\r\n\r\nState $i$ {\\it communicates } with state $j$, denoted\r\nas $i \\leftrightarrow j $, if\r\n$i \\to j$ and $j \\to i$. In the above example,\r\n$0 \\leftrightarrow 1$, $2 \\leftrightarrow 3$, but\r\n$0 \\nleftrightarrow 2$.\r\n\r\nIt is easily seen that communicativeness has properties:\r\n(a). reflexivity: $i \\leftrightarrow i$; (b). symmetry:\r\n$i \\leftrightarrow j$ is the same as $j \\leftrightarrow i$;\r\n(c). transitivity: if $i \\leftrightarrow j$ and\r\n$j \\leftrightarrow k$, then $i \\leftrightarrow k$.\r\nA relation satisfies the above three properties\r\nusually defines classes of certain equivalency.\r\nAll states of a {\\MC} can be partitioned into\r\na number of classes such that states within a class\r\nall communicate with each other and states belong to\r\ndifferent classes do not.','','','','','','','','','','','','','','','','','','','A','','','','A {MC}  is called { irreducible}\r\n if all states communicate with each\r\nother. The {\\it MC} with transition probability matrix\r\nin Example 4.8 is  not irreducible. A regular {MC}\r\nis irreducible, but an irreducible {MC} is not necessarily\r\nregular.\r\n\r\nState $i$ is {\\periodic} if, for some $k>1$, $P_{ii}^{(n)}=0$\r\nfor {\\all} $n$ that are not multiples $k$ and $P_{ii}^{(n)}>0$\r\nfor {some } $n$ as multiples of $k$.\r\n  In other words,\r\n$P_{ii}^{(km+1)}=P_{ii}^{(km+2)}=\\cdots=P_{ii}^{(km+k-1)}=0$\r\nfor all $m\\geq 0$ and $P_{ii}^{(kj)}>0$ for some $j \\geq 1$.\r\n\r\nThe {period} of state $i$, denoted as $d(i)$, is\r\nthe greatest common divisor (gcd) of all $n\\geq 1$\r\nsuch that $P_{ii}^{(n)}>0$, i.e.,\r\n$$d(i) = \\hbox{gcd}\\{n \\geq 1: P_{ii}^{(n)} > 0 \\}.$$\r\nIf $d(i)\\geq 2$, we say $i$ is periodic with period $d(i)$;\r\nif $d(i)=1$, we say state $i$ is {\\it aperiodic}. We\r\nset $d(i)=0$ if $P_{ii}^{(n)}=0$ for all $n\\geq 1$.\r\nStarting from state $i$, only when the $MC$ takes\r\nsome multiples of $d(i)$ steps, would $MC$ have a\r\npositive chance to return to state $i$.\r\n\r\nConsider a $MC$ with states 0, 1, 2, 3 and\r\none step transition probabilities $P_{01}=P_{12}=P_{23}=P_{30}=1$.\r\nThen, all states are periodic with period 4.\r\nSuppose the one-step\r\ntransition probabilities are such\r\nthat $P_{01}=P_{12}=P_{23}=1$, $P_{30}=1/3$ and $P_{32}=2/3$.\r\nThen, all states are periodic with period 2.\r\nIn Example 4.8, all states are aperiodic.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(112,'4.2.2',2,'In the reliability example of Textbook Section 4.2.2(see figures), what fraction of time is the repair\r\nfacility idle? When a second repair facility is added, what fraction of time is\r\neach facility idle?','theall/image/exercise4-2-2-1or4.PNG','theall/image/exercise4-2-2-2.PNG','theall/image/exercise4-2-2-3.PNG','theall/image/exercise4-2-2-4.PNG','','','1;1','$\\frac{q^{2}}{1+p^{2}}$;$\\frac{q+p}{1+p+p^{2}}$','none of above is correct','','','','','','','','','','B','','','','By understanding the problem,we get that the fraction of the the repair\r\nfacility idling equals to $\\pi_{0}$=$\\frac{q^{2}}{1+p^{2}}$(see the textbook p181)<br>\r\nOn the other hand,we get when the another repair facility is added, fraction of each  the repair\r\nfacility idling is equal to  $\\pi_{0}^{,}$+0.5* $\\pi_{1}^{,}$+ 0.5*$\\pi_{2}^{,}$=$\\frac{q+p}{1+p+p^{2}}$.(distrubution computing see the textbook p181)<br>',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(113,'4.2.3',2,'Determine the average fraction inspected, AFI, and the average outgoing quality,\r\nAOQ, of Section 4.2.3(attached figure) for p=0.5 when r=10 and i=5?','theall/image/exercise4-2-3-1.PNG','theall/image/exercise4-2-3-2.PNG','theall/image/exercise4-2-3-3.PNG','theall/image/exercise4-2-3-4.PNG','','','0.47;0.3','$\\frac{32}{41}$;$\\frac{9}{82}$','none of above is correct','','','','','','','','','','B','','','','After understanding the problem,we use the formula in the attached figure<br>\r\nAFI=$\\frac{1}{1+(r-1)(1-p)^i}$=$\\frac{32}{41}$<br>\r\nAOQ=(1-AFI)$*$p=$\\frac{9}{82}$<br>',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(114,'4.2.4',2,'Section 4.2.2 already determined the availability R of a certain computer system.R1 for one repair facility and<br>\r\nR2 for two repair facilities.\r\n p is the computer failure probability on a single day. Compute and compare\r\nR1 and R2 for p=0.3.','theall/image/exercise4-2-2-1or4_654n1aX.PNG','theall/image/exercise4-2-2-2_IvEYHBK.PNG','theall/image/exercise4-2-2-3_KSIVAJ7.PNG','theall/image/exercise4-2-2-4_1cFyAT3.PNG','','','1;1','0.02;0.4','none of above is correct','','','','','','','','','','C','','','','Directly use the formula on textbook\r\n  R1=$\\frac{1}{1+p^{2}}$=$\\frac{100}{109}$<br>\r\n  R2=$\\frac{1+p}{1+p+p^{2}}$=$\\frac{130}{139}$<br>',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(115,'4.2.5',2,'From purchase to purchase, a particular customer switches brands among products\r\nA,B and C according to a Markov chain whose transition probability\r\nmatrix is\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,A\\quad\\quad B\\quad\\quad C\\\\\r\n {\\bf P}&=& \\matrix{A \\cr B \\cr C }\r\n  \\pmatrix{\r\n&  0.6& 0.2   &0.2 \\cr\r\n \r\n &    0.1& 0.7  &0.2    \\cr\r\n  &  0.1   & 0.1      & 0.8\r\n}\r\n\\end{eqnarray*}\r\nIn the long run, what fraction of time does this customer purchase brand A?','','','','','','','0.3','0.2','none of above is correct','','','','','','','','','','B','','','','As the matrix is regular,we directly compute the liming distrubution by solving<br> \r\n                       $ P^{T}$$*$$\\pi$=$\\pi$<br>\r\n                        $\\sum$$\\pi_{i}$=1<br>\r\n  get $\\pi_{A}$=0.2',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(116,'4.2.6',2,'A component of a computer has an active life, measured in discrete units, that\r\nis a random variable T where<br>\r\n$Pr(T=1)$=$0.1$,$Pr(T=2)$=$0.2$,<br>\r\n$Pr(T=3)$=$0.3$,$Pr(T=4)$=$0.4$,<br>\r\nSuppose one starts with a fresh component, and each component is replaced by\r\na new component upon failure. Determine the long run probability that a failure\r\noccurs in a given period.','','','','','','','$\\frac{5}{17}$','0.2','none of above is correct','','','','','','','','','','A','','','','the time before the failure we denote as 0,1,2,3(0 means the error already happened)<br>\r\nthen we get the probability transition matrix<br>\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,\\,\\,0\\quad\\quad 1\\quad\\quad 2\\quad\\quad 3\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr1  \\cr 2\\cr3}\r\n  \\pmatrix{\r\n&  0.1& 0.2   &0.3 & 0.4\\cr\r\n  &   1& 0  &0&0    \\cr\r\n &    0& 1  &0 &0    \\cr\r\n  &  0  & 0&1      & 0\r\n}\r\n\\end{eqnarray*}<br>\r\nthe matrix is regular we directly compute the limiting distrubution by solving<br> \r\n                       $ P^{T}$$*$$\\pi$=$\\pi$<br>\r\n                        $\\sum$$\\pi_{i}$=1<br>get \r\n$\\pi_{0}$=$\\frac{5}{17}$,that means error happens at rate  of $\\pi_{0}$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(117,'4.2.7',2,'Consider a machine whose condition at any time can be observed and classified\r\nas being in one of the following three states:<br>\r\nState 1: Good operating order<br>\r\nState 2: Deteriorated operating order<br>\r\nState 3: In repair<br>\r\nWe observe the condition of the machine at the end of each period in a sequence\r\nof periods. Let Xn denote the condition of the machine at the end of period\r\nn for n =1,2..... Let X0 be the condition of the machine at the start. We\r\nassume that the sequence of machine conditions is a Markov chain with transition\r\nprobabilities(see figure)<br>\r\nand that the process starts in state X0=1.\r\nFind $Pr(X_{4}=1)$and the long run rate of repairs per unit time.','theall/image/exercise4-2-7.PNG','','','','','','1?1','0.2?0.3','none of above is correct','','','','','','','','','','C','','','','directly computing the $P^{4}_{11}$,we get $Pr(X_{4}=1)$=$\\frac{3367}{4929}$;<br>\r\nsolve the equations,we get the liming distrubution $\\pi$,and get $\\pi_{3}$=$\\frac{1}{31}$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(118,'4.2.8',2,'At the end of a month, a large retail store classifies each receivable account\r\naccording to\r\n0: Current  <br>\r\n1: 3060 days overdue<br>\r\n2: 6090 days overdue<br>\r\n3: Over 90 days<br>\r\nEach such account moves from state to state according to a Markov chain with\r\ntransition probability matrix\r\n\\begin{eqnarray*}\r\n && \\quad \\quad \\,\\,\\,\\,\\,\\,\\,\\,0\\quad\\quad 1\\quad\\quad 2\\quad\\quad 3\\\\\r\n {\\bf P}&=& \\matrix{0 \\cr1  \\cr 2\\cr3}\r\n  \\pmatrix{\r\n&  0.95& 0.05   &0.3 & 0.4\\cr\r\n  &   0.5& 0  &0.5&0    \\cr\r\n &    0.2& 0  &0 &0.8    \\cr\r\n  &  0.1  & 0&0     & 0.9\r\n}\r\n\\end{eqnarray*}<br>\r\nIn the long run, what fraction of accounts are over 90 days overdue?','','','','','','','$\\frac{8}{51}$','0.2','none of above is correct','','','','','','','','','','A','','','','P is regular,the long time behaviour is determined by the solution of<br>\r\n  $P^{T}$*$X$=$X$<br>\r\n   $\\sum$$x_{i}$=$1$<br>\r\n get the answer $x_{3}$=$\\frac{8}{51}$.<br>\r\nSo the fraction is equal to $\\frac{8}{51}$.<br>',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(119,'4.3.1',2,'A Markov chain has a transition probability matrix(see attached figures)<br>\r\nFind the number of equivalence classes.What is the period of the Markov chain?','theall/image/exercise4-3-1.PNG','','','','','','1;1','2;5','none of above is correct','','','','','','','','','','A','','','','one can carefully check that every state is communicated(or you can compute the matrix $P^{n}$ to see that).So there are only one class.Besides,all the state share  the same period.Easily checking one(5,for eaxmple) we can find $P^{n}_{55}$$>$0,when n=8 or 5.As 8 and 5 are co-prime,we can see the period should be one.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(120,'4.3.2',2,'Which states are transient and which are recurrent in the Markov chain whose\r\ntransition probability matrix is attached?','theall/image/exercise4-3-2.PNG','','','','','','2,4,5is recurrent?0,1,3 is transient','2,4,5is trasient?0,1,3 is recurrent','none of above is correct','','','','','','','','','','A','','','','Carefully computing the$\\sum_{n=0}^{\\infty}$$P^{n}_{ii}$?we can find  only i=2,4,5 is infinity(easily see,there is only 1 remains).So the 2,4,5 is recurrent state,others are trasient.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,3),(121,'4.3.3',2,'A Markov chain on states {0,1,2,3,4,5} has transition probability matrix(attached figure).\r\nFind the number of communicating classes; which states are recurrent?','theall/image/exercise4-3-3.PNG','','','','','','3?state 1','3;state 0,1,2,3','none of above is correct','','','','','','','','','','B','','','','Carefully check,we can get (1)0 and 2 (2)1 and 3 (3) 4 and 5 ;the three are communicating class.\r\nso we can only compute one of their $\\sum_{n=0}^{\\infty}$$P^{n}_{ii}$ to decide if they are recurrent.\r\nWe find when i=1,0,the value is infinity. so 0,1,2,3 is recurrent.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,1),(122,'4.3.4',2,'Determine the communicating classes and period for each state of the Markov\r\nchain whose transition probability matrix is attached','theall/image/exercise4-3-4.PNG','','','','','','state 0,2,3,4,5is 1?state 1 is zero','state 2,3,4,5is 4;state 0,1 is zero','none of above is correct','','','','','','','','','','A','','','','Carefully checking,we can get (1)2 3 4 5 (2)1  (3) 0 ;the three are communicating class.\r\nso we can only compute one of their $P^{n}_{ii}$ to decide their period.\r\nWe find i=5,0,the value$>$0 when n is large enough,so the period is 1 for state 0,2,3,4,5.Value equals to zero\r\nwhen i=1 and n$>$2. So the period of 1 is zero.',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(123,'4.1.1',4,'Following Example 4.4,\r\nWhat is the average number of times per day that Mr C goes from Gym to\r\nHome?','','','','','','','0.343','0.274','0.835','0.8','','','','','','','','','C','','','','Recall that in Example 4.4, $X_n$ represent the states (whereabouts)\r\nof Mr. C. after $n$ steps of transitions. Over a large number, $n$, of steps,\r\nthe fraction of  transitions from G to H is\r\n$${ \\sum_{i=1}^n 1_{\\{X_{i-1}=G, X_i=H\\}} \\over n }\r\n\\approx {\\pi_G } \\times P(X_{n+1}=H|X_n=G)= 0.343 \\times 0.8= 0.274$$\r\nFor a large $n$ transitions, the time in hours are\r\n$$ (12 \\pi_H + 10 \\pi_O + 2 \\pi_G )n = 7.864 n,$$\r\nwhich is $ 7.864 n/24= 0.328n$ days.\r\nIn other words, there are about $ 0.274n$ transitions from G to H\r\nover $0.328n$ days. On average, it is about $ 0.274/.328= 0.835$ times\r\nfrom G to H daily.',3,2,3,3,0,0,0,'','','Oops, try again. You may revise Chapter 4.1 - 4.2.','You get it!',1,0.25,5),(124,'4.2.2',4,'In Example 4.5, is the process $\\{(X_{n-1}, X_{n}): n \\geq 2\\}$   a $MC$?','','','','','','','Yes.','No.','It is uncertain.','','','','','','','','','','B','','','','No. It is not a MC, because,\r\n\\begin{eqnarray*} && P(X_{n+1}=2| X_n=1, X_{n-1}=1, X_{n-2}=2) =1-p  \\\\\r\n&\\not=& 0\r\n=P(X_{n+1}=2| X_n=1, X_{n-1}=1, X_{n-2}=1, X_{n-3}=2).\r\n\\end{eqnarray*}\r\n(Please contemplate why?)',3,1,3,3,0,0,0,'','','Sorry, it is not correct. Time to revise the concept of Markov chain!','Great! You got the concept of Markov chain!',1,0.333,4),(125,'4.2.4',4,'Let a random variable $\\xi$\r\nfollow   exponential distribution\r\nwith density\r\n$ f(x)= e^{-x}$ for $x>0$ and $0$ for $x < 0$.\r\nIn Example 4.6, suppose every up (down) day, the rise (fall), as multiple\r\nof the previous day\'s closing price, has the same distribution\r\nas that of $\\xi+1$ $(1/(a \\xi+1))$, independent of everything else, where\r\n$a>0$ is a constant.\r\nIs the information above enough to calculate the long term return of the stock, particularly when\r\n$a=1$ or $a=7/4$ or $a=4/7$?','','','','','','','Yes','No.','It is uncertain.','','','','','','','','','','A','','','','Let $\\eta_i$ be the return of day $n$, as multiple of\r\nthe closing price of day\r\n$n-1$. Then, 1 dollar will grow to\r\n$$\\prod_{i=1}^n \\eta_i\r\n= e^{n \\times (1/n) \\sum_{i=1}^n \\log \\eta_i } $$\r\nafter\r\n$n$ days of trading. Recall that the long run fraction of up (down)\r\ndays is $4/11$ ($7/11$).\r\nObserve that\r\n\\begin{eqnarray*}\r\n&& (1/n)\\sum_{i=1}^n \\log \\eta_i   \\approx 4/11 \\times E\\{\\log(\\xi+1)\\}\r\n+ 7/11 \\times E\\{-\\log(a\\xi + 1)\\}\r\n\\\\\r\n&=&\r\n {4\\over 11}  \\int_0^\\infty \\log(x+1) e^{-x} dx\r\n- {7\\over 11}  \\int_0^\\infty \\log(ax+1) e^{-x} dx\r\n\\end{eqnarray*}\r\nIf $a \\geq 1$, obviously the above quantity is negative.\r\nTherefore, in the long run the stock decreases exponentially fast.\r\nIf $a=4/7$, since $\\log(ax+1) > a \\log(x+1)$, the above quantity is\r\nstill negative, and the stock still decreases exponentially fast.\r\nThe dividing value of $a$ is 0.4635. And the stock increases\r\n(decreases) exponentially fast\r\nwhen $a<0.4635$ ($a>0.4635$).',3,3,3,3,0,0,0,'','','Oops, try again.','Well done! You have mastered the concept of long run behavior.',1,0.33,5),(126,'4.3.2',4,'Can you prove Proposition 4.1 (a), (b) and (c)?\r\n<br>\r\n((b) and (c) are beyond requirement)','','','','','','','Yes, I can!','I have no idea...','','','','','','','','','','','A','','','','(a). By the definition of $d(i)$, all $n$ with $P_{ii}^{(n)} >0$\r\ncontains $d(i)$ as a factor. Whence, if $k$ is not a multiple of $d(i)$,\r\n$P_{ii}^{(k)}=0$.\r\n\r\n(b). Citing a result of algebra regarding gcd: there exists\r\n$n_1, n_2, ..., n_k$ with $P_{ii}^{(n_j)}>0$ and non-zero integers $a_1,\r\n..., a_k$ such that\r\n$d(i)=a_1 n_1 + \\cdots + a_k n_k$. (Proof of this result is also elmentary;\r\nsee the following remark.)\r\nFor notational simplicity, assume $a_1,..., a_l$ are positive and $a_{l+1},..., a_k$ are\r\nnegative, for some $1 \\leq l \\leq k$.\r\nLet $m=-(a_{l+1}n_{l+1} +\\cdots + a_k n_k) >0$.\r\nFor any  $n\\geq m^2+m$, let $n^*$ be such that\r\n$n  = n^* m + r$, where $ 0 \\leq r < m$. Then,\r\n$n^* > m $ and\r\n\\begin{eqnarray*}\r\nnd(i) &=& n^*m d(i) + r d(i) = n^*m d(i) + r a_1 n_1 + \\cdots + ra_k a_k\r\n\\\\\r\n&=&\r\n -(n^*d(i) - r)(a_{l+1} n_{l+1} + \\cdots + a_k n_k)\r\n+ r a_1 n_1 \\cdots + ra_l a_l.\r\n\\end{eqnarray*}\r\nNotice that $n^*d(i) - r>0$.\r\nThen the claim of part (b) follows by\r\nobserving that, for any nonnegative integers $s_1,...,s_k$,\r\n$$P_{ii}^{(s_1 n_1+ \\cdots s_k n_k)} \\geq\r\nP_{ii}^{(s_1 n_1)} \\times \\cdots \\times P_{ii}^{(s_k n_k)}\r\n\\geq \\{ P_{ii}^{( n_1)} \\}^{s_1}\\times \\cdots \\times\r\n\\{P_{ii}^{(n_k)}\\}^{s_k} > 0.$$\r\n\r\n\r\n(c). If $i \\leftrightarrow j$, there exist $k>0$ and $l>0$ such that\r\n$P_{ij}^{(k)}>0$ and $P_{ji}^{(l)}>0$.\r\nSince, by part (b), $P_{jj}^{(n d(j))}>0$ for all large $n$, it follows\r\nthat\r\n$$ P_{ii}^{ (k+l + n d(j))} \\geq P_{ij}^{(k)} P_{jj}^{ (n d(j))} P_{ji}^{(l)}\r\n>0,$$\r\nfor all large $n$. Hence $k+l + n d(j)$ contains $d(i)$ as a factor\r\nfor all large $n$. Then the difference of\r\n$k+l + (n+1) d(j)$ and $k+l+n d(j)$ for a large $n$, which is\r\n$d(j)$ must contain $d(i)$ as a factor.\r\nAs a result, $d(j) \\geq d(i)$. Likewise, one can show\r\n$d(i) \\geq d(j)$. It then holds that\r\n$d(i)=d(j)$.\r\n\r\n<br><br>\r\nRemark. \r\n<br>\r\nSuppose ${\\cal N}$ is a set of positive integers and $J$ is\r\nthe gcd of ${\\cal N}$. Then,\r\n$$ J= \\min\\{ a_1 n_1+ \\cdots + a_k n_k > 0: \\hbox{ for\r\nall $k\\geq 1$, } n_i \\in {\\cal N} \\hbox{ and } a_i\r\n\\hbox{ are nonzero integers} \\}.$$\r\n In other words, $J$ is the smallest positive integer as a\r\n  combination of elements of ${\\cal N}$ with the combination coefficients\r\n  being integers.\r\n  Proof is a little technical but not too difficult: Suppose $J^* \\equiv a_1 n_1 + \\cdots +a_k n_k$\r\n  is indeed the smallest positive number among such combinations.\r\n  Obviously, $J^* \\leq n$ for all $n \\in {\\cal A}$.\r\n  For any $n \\in {\\cal A}$, write $n= n^* J^* + r$ where\r\n  $ 0 \\leq r < J^*$. Then, $r=0$, for otherwise\r\n  $$n-n^*(a_1 n_1+\\cdots a_k n_k)= n-n^*J^* = r$$\r\n  produces a positive number smaller than $J^*$.\r\n  Hence $J^*$ is a common divisor of ${\\cal N}$.\r\n  Suppose now all $n \\in {\\cal N}$ contain, in addition to\r\n  $J^*$, another common factor $j\\geq 1$.\r\n  Set $b_i = n_i/(J^* j)$ which are positive integers.\r\n  Then,\r\n  $$J^* = a_1 n_1+ \\cdots a_k n_k = J^* j (a_1b_1 + \\cdots + a_kb_k)$$\r\n  As a result, $1/j= (a_1 b_1 + \\cdots +a_k b_k)$, implying that\r\n  $j$ must be $1$. This proves $J^*$ is the gcd $J$.\r\n  As mentioned before, part (b) and (c) in the above Exercise\r\n  is too technical and probabilistically uninteresting and thus not required.',3,1,3,3,0,0,0,'','','Don\'t worry, it is not that easy. You may go through the answer and see whether you understand it!','Excellent! You make it! I am sure now you have a better understanding of Proposition 4.1!',1,0.5,5),(127,'3.1.9',4,'A Markov chain $X_n$ ($X_0=0$) has two states 0 and 1 \r\nand transition probability matrix \r\n$$ \\hskip .41in 0 \\quad \\,\\,\r\n\\, 1 $$\r\n$$ \r\n{\\bf P}= \\matrix{0 \\cr 1} \\pmatrix{.80  \\quad .20 \\cr \r\n	.30\\quad .70}\r\n$$\r\nThen what is $P(X_3=0, X_1 \\not=0, X_2 \\not=0 \\,\\,|\\,\\, X_0=0)$?','','','','','','','0.7','0.512','0.042','0.8','','','','','','','','','C','','','','\\begin{eqnarray*}\r\n&&P(X_{3}=0, X_{1} \\not=0, X_{2} \\not=0 \\,\\,|\\,\\, X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{1}\\not=0, X_{2}\\not=0, X_{0}\\not=0)P(X_{1}\\not=0, X_{2}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{2}\\not=0)P(X_{2}\\not=0\\,\\,|\\,\\,X_{1}\\not=0, X_{0}=0)P(X_{1}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP(X_{3}=0\\,\\,|\\,\\, X_{2}\\not=0)P(X_{2}\\not=0\\,\\,|\\,\\,X_{1}\\not=0)P(X_{1}\\not=0\\,\\,|\\,\\,X_{0}=0)\r\n\\\\\r\n&=&\r\nP_{10}P_{11}P_{01}\r\n\\\\\r\n&=&\r\n0.3\\times 0.7\\times 0.2\r\n\\\\\r\n&=&\r\n0.042\r\n\\end{eqnarray*}',1,1,1,1,0,0,0,'','','Oops, you may try it again. Or revise the concept of Markov chain.','Great! You have mastered the concept of Markov chain!',1,0.25,1),(128,'3.4.12',4,'A Markov chain has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n	0 \\quad .5 \\quad .5 \\cr\r\n	0 \\quad .5 \\quad .5 \\cr\r\n	0 \\quad .5 \\quad .5  }\r\n$$\r\nWhat is the mean time to reach state 2 starting from state 0?','','','','','','','0.','1.','2.','3.','None of the above is correct.','','','','','','','','C','','','','Let $s_i$ be the mean time to reach state 2 starting from\r\nstate $i$, $including$ the beginning state if it is 2.\r\nThen,\r\n\\begin{eqnarray*}\r\n&&s_0 = 1 + 0.5\\times(s_1+s_2)\r\n\\\\\r\n&&s_1 = 1 + 0.5\\times(s_1+s_2)\r\n\\\\\r\n&&s_2 = 0\r\n\\end{eqnarray*}\r\nSolve this equation, we have\r\n$$s_0=s_1=2,\\,\\, s_2= 0$$\r\n The mean time to reach state 2 starting from state 0 is 2.',2,1,1,1,0,0,0,'','','Oops, try again. It is all about calculating the mean step to a state!','Good! Now you know how to calculate the mean step to a state!',1,0.2,1),(129,'3.4.14',4,'A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n $$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n 	0  \\quad 0.6 \\quad 0.4 \\cr\r\n 	0   \\quad 0.5 \\quad 0.5 \\cr\r\n 	0   \\quad 0.5 \\quad 0.5  }\r\n $$\r\nLet $T=\\min\\{ n\\geq 0: X_n=2, X_{n+1}=2\\}$, then what is $E(T)$?','','','','','','','1.1','2.0','2.2','4.2','None of the above is correct.','','','','','','','','D','','','','Let $s_i$ be the mean time to reach 2 such that $X_{s_i} = X_{s_i + 1} = 2$, beginning from state $i$. Then,\r\n\\begin{eqnarray*}\r\n&&s_0 = 0.6s_1 + 0.4s_2 + 1\r\n\\\\\r\n&&s_1 = 0.5s_1 + 0.5s_2 + 1\r\n\\\\\r\n&&s_2 = 0.5s_1 + 0.5\\times0\r\n\\end{eqnarray*}\r\nThe solutions are $s_0 = 4.2, \\,\\, s_1 = 4, \\,\\, s_2 = 2$.\r\n\r\nSince $X_0 = 0$, we have $E(T) = s_0 = 4.2$.',2,2,1,2,0,0,0,'','','Oops, try again. It is about calculating the mean step to a state!','Great! Now you should be familiar with calculating the mean step to a state.',1,0.2,3),(130,'3.4.16',4,'A Markov chain $\\{X_n\\}$ $(X_0=0)$ has transition probability matrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n	.2  \\quad 0.4 \\quad 0.4 \\cr\r\n	0   \\quad 0.5 \\quad 0.5 \\cr\r\n	0   \\quad 0.5 \\quad 0.5  }\r\n$$\r\n\r\nWhat is the mean number of visits of state 1 before reaching state 2?','','','','','','','1','2','3','4','None of the above is correct.','','','','','','','','A','','','','Let $w_i$ be the mean number of visits of state 1 before reaching state 2, starting from state $i$ (including the starting state).\r\nThen,\r\n\\begin{eqnarray*}\r\n&&w_0 = 0.2w_0 + 0.4w_1 + 0.4w_2\r\n\\\\\r\n&&w_1 = 1 + 0.5w_1 + 0.5w_2\r\n\\\\\r\n&&w_2 = 0\r\n\\end{eqnarray*}\r\nSolving the equations, \r\n$$w_0 = 1, \\,\\, w_1 = 2, \\,\\, w_2 = 0$$\r\nSince $X_0 = 0$, the mean number of visits of state 1 before reaching state 2 is 1.',1,1,1,1,0,0,0,'','','Oops, try again. It is about calculating the mean number of visits to a state!','Great! Now you should be familiar with calculating the mean number of visits to a state.',1,0.2,2),(131,'3.9.4',4,'Recall that the $n$-th generation of a branching\r\nprocess can be expressed as\r\n$$X_{n+1}= \\sum_{j=0}^{X_n} \\xi^{(n)}_j.$$\r\nwith $\\xi, \\xi^{(n)}_j, j \\geq 1$ are iid. And\r\n$X_0=1$ and $\\xi^{(n)}_0=0.$\r\nLet $\\phi(s)$ be the probability generating function\r\nof $\\xi$.\r\nSuppose $\\phi(s)=0.7+0.1s+0.1s^2+0.1s^3.$\r\nWhat is the probability of eventual extinction?','','','','','','','0.2','0.4','0.6','0.8','1','','','','','','','','E','','','','Since\r\n$\\phi(s)=0.7+0.1s+0.1s^2+0.1s^3,$\r\n\\begin{eqnarray*}\r\n\\phi(s)-s=0 &\\Rightarrow& 0.7+0.1s+0.1s^2+0.1s^3-s=0 \\\\\r\n&\\Rightarrow&(s-1)(s^2 + 2s - 7)=0\\\\\r\n&\\Rightarrow& s=1 \\,\\, {\\rm or }\\,\\, s = -1+2\\sqrt{2} \\,\\, {\\rm or }\\,\\, s = -1-2\\sqrt{2}.\r\n\\end{eqnarray*}\r\nSince only $s=1\\in [0,1]$, we have $u_{\\infty}=1$.',2,1,2,2,0,0,0,'','','Oops, try again. Get yourself familiar with Branching process.','Great! You have mastered a typical type of questions about Branching process.',1,0.2,2),(132,'3.9.6',4,'Let $\\phi(\\cdot)$ be  the probability generating\r\nfunction of $\\xi$, the number of direct next generation of any\r\nindividual of a population that evolves as a branching process.\r\nSuppose\r\n$$\\phi(s) =\r\n.5 s^2 + bs +c, \\qquad 0 \\leq s \\leq 1.$$\r\nAssume the population begins with one\r\nindividual ($X_0=1$). What is the probability of eventual\r\nextinction?','','','','','','','0.4','0.6','0.8','1.0','None of the above is correct.','','','','','','','','E','','','','\\begin{eqnarray*}\r\n{\\rm Since}\\,\\,\\phi(1) = 1 &\\Rightarrow&\\phi(1) = 0.5 + b + c = 1\\\\\r\n&\\Rightarrow&c = .5 - b\\\\\r\n&\\Rightarrow&\\phi(s) = .5 s^2 + bs +.5-b \\\\\r\n\\end{eqnarray*}\r\n\\begin{eqnarray*}\r\n\\phi(s)-s=0 &\\Rightarrow& .5s^2+(b-1)s+.5-b=0 \\\\\r\n&\\Rightarrow&(s-1)(.5s+b-.5)=0\\\\\r\n&\\Rightarrow&u_{\\infty} = s = \\left\\{ \\begin{array}{rl}\r\n 1-2b &\\mbox{ if $0<b<.5$} \\\\\r\n  1 &\\mbox{ otherwise.}\r\n       \\end{array} \\right.\r\n\\end{eqnarray*}',2,2,2,2,0,0,0,'','','Oops, try again. Get yourself familiar with Branching process.','Great! You have mastered a typical type of questions about Branching process.',1,0.2,3),(133,'4.3.1',3,'A two-state Markov chain has the transition probability matrix\r\n\\[\\mathbf{P}=\r\n\\begin{array}{c|cc|}\r\n&0&1\\\\\r\n0&1-a&a\\\\\r\n1&b&1-b\r\n\\end{array}\r\n\\]\r\nDetermine the first return probability\r\n\\begin{equation*}\r\nf^{(n)}_{00}=\\text{Pr}\\{X_1\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_0=0\\}\r\n\\end{equation*}','','','','','','','$b(1-b)^{n-1}a$','$(1-b)^{n-1}a$','$b(1-b)^{n-1}$','$b(1-b)^{n-2}a$','None of the above','','','','','','','','D','','','','\\begin{align*}\r\nf^{(n)}_{00}&=\\text{Pr}\\{X_1\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_0=0\\}\\\\\r\n&=\\text{Pr}\\{X_2\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_1=1\\}\\text{Pr}\\{X_1=1|X_0=0\\}\\\\\r\n&=\\text{Pr}\\{X_2\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_1=1\\}P_{01}\\\\\r\n&=\\text{Pr}\\{X_3\\neq0,\\dots,X_{n-1}\\neq0,X_n=0|X_2=1\\}P_{11}P_{01}\\\\\r\n&=\\text{Pr}\\{X_n=0|X_{n-1}=1\\}P_{11}^{n-2}P_{01}\\\\\r\n&=P_{10}P_{11}^{n-2}P_{01}\r\n\\end{align*}\r\n\r\nSo the probability is $b(1-b)^{n-2}a$',NULL,NULL,NULL,NULL,NULL,NULL,NULL,'','','','',NULL,NULL,2),(134,'4.1.2',4,'A mouse initially stays in cell 0\r\n in the following cage of four cells.\r\n At each step, it moves to one of\r\n the two connected cells with equal chance.\r\n Let $X_n=i$ if the mouse\r\n stays at cell $i$ at step $n$.\r\n Over a very long time,\r\n what  fraction of\r\n the transitions are the transitions from cell 0 to cell 1?','theall/image/DIY-4.1.2-1.PNG','','','','','','1/8','1/6','1/4','1/2','None of the above is correct.','','','','','','','','A','','','','The transition matrix for $X_n$ is\r\n \\begin{eqnarray*}\r\n&&  \\,\\,\\qquad  0\\quad \\,\\, 1 \\quad \\,\\, 2 \\quad\\,\r\n 3\\\\\r\n{\\bf P} &=& \\matrix{ \\hbox{0}  \\cr\r\n\\hbox{1} \\cr  \\hbox{2} \\cr \\hbox{3}}\r\n\\pmatrix{\r\n0 & .5 & 0 & .5  \\cr\r\n.5   & 0   & .5 & 0  \\cr\r\n0 & .5 & 0 & .5  \\cr\r\n.5   & 0   & .5 & 0\r\n}\r\n\\end{eqnarray*}\r\n\r\nNotice that\r\n$${\\bf P}^k ={\\bf P} \\quad \\hbox {for all odd $k$}, \\qquad\r\n{\\bf P}^k= \\pmatrix{.5 & 0 & .5 & 0 \\cr 0  & .5 & 0 & .5 \\cr .5 & 0 & .5 & 0 \\cr 0  & .5 & 0 & .5 } \\qquad \\hbox{for all even $k$}.$$\r\nLet $\\pi_j$ be the long run fraction of time staying in state $j$ after an odd-number times of transitions. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3=1$$\r\nThe solutions are $\\pi_0 = \\pi_1 = \\pi_2 = \\pi_3 = 1/4$.\r\n\r\nSimilarly, if the number of transitions is even in a long run, we also have\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2 + \\pi_3=1$$\r\nThus in the long run, $\\pi_0 = \\pi_1 = \\pi_2 = \\pi_3 = 1/4$.\r\n\r\nIn the long run, the fraction of the transitions from cell 0 to cell 1 is\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=1,\\,X_n=0)\r\n\\\\\r\n&=&\r\nP(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=0)+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=1)\r\n\\\\\r\n&&\r\n+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=2)+P(X_{n+1}=1,\\,X_n=0\\,|\\,X_{n-1}=3)\r\n\\\\\r\n&=& \r\nP_{01}P_{00}\\pi_0+P_{01}P_{10}\\pi_1+P_{01}P_{20}\\pi_2+P_{01}P_{30}\\pi_3\r\n\\\\\r\n&=&\r\n{1\\over 4} P_{01}(P_{00}+P_{10}+P_{20}+P_{30})\r\n\\\\\r\n&=&\r\n1\\over 8\r\n\\end{eqnarray*}',2,2,2,2,0,0,0,'','','Oops, try again. Get yourself familiar with Long run behavior.','Great! You have mastered a typical type of questions about long run behavior.',1,0.2,3),(135,'4.1.4',4,'A Markov chain $X_n, n\\geq 0$, has transition probability\r\nmatrix\r\n$$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n$${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n	.2 \\quad .5 \\quad .3 \\cr 0 \\quad .5 \\quad .5 \\cr .4 \\quad .5 \\quad\r\n	.1  }\r\n$$\r\nWhat is the limit of $P(X_n=0, X_{n+1}=1, X_{n+2}=2)$?','','','','','','','1/24','1/8','1/6','1/4','None of the above is correct.','','','','','','','','A','','','','Let $\\pi_j$ be the long run fraction of time staying in state $j$. Notice that ${\\bf P}$ is regular. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2){\\bf P}^2$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2=1$$\r\nThe solutions are $\\pi_0 = 1/6$, $\\pi_1 =1/2$, $\\pi_2 =1/3$.\r\n\\begin{eqnarray*}\r\n&&P(X_n=0, X_{n+1}=1, X_{n+2}=2)\r\n\\\\\r\n&=& \r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1,\\,X_n=0)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&=&\r\nP_{12}P_{01}\\pi_0\r\n\\\\\r\n&=&\r\n1/2 \\times 1/2 \\times 1/6\r\n\\\\\r\n&=&\r\n1/24.\r\n\\end{eqnarray*}',2,2,1,2,0,0,0,'','','Oops, try again. Get yourself familiar with Long run behavior.','Great! You have mastered a typical type of questions about long run behavior.',1,0.2,3),(136,'4.1.6',4,'The following is the transition probability matrix\r\nof a Markov chain with states 0, 1 and 2:\r\n$$ \\hskip .4in 0 \\quad \\,\\,\\,\\,\\, 1 \\quad \\,\\,\\,\\,\\, 2 $$\r\n$${\\bf P} = \\matrix{0 \\cr 1 \\cr 2}\r\n\\pmatrix{.50 \\quad .50 \\quad \\,\\, 0 \\cr\r\n	\\,\\,.25 \\quad .50 \\quad .25 \\cr\r\n	\\,\\,\\, 0 \\quad \\, \\,\\,\\, .50 \\quad \\, .50 }\r\n$$\r\nSomeone is paid 2 dollar for each one step transit from\r\nstate 1 to state 2 and  $-1$ dollar each one step transit from\r\nstate 2 to state 1. After 1000,000 steps, approximately how much\r\nis gained?','','','','','','','500,000','750,000','1000,000','1250,000','None of the above is correct.','','','','','','','','D','','','','Let $\\pi_j$ be the long run fraction of time staying in state $j$. Notice that ${\\bf P}$ is regular. Solve the equations\r\n$$(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2)=(\\pi_0\\,\\,\\pi_1\\,\\,\\pi_2){\\bf P}$$\r\n$$\\pi_0 + \\pi_1 + \\pi_2=1$$\r\nThe solutions are $\\pi_0 = 1/4$, \\, $\\pi_1 =1/2$,\\, $\\pi_2 =1/4$.\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=1, X_{n+2}=2)\r\n\\\\\r\n&=& \r\nP(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=0)+P(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=1)+P(X_{n+1}=1, X_{n+2}=2\\,|\\,X_n=2)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=1)P(X_n=1)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=2\\,|\\,X_{n+1}=1)P(X_{n+1}=1\\,|\\,X_n=2)P(X_n=2)\r\n\\\\\r\n&=&\r\nP_{12}P_{01}\\pi_0+P_{12}P_{11}\\pi_1+P_{12}P_{21}\\pi_2\r\n\\\\\r\n&=&\r\n1/8\r\n\\end{eqnarray*}\r\n\\begin{eqnarray*}\r\n&&P(X_{n+1}=2, X_{n+2}=1)\r\n\\\\\r\n&=& \r\nP(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=0)+P(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=1)+P(X_{n+1}=2, X_{n+2}=1\\,|\\,X_n=2)\r\n\\\\\r\n&=&\r\nP(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=0)P(X_n=0)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=1)P(X_n=1)\r\n\\\\\r\n&&\r\n+P(X_{n+2}=1\\,|\\,X_{n+1}=2)P(X_{n+1}=2\\,|\\,X_n=2)P(X_n=2)\r\n\\\\\r\n&=&\r\nP_{21}P_{02}\\pi_0+P_{21}P_{12}\\pi_1+P_{21}P_{22}\\pi_2\r\n\\\\\r\n&=&\r\n1/8\r\n\\end{eqnarray*}\r\nThe approximate gain after 1000,000 steps would be\r\n$$1000,0000 \\times {{1\\over 8} \\times {2} + {1\\over 8} \\times (-1)} = 1,250,000.$$',2,2,2,2,0,0,0,'','','Oops, try again. Get yourself familiar with Long run behavior.','Great! You have mastered a typical type of questions about long run behavior.',1,0.2,3),(137,'4.2.6',4,'A stock has only two outcomes of performance\r\n on each trading day: up (U) or down (D). Let $X_n$ be\r\n its performance outcome on trading day $n$. The stock performance\r\n on any day depends only on those of the preceding two days. Assume\r\n for all $n\\geq 1$,\r\n $P(X_{n+1}=U| X_n = U, X_{n-1}=U) = 0.9$,\r\n $P(X_{n+1}=U| X_n = U, X_{n-1}=D) = 0.7$, $P(X_{n+1}=U| X_n = D,\r\n X_{n-1}=U) = 0.4$, $P(X_{n+1}=U| X_n = D, X_{n-1}=D) = 0.2$. What\r\n is the approximate chance that the stock is up on the trading day ten years from\r\n now?','','','','','','','1/3','1/4','2/3','3/4','None of the above is correct.','','','','','','','','C','','','','Set $Y_n=(X_{n-1},X_n)$. Then $Y_n$ is a $MC$\r\nwith state space $\\{({\\rm U, U}), ({\\rm D, U}),\r\n({\\rm U, D}),   ({\\rm D, D})\\}$ and transition probability\r\nmatrix\r\n$$ \\hskip .8 in 0 ({\\rm U, U}) \\quad \\,\\,\\,\\,\\, 1 ({\\rm D, U}) \\quad \\,\\,\\,\\,\\, 2 ({\\rm U, D})\\quad \\,\\,\\,\\,\\, 3 ({\\rm D, D}) $$\r\n$${\\bf P} = \\matrix{0 ({\\rm U, U}) \\cr 1 ({\\rm D, U}) \\cr 2 ({\\rm U, D})\\cr 3 ({\\rm D, D})}\r\n\\pmatrix{.9\\quad\\quad \\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .1 \\quad\\quad\\quad\\quad\\quad 0\\cr\r\n	.7\\quad\\quad \\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .3 \\quad\\quad\\quad\\quad\\quad 0\\cr\r\n	0\\quad\\quad \\quad\\quad\\quad .4 \\quad\\quad\\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .6\\cr\r\n      0\\quad\\quad \\quad\\quad\\quad .2 \\quad\\quad\\quad\\quad\\quad 0 \\quad\\quad\\quad\\quad\\quad .8}\r\n$$\r\nSolve the equations\r\n$$(\\pi_0\\,\\, \\pi_1 \\,\\,\\pi_2\\,\\,\\pi_3)=(\\pi_0\\,\\, \\pi_1 \\,\\,\\pi_2\\,\\,\\pi_3){\\bf P}$$\r\n$$\\pi_0+\\pi_1+\\pi_2+\\pi_3=1$$\r\nThe solutions are $\\pi_0=7/12,\\,\\, \\pi_1=\\pi_2=1/12, \\,\\,\\pi_3=1/4$.\r\n<br>\r\nIn the long run, we have\r\n\\begin{eqnarray*}\r\n&&P(X_n=U)\r\n\\\\\r\n&=&\r\nP(X_n=U, X_{n-1}=D)+P(X_n=U, X_{n-1}=U)\r\n\\\\\r\n&=&\r\nP(Y_n=(D,U))+P(Y_n=(U,U))\r\n\\\\\r\n&=&\r\n\\pi_1 + \\pi_0\r\n\\\\\r\n&=&\r\n1/2+7/12\r\n\\\\\r\n&=&\r\n2/3.\r\n\\end{eqnarray*}',3,2,3,2,0,0,0,'','','Oops, try again. Get yourself familiar with Long run behavior.','Great! You have mastered a typical type of questions about long run behavior.',1,0.2,4),(138,'4.3.4',4,'A Markov chain has transition probability matrix\r\n $$ \\hskip .4in 0 \\quad \\, 1 \\quad \\,\\,2 $$\r\n $${\\bf P}= \\matrix{0 \\cr 1 \\cr 2} \\pmatrix{\r\n 	0 \\quad .6 \\quad .4 \\cr\r\n 	0 \\quad .7 \\quad .3 \\cr\r\n 	0 \\quad .5 \\quad .5  }\r\n $$\r\nWhich states are transient states?','','','','','','','State 0.','State 1.','State 2.','Both State 1 and State 2.','Both State 2 and State 3.','Both State 1 and State 3.','','','','','','','A','','','','For $n>0$, starting from state 0, the $MC$ will never return to state 0 at any step $n$.\r\n$f_{00}^{(0)}=0$, $f_{00}=0$. Therefore, state 0 is transient.',2,1,1,1,0,0,0,'','','Oops, try again. Get yourself familiar with Long run behavior.','Great! You have mastered a typical type of questions about long run behavior.',1,0.167,3);
/*!40000 ALTER TABLE `mathematics_question` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_question_linkneuron`
--

DROP TABLE IF EXISTS `mathematics_question_linkneuron`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_question_linkneuron` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `question_id` int(11) NOT NULL,
  `neuron_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `mathematics_question_linkneuron_question_id_494f1d18_uniq` (`question_id`,`neuron_id`),
  KEY `mathematics_question_neuron_id_72d5b9e9_fk_mathematics_neuron_id` (`neuron_id`),
  CONSTRAINT `mathematics_ques_question_id_f881df2e_fk_mathematics_question_id` FOREIGN KEY (`question_id`) REFERENCES `mathematics_question` (`id`),
  CONSTRAINT `mathematics_question_neuron_id_72d5b9e9_fk_mathematics_neuron_id` FOREIGN KEY (`neuron_id`) REFERENCES `mathematics_neuron` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=257 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_question_linkneuron`
--

LOCK TABLES `mathematics_question_linkneuron` WRITE;
/*!40000 ALTER TABLE `mathematics_question_linkneuron` DISABLE KEYS */;
INSERT INTO `mathematics_question_linkneuron` VALUES (2,1,7),(69,2,3),(70,3,3),(71,3,7),(105,4,9),(106,6,14),(73,7,2),(74,7,3),(76,7,5),(72,7,9),(75,7,12),(78,8,3),(77,8,9),(79,8,13),(100,9,6),(98,9,9),(99,9,12),(4,11,3),(5,11,7),(8,12,3),(9,12,7),(6,13,3),(7,13,7),(10,14,3),(11,14,7),(12,15,5),(13,16,9),(14,17,9),(15,18,9),(16,19,9),(18,20,6),(17,20,9),(86,21,23),(87,22,25),(88,23,31),(89,24,31),(90,25,23),(91,26,32),(92,26,33),(93,27,33),(94,29,29),(95,30,31),(19,31,3),(20,31,7),(21,32,3),(22,32,7),(23,33,3),(24,33,6),(25,33,7),(26,34,3),(27,34,6),(28,35,3),(29,35,6),(31,36,3),(32,36,6),(30,36,9),(33,37,3),(34,37,7),(35,38,3),(36,38,7),(37,39,3),(38,39,7),(39,40,3),(40,40,7),(41,41,3),(42,41,7),(43,42,3),(44,42,5),(45,43,3),(46,43,5),(47,43,7),(48,44,3),(49,44,5),(50,45,3),(51,45,7),(52,46,3),(53,46,5),(54,47,3),(55,47,5),(56,47,7),(57,48,7),(58,49,2),(59,49,3),(60,49,4),(61,50,9),(62,51,9),(63,51,13),(64,52,16),(65,52,21),(66,53,20),(67,53,21),(68,53,22),(81,54,3),(82,54,5),(80,54,9),(83,54,15),(84,55,19),(85,55,20),(96,56,2),(97,56,4),(101,57,2),(102,57,4),(103,58,3),(104,58,6),(107,59,14),(108,60,9),(109,61,2),(111,61,5),(110,61,10),(112,62,2),(113,62,6),(114,63,3),(115,63,6),(151,63,26),(116,64,2),(117,64,6),(119,66,6),(120,68,6),(121,69,6),(122,70,6),(124,71,2),(123,71,9),(125,71,13),(127,72,2),(129,72,6),(126,72,9),(128,72,13),(131,73,2),(130,73,9),(132,73,12),(133,75,9),(134,76,9),(136,77,3),(137,77,7),(135,77,9),(138,78,9),(139,79,14),(142,80,14),(140,80,20),(141,80,21),(145,81,14),(143,81,20),(144,81,21),(147,82,2),(146,82,16),(150,83,14),(148,83,20),(149,83,21),(152,84,2),(153,84,20),(154,84,21),(155,84,22),(156,86,20),(158,86,21),(159,86,22),(157,86,28),(160,87,9),(162,88,6),(161,88,9),(163,89,9),(164,90,3),(165,90,6),(166,91,6),(167,92,6),(168,93,6),(169,94,6),(170,95,9),(171,96,9),(172,97,9),(173,98,9),(174,99,9),(175,100,9),(182,101,3),(183,101,37),(184,101,39),(178,102,3),(176,102,27),(177,102,37),(179,103,3),(180,103,37),(181,103,39),(185,104,3),(186,104,37),(187,104,39),(188,105,3),(189,105,25),(190,105,27),(191,105,37),(192,105,39),(194,106,3),(195,106,37),(196,106,39),(193,106,40),(198,107,3),(197,107,34),(199,107,37),(200,107,39),(202,108,37),(201,108,40),(203,109,3),(204,109,37),(205,110,3),(206,110,37),(208,111,3),(207,111,40),(210,112,37),(209,112,40),(211,113,37),(212,114,37),(213,115,37),(214,116,37),(215,117,37),(216,119,35),(217,119,37),(218,120,37),(219,121,35),(220,122,37),(221,123,37),(222,124,3),(223,125,37),(224,126,37),(225,127,3),(226,127,5),(227,127,6),(228,127,7),(229,128,9),(230,128,13),(231,129,13),(232,130,15),(233,131,14),(234,131,20),(235,131,21),(236,131,22),(237,132,14),(238,132,20),(239,132,21),(240,132,22),(241,133,3),(243,133,7),(242,133,28),(244,134,35),(245,134,37),(246,134,39),(247,135,35),(248,135,37),(249,135,39),(250,136,35),(251,136,37),(252,136,39),(253,137,35),(254,137,37),(255,137,39),(256,138,37);
/*!40000 ALTER TABLE `mathematics_question_linkneuron` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_question_rightproblems`
--

DROP TABLE IF EXISTS `mathematics_question_rightproblems`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_question_rightproblems` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `from_question_id` int(11) NOT NULL,
  `to_question_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `mathematics_question_rightproblem_from_question_id_129c9a06_uniq` (`from_question_id`,`to_question_id`),
  KEY `mathematics_q_to_question_id_ced31d52_fk_mathematics_question_id` (`to_question_id`),
  CONSTRAINT `mathematics_q_to_question_id_ced31d52_fk_mathematics_question_id` FOREIGN KEY (`to_question_id`) REFERENCES `mathematics_question` (`id`),
  CONSTRAINT `mathematics_from_question_id_4b5f6e19_fk_mathematics_question_id` FOREIGN KEY (`from_question_id`) REFERENCES `mathematics_question` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=50 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_question_rightproblems`
--

LOCK TABLES `mathematics_question_rightproblems` WRITE;
/*!40000 ALTER TABLE `mathematics_question_rightproblems` DISABLE KEYS */;
INSERT INTO `mathematics_question_rightproblems` VALUES (14,2,32),(16,3,49),(24,4,5),(29,4,60),(25,5,4),(23,5,54),(27,6,59),(18,7,50),(20,8,51),(35,8,128),(5,12,14),(6,14,12),(13,19,19),(15,32,2),(33,41,127),(17,49,3),(19,50,7),(39,50,130),(21,51,8),(37,51,129),(22,54,5),(41,55,131),(43,55,132),(26,59,6),(28,60,4),(31,98,99),(30,99,98),(45,103,134),(49,108,137),(32,127,41),(34,128,8),(36,129,51),(38,130,50),(40,131,55),(42,132,55),(44,134,103),(47,134,135),(46,135,134),(48,137,108);
/*!40000 ALTER TABLE `mathematics_question_rightproblems` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_question_twinproblems`
--

DROP TABLE IF EXISTS `mathematics_question_twinproblems`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_question_twinproblems` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `from_question_id` int(11) NOT NULL,
  `to_question_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `mathematics_question_twinproblems_from_question_id_e90c9b3c_uniq` (`from_question_id`,`to_question_id`),
  KEY `mathematics_q_to_question_id_ecb9db4e_fk_mathematics_question_id` (`to_question_id`),
  CONSTRAINT `mathematics_q_to_question_id_ecb9db4e_fk_mathematics_question_id` FOREIGN KEY (`to_question_id`) REFERENCES `mathematics_question` (`id`),
  CONSTRAINT `mathematics_from_question_id_5d8cc6fd_fk_mathematics_question_id` FOREIGN KEY (`from_question_id`) REFERENCES `mathematics_question` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=67 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_question_twinproblems`
--

LOCK TABLES `mathematics_question_twinproblems` WRITE;
/*!40000 ALTER TABLE `mathematics_question_twinproblems` DISABLE KEYS */;
INSERT INTO `mathematics_question_twinproblems` VALUES (48,2,124),(21,3,49),(26,4,8),(28,4,54),(56,4,130),(23,7,50),(25,8,4),(4,11,12),(6,11,14),(3,12,11),(5,14,11),(8,16,17),(10,16,18),(12,16,19),(14,16,20),(7,17,16),(9,18,16),(11,19,16),(13,20,16),(16,34,35),(30,34,90),(15,35,34),(50,40,127),(22,49,3),(24,50,7),(27,54,4),(58,83,131),(29,90,34),(32,90,92),(34,90,93),(36,91,94),(31,92,90),(33,93,90),(35,94,91),(52,95,128),(54,95,129),(38,96,97),(40,96,98),(37,97,96),(44,97,100),(39,98,96),(42,98,99),(41,99,98),(43,100,97),(46,102,105),(62,102,134),(64,102,135),(45,105,102),(66,109,137),(47,124,2),(49,127,40),(51,128,95),(53,129,95),(55,130,4),(57,131,83),(60,131,132),(59,132,131),(61,134,102),(63,135,102),(65,137,109);
/*!40000 ALTER TABLE `mathematics_question_twinproblems` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_question_wrongproblems`
--

DROP TABLE IF EXISTS `mathematics_question_wrongproblems`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_question_wrongproblems` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `from_question_id` int(11) NOT NULL,
  `to_question_id` int(11) NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `mathematics_question_wrongproblem_from_question_id_11e8abd4_uniq` (`from_question_id`,`to_question_id`),
  KEY `mathematics_q_to_question_id_c3e09f2b_fk_mathematics_question_id` (`to_question_id`),
  CONSTRAINT `mathematics_q_to_question_id_c3e09f2b_fk_mathematics_question_id` FOREIGN KEY (`to_question_id`) REFERENCES `mathematics_question` (`id`),
  CONSTRAINT `mathematics_from_question_id_665e701c_fk_mathematics_question_id` FOREIGN KEY (`from_question_id`) REFERENCES `mathematics_question` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=57 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_question_wrongproblems`
--

LOCK TABLES `mathematics_question_wrongproblems` WRITE;
/*!40000 ALTER TABLE `mathematics_question_wrongproblems` DISABLE KEYS */;
INSERT INTO `mathematics_question_wrongproblems` VALUES (21,2,31),(36,2,124),(23,3,48),(28,4,8),(30,4,54),(44,4,130),(25,7,36),(27,8,4),(4,11,12),(6,11,13),(8,11,14),(3,12,11),(5,13,11),(7,14,11),(10,16,17),(18,16,18),(9,17,16),(17,18,16),(20,18,19),(19,19,18),(22,31,2),(26,36,7),(38,40,127),(24,48,3),(29,54,4),(32,56,74),(31,74,56),(46,83,131),(34,95,96),(40,95,128),(42,95,129),(33,96,95),(50,102,134),(52,102,135),(54,102,136),(56,109,137),(35,124,2),(37,127,40),(39,128,95),(41,129,95),(43,130,4),(45,131,83),(48,131,132),(47,132,131),(49,134,102),(51,135,102),(53,136,102),(55,137,109);
/*!40000 ALTER TABLE `mathematics_question_wrongproblems` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_user`
--

DROP TABLE IF EXISTS `mathematics_user`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_user` (
  `id` varchar(100) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_user`
--

LOCK TABLES `mathematics_user` WRITE;
/*!40000 ALTER TABLE `mathematics_user` DISABLE KEYS */;
/*!40000 ALTER TABLE `mathematics_user` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_userneuron`
--

DROP TABLE IF EXISTS `mathematics_userneuron`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_userneuron` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `familiar` double NOT NULL,
  `neuronid_id` int(11) NOT NULL,
  `userid_id` int(11) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_userneuron`
--

LOCK TABLES `mathematics_userneuron` WRITE;
/*!40000 ALTER TABLE `mathematics_userneuron` DISABLE KEYS */;
/*!40000 ALTER TABLE `mathematics_userneuron` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_userquestion`
--

DROP TABLE IF EXISTS `mathematics_userquestion`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_userquestion` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `time` datetime NOT NULL,
  `e` int(11) NOT NULL,
  `answer` varchar(100) NOT NULL,
  `right` varchar(100) NOT NULL,
  `questionid_id` int(11) NOT NULL,
  `userid_id` int(11) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_userquestion`
--

LOCK TABLES `mathematics_userquestion` WRITE;
/*!40000 ALTER TABLE `mathematics_userquestion` DISABLE KEYS */;
/*!40000 ALTER TABLE `mathematics_userquestion` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `mathematics_users`
--

DROP TABLE IF EXISTS `mathematics_users`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `mathematics_users` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `names` varchar(200) NOT NULL,
  `token` varchar(100) DEFAULT NULL,
  `login` datetime NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=latin1;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `mathematics_users`
--

LOCK TABLES `mathematics_users` WRITE;
/*!40000 ALTER TABLE `mathematics_users` DISABLE KEYS */;
INSERT INTO `mathematics_users` VALUES (2,'hwangbz','e7562aba3c2fb99e43acf6c8ad9a6bdd','2017-04-03 06:57:37'),(3,'rhanab','e94362d39a8f404cd50cfc1a62a64e3f','2017-04-04 02:24:42'),(4,'jlicy','cba23fd65d996dda35f3b1e8fcc8ab11','2017-03-25 07:53:51'),(5,'ttangad','z','2017-03-01 06:00:00'),(6,'wzhouaf','e','2017-03-05 06:00:00'),(7,'ctanac','12206e54b9708c0ae7377238b505239d','2017-04-04 02:24:47'),(8,'zlicx','c72bc7142cc1d2186447b0ab085a41e6','2017-04-04 10:47:05'),(9,'xpanad','78ff30f8730578f1a8365375c023a831','2017-03-29 06:25:16'),(10,'ttlaiab','','2017-03-29 10:36:00'),(11,'shuangak','e7cc0c04b8e1acf8f3435ffbea14fa07','2017-03-31 11:45:18');
/*!40000 ALTER TABLE `mathematics_users` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2017-04-04 18:11:21
